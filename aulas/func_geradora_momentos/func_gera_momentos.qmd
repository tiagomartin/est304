---
title: "Função Geradora de Momentos"
format: 
  revealjs:
    width: 1600
    height: 900
    footer: ""
    theme: quartomonothemer.scss
    slide-number: c/t
    show-slide-number: all
    preview-links: auto
    self-contained: false
    embed-resources: false
  beamer:          # Slides em PDF (LaTeX/Beamer)
    aspectratio: 169   # 16:9
    keep-tex: true     # opcional, guarda o .tex gerado
incremental: false
code-link: true
bibliography: references.bib
title-slide-attributes:
    data-background-image: /images/back.png
    data-background-size: cover
    data-background-opacity: "0.3"
execute:
  echo: true
  freeze: auto
---


## Momentos

Calculamos algumas características de uma variável aleatória $X$, tais como $E(X)$ e $Var(X)$, através da distribuição de probabilidade de $X$

. . .

Vimos que a variância pode ser expressa como uma **função da esperança** das duas primeiras potências de $X$, ou seja,

$$Var(X) = E(X^2) - \Big[E(X)\Big]^2$$


. . .



Outras características da distribuição de probabilidade de $X$ podem ser expressas por meio das esperanças das potências de $X$ como por exemplo **coeficientes de assimetria** e **curtose**.


## Momentos

::: {.callout-note title="Definição 01: Momentos"}
Seja $X$ uma variável aleatória. Então, o **k-ésimo momento** de $X$, denotado por $\mu'_k$ é definido como,

$$\mu'_k = E(X^k)$$

desde que essa quantidade exista. O **k-ésimo momento central** de uma variável aleatória $X$, denotado por $\mu_k$ é definido como,

$$\mu^k = E\Big[X - E(X)\Big]^k$$

sempre que essa quantidade existir.
:::



## Momentos

Note que,

- $E(X) = \mu'_1$

. . .


- $Var(X) = \mu_2 = \mu'_2 - [\mu'_1]^2$


. . .


- Para qualquer variável aleatória, $\mu_1 = 0$.




## Momentos


- Se $X$ é uma variável aleatória discreta,

$$\mu'_k = E(X^k) = \sum_{i=1}^{\infty} x_i^k p(x_i)$$

. . .


- Se $X$ é uma variável aleatória contínua,

$$\mu'_k = E(X^k) = \int_{-\infty}^{\infty} x_i^k f(x) \, dx$$



## Momentos


::: {.callout-note title="Exemplo 01: Momentos da distribuição Gamma"}
Encontre o $k$-ésimo momento de $X \sim Gamma(\alpha, \lambda)$.
:::

. . . 

**Solução:** Temos que, se $X \sim Gamma(\alpha, \lambda)$, então sua função densidade é dada por,


$$
f(x \mid \alpha,\lambda) =
\begin{cases}
\dfrac{\lambda e^{-\lambda x}(\lambda x)^{\alpha-1}}{\Gamma(\alpha)}, & x\ge 0,\\[6pt]
0, & x<0
\end{cases}
$$


## Momentos


Assim, 

$$
\begin{aligned}
E(X^k) &= \int_0^\infty x^k f(x\mid \alpha,\lambda)\, dx = \int_0^\infty x^k 
\frac{\lambda e^{-\lambda x}(\lambda x)^{\alpha-1}}
{\Gamma(\alpha)}\, dx \\[6pt]
 &= \int_0^\infty x^k 
\frac{\lambda e^{-\lambda x}\lambda^{\alpha-1} \, x^{\alpha-1}}
{\Gamma(\alpha)}\, dx = \frac{\lambda^\alpha}{\Gamma(\alpha)}
\int_0^\infty x^{\alpha+k-1} e^{-\lambda x}\, dx
\end{aligned}
$$


Note que a integral é quase a função gama, a não ser pelo termo $e^{-\lambda x}$. Seja então a seguinte mudança de variável,


$$u = \lambda  x \,\,\, \Rightarrow \,\,\, x = \dfrac{u}{\lambda}, \qquad dx = \dfrac{du}{\lambda}$$


## Momentos

Assim, 


$$
\begin{aligned}
E(X^k) &=  \dfrac{\lambda^\alpha}{\Gamma(\alpha)}
\int_0^\infty x^{\alpha+k-1} e^{-\lambda x}\, dx  = \dfrac{\lambda^\alpha}{\Gamma(\alpha)}
\int_0^\infty \Big(\dfrac{u}{\lambda}\Big)^{\alpha+k-1} e^{-u}\, \dfrac{du}{\lambda} \\[6pt] &= \dfrac{\lambda^\alpha}{\lambda ^{\alpha + k}\,\Gamma(\alpha)}
\int_0^\infty u^{\alpha+k-1} e^{-u}\, du = \dfrac{\lambda^\alpha \Gamma(\alpha + k)}{\lambda ^{\alpha + k}\,\Gamma(\alpha)} \\[6pt] &= \dfrac{\Gamma(\alpha+k)}{\Gamma(\alpha)\lambda^k}
\end{aligned}
$$


Mas,

$$
\Gamma(\alpha+k) = (\alpha+k-1)(\alpha+k-2)\cdots(\alpha+1)\alpha\Gamma(\alpha)
$$




## Momentos

De forma que, o $k$-ésimo momento de uma variável aleatória $X \sim Gamma (\alpha, \lambda)$ é dado por

$$
E(X^k)
=
\frac{\alpha(\alpha+1)\cdots(\alpha+k-1)}{\lambda^k}
$$




## Momentos


::: {.callout-note title="Exemplo 02: Momentos da distribuição Weibull"}
Encontre o $k$-ésimo momento de $X \sim Weibull(\alpha, \beta)$.
:::

. . . 

**Solução:** Temos que, se $X \sim Weibull(\alpha, \beta)$, então sua função densidade é dada por,

$$
f(x \mid \alpha,\beta)=
\begin{cases}
\dfrac{\beta}{\alpha}
\left(\dfrac{x}{\alpha}\right)^{\beta-1}
\exp\!\left[-\left(\dfrac{x}{\alpha}\right)^{\beta}\right], & x\ge 0,\\[6pt]
0, & x<0.
\end{cases}
$$



## Momentos


Assim, 

$$
\begin{aligned}
E(X^k) &= \int_0^\infty x^k 
\frac{\beta}{\alpha}
\left(\frac{x}{\alpha}\right)^{\beta-1}
\exp\!\left[-\left(\frac{x}{\alpha}\right)^{\beta}\right] dx 
\end{aligned}
$$

Mudança de variável:

$$
u = \left(\frac{x}{\alpha}\right)^{\beta}
\quad\Rightarrow\quad
x = \alpha u^{1/\beta},\qquad
dx = \alpha \frac{1}{\beta} u^{1/\beta - 1} du
$$


Além disso:

$$
x^k = \alpha^k u^{k/\beta},
\qquad
\left(\frac{x}{\alpha}\right)^{\beta-1}=u^{(\beta-1)/\beta},
\qquad
\exp\!\left[-\left(\frac{x}{\alpha}\right)^{\beta}\right]=e^{-u}
$$


## Momentos


logo, 

$$
\begin{aligned}
E(X^k) &= \int_0^\infty x^k 
\frac{\beta}{\alpha}
\left(\frac{x}{\alpha}\right)^{\beta-1}
\exp\!\left[-\left(\frac{x}{\alpha}\right)^{\beta}\right] dx \\[6pt] &= \int_0^\infty 
\alpha^k u^{k/\beta}\,
\frac{\beta}{\alpha}\,
u^{(\beta-1)/\beta}\,
e^{-u}\,
\alpha \frac{1}{\beta}u^{1/\beta -1}\,du \\[6pt] &= \alpha^k \int_0^\infty 
u^{\frac{k+\beta}{\beta}-1} 
e^{-u}\,du = \alpha^k\,\,\Gamma\!\left(1+\frac{k}{\beta}\right)
\end{aligned}
$$




## Função Geradora de Momentos


::: {.callout-note title="Definição 02: Função Geradora de Momentos"}
Seja $X$ uma variável aleatória qualquer. A **função geradora de momentos** (FGM) de $X$, denotada por $M_X$, é definida por

$$
M_X(t)=E(e^{tX}),
$$

para valores de $t$ em um intervalo contendo $0$ onde a experença exista.
:::



**Importante:** a função geradora de momentos é função de $t$. Para ela existir basta que exista $\varepsilon > 0$ tal que $E(e^{tX})$ esteja bem definida para qualquer $t \in (-\varepsilon, \varepsilon)$.



<!-- ## -->

<!-- ```{r, echo=FALSE} -->
<!-- #| label: fig-mgf-exp -->
<!-- #| fig-cap: "Função geradora de momentos da Exponencial(λ = 1). A região sombreada indica um intervalo aberto em torno de 0 onde a MGF é bem definida." -->
<!-- #| warning: false -->
<!-- #| message: false -->

<!-- library(ggplot2) -->

<!-- lambda <- 1 -->
<!-- eps    <- 0.5  # intervalo (-eps, eps) ao redor de 0 -->

<!-- # pontos apenas onde a MGF está bem definida: t < lambda -->
<!-- t  <- seq(-3, lambda - 0.01, length.out = 400) -->
<!-- mgf <- lambda / (lambda - t) -->

<!-- df <- data.frame(t = t, M = mgf) -->

<!-- ggplot(df, aes(x = t, y = M)) + -->
<!--   # faixa sombreada (-eps, eps) -->
<!--   geom_rect(aes(xmin = -eps, xmax = eps, ymin = -Inf, ymax = Inf), -->
<!--             fill = "grey80", alpha = 0.5, inherit.aes = FALSE) + -->
<!--   # curva da MGF -->
<!--   geom_line(size = 1) + -->
<!--   # assíntota em t = lambda -->
<!--   geom_vline(xintercept = lambda, linetype = "dashed") + -->
<!--   # linha vertical em t = 0 -->
<!--   geom_vline(xintercept = 0, linetype = "dotted") + -->
<!--   labs( -->
<!--     x = "t", -->
<!--     y = "M_X(t)", -->
<!--     title = "MGF da Exponencial com λ = 1", -->
<!--     subtitle = "Domínio: t < 1. A região sombreada é um intervalo (-ε, ε) em torno de 0." -->
<!--   ) + -->
<!--   coord_cartesian(ylim = c(0, 10)) + -->
<!--   theme_minimal(base_size = 14) -->
<!-- ``` -->





## Função Geradora de Momentos

Note que a **definição de função geradora de momentos** é feita independente do tipo de variável, mas a forma de encontrá-la depende se a variável for **discreta** ou **contínua**, isto é, 


- Se $X$ é discreta, 

$$M_X(t) = E(e^{tX}) = \sum_{\forall x \in S_X} \, e^{tx} p_X(x)$$



- Se $X$ é contínua, 

$$M_X(t) = E(e^{tX}) = \int_{-\infty}^{\infty} \, e^{tx} f_X(x) \, dx$$



## Função Geradora de Momentos

::: {.callout-note title="Teorema 01"}
Suponha que a função geradora de momentos de $X$ exista para $|t| < \varepsilon$, $\varepsilon > 0$. Então, $E(X^k)$ existe para $k = 1, 2, \cdots$ e temos:

$$E(X^k) = \dfrac{d^k}{dt^k} M_X(t) \Bigg|_{t=0}$$

ou seja, o $k$-ésimo momento de $X$ é igual à derivada de ordem $k$ de $M_X(t)$ avaliada em $t = 0$.
:::

**Demonstração:** Suponha que a função geradora de momentos de $X$ exista para todo $t$ tal que $|t|<\varepsilon$, com $\varepsilon>0$, isto é,

$$
M_X(t)=E(e^{tX})<\infty,\qquad |t|<\varepsilon.
$$

## Função Geradora de Momentos

Pela série de Maclaurin da função exponencial, para qualquer número real $y$ temos
$$
e^{y} = \sum_{n=0}^{\infty} \frac{y^n}{n!}
$$

Aplicando isso a $y=tX$, obtemos, para cada $t$ com $|t|<\varepsilon$,
$$
e^{tX} = \sum_{n=0}^{\infty} \frac{(tX)^n}{n!} = 1 + tX +\frac{(tX)^2}{2!} + \frac{(tX)^3}{3!} + \cdots
$$


Temos que $M_X(t)=E(e^{tX})$. Logo, admitindo ser válido permutar soma infinita e esperança, temos


## Função Geradora de Momentos

$$
M_X(t)
= E(e^{tX})
= E\left(\sum_{n=0}^{\infty} \frac{(tX)^n}{n!}\right)
= \sum_{n=0}^{\infty} \frac{t^n}{n!} E(X^n),
\qquad |t|<\varepsilon
$$


Portanto, $M_X(t)$ é dada, em uma vizinhança de $0$, por uma série de potência da forma
$$
M_X(t) = \sum_{n=0}^{\infty} a_n t^n,
\qquad \text{com } a_n = \frac{E(X^n)}{n!}
$$




## Função Geradora de Momentos


Da teoria de séries de potência, sabemos que, se
$$
M_X(t) = \sum_{n=0}^{\infty} a_n t^n,
$$
então a $k$-ésima derivada é
$$
M_X^{(k)}(t) = \sum_{n=k}^{\infty} a_n\, n(n-1)\cdots (n-k+1)\, t^{\,n-k}
$$


## Função Geradora de Momentos

Agora substituímos $t=0$:

Observe:

- Se $n > k$, aparece o fator $t^{n-k} = 0^{n-k} = 0$;  
- Então **todos** os termos com $n>k$ desaparecem;
- Só o termo com $n=k$ permanece.

O único termo sobrevivente é:

$$
a_k \, k(k-1)(k-2)\cdots 1 \, t^{\,0}
= a_k \, k!
$$

## Função Geradora de Momentos

Assim,

$$
M_X^{(k)}(0) = a_k\, k!
= \frac{E(X^k)}{k!}\,k!
= E(X^k)
$$


Logo,
$$
E(X^k) = \left.\frac{d^k}{dt^k} M_X(t)\right|_{t=0},
\qquad k=1,2,\dots
$$

o que mostra que o $k$-ésimo momento de $X$ é igual à derivada de ordem $k$ da função geradora de momentos avaliada em $t=0$.



## Função Geradora de Momentos



::: {#tip-FGM .callout-tip}
## Dica Importante!

Para qualquer variável aleatória $X$:

$$
M_X(0) = E(e^{0X}) = 1.
$$

Isso sempre deve ocorrer. Use esse fato para verificar se sua FGM está correta.
:::


## Função Geradora de Momentos

::: {.callout-note title="Exemplo 03: Distribuição de Bernoulli"}
Seja $X \sim Bernoulli(p)$. Encontre sua função geradora de momentos e a partir dela, encontre $E(X)$ e $\operatorname{Var}(X)$.
:::

. . . 

**Solução:** Por definição,

$$
M_X(t) = E(e^{tX}) = \sum_{x=0}^1 e^{tx} p^x(1-p)^{1-x}
$$

Como $X$ só assume os valores $0$ e $1$:

$$
M_X(t)
= e^{t\cdot 0} p^0(1-p)^{1-0} + e^{t\cdot 1} p^1(1-p)^{1-1}
= (1-p) + e^{t}p
= 1 - p +  e^{t} p
$$


## Função Geradora de Momentos

Portanto,

$$
\boxed{M_X(t) = 1 - p + p e^{t}, \quad t \in \mathbb{R}}
$$

Veja que pela @tip-FGM $M_X(0) = 1 - p + p e^{0} = 1 - p + p \times 1 = 1$.  Assim, 


- Primeira derivada de $M_X(t)$:

$$
M_X'(t) = \frac{d}{dt}\big(1 - p + p e^{t}\big)
= p e^{t}
$$

Avaliada em $t = 0$, temos $E(X) = M_X'(0) = p e^{0} = p$


## Função Geradora de Momentos


- Segunda derivada de $M_X(t)$:

$$
M_X''(t) = \frac{d}{dt}\big(M_X'(t)\big)
= \frac{d}{dt}(p e^{t}) = p e^{t}
$$

Avaliada em $t = 0$, temos $E(X) = M_X''(0) = p e^{0} = p$. 


Assim,


$$
\operatorname{Var}(X)
= M_X''(0) - [M_X'(0)]^2
= p - p^2
= p(1-p)
$$



## Função Geradora de Momentos

::: {.callout-note title="Exemplo 03: Distribuição Binomial"}
Seja $X \sim Binomial(n,p)$. Encontre sua função geradora de momentos e a partir dela, encontre $E(X)$ e $\operatorname{Var}(X)$.
:::

. . . 

**Solução:** Se $X \sim Binomial(n,p)$ então sua f.p. é dada por 

$$
P(X = k) = \binom{n}{k} p^k (1-p)^{n-k}, \qquad k=0,1,\dots,n
$$

Por definição,

$$
M_X(t) = E(e^{tX}) = \sum_{k=0}^{n} e^{tk} P(X = k)
= \sum_{k=0}^{n} e^{tk} \binom{n}{k} p^k (1-p)^{n-k}
$$


## Função Geradora de Momentos


Escrevendo $e^{tk} = (e^{t})^k$:

$$
M_X(t)
= \sum_{k=0}^{n} \binom{n}{k} (p e^t)^k (1-p)^{n-k}
$$

Temos que o binômio de Newton é dado por:

$$
(a+b)^n = \sum_{k=0}^{n} \binom{n}{k} a^k b^{n-k}
$$





## Função Geradora de Momentos


Assim, tomando $a = p e^{t}$ e $b = 1-p$:

$$
M_X(t) = (1-p + p e^{t})^n
$$

Portanto,

$$
\boxed{M_X(t) = (1-p + p e^{t})^n,\quad t\in\mathbb{R}}
$$


Veja que pela @tip-FGM $M_X(0) = (1-p + p e^{0})^n = (1 - p + p \times 1)^n = 1^n = 1$.   

## Função Geradora de Momentos

Sabemos que

$$
E(X) = M_X'(0)
$$

Derivada primeira em relação a $t$,

$$
M_X'(t) = \frac{d}{dt} (1-p + p e^{t})^n
= n(1-p + p e^{t})^{n-1} \cdot p e^{t}
$$

Logo,

$$
M_X'(0)
= n(1-p + p e^{0})^{n-1} \cdot p e^{0}
= n(1-p + p)^{n-1} p
= n p
$$



## Função Geradora de Momentos



Vamos agora encontrar a segunda derivada. Temos

$$
M_X'(t) = n p e^{t} (1-p + p e^{t})^{n-1}
$$

Aplicando a regra do produto:

$$
\begin{aligned}
M_X''(t)
&= \frac{d}{dt}\Big[ n p e^{t} (1-p + p e^{t})^{n-1} \Big] \\
&= n p e^{t} (n-1)(1-p + p e^{t})^{n-2} \cdot p e^{t}
 \;+\; n p e^{t} (1-p + p e^{t})^{n-1}
\end{aligned}
$$


## Função Geradora de Momentos

Avaliando em $t=0$:

- $e^{0}=1$  
- $1-p + p e^{0} = 1-p + p = 1$


Assim,

$$
\begin{aligned}
M_X''(0)
&= n p \cdot 1 \cdot (n-1) \cdot 1^{\,n-2} \cdot p \cdot 1
   \;+\; n p \cdot 1 \cdot 1^{\,n-1} \\[6pt]
&= n p (n-1)p + n p \\[6pt]
&= n p \big[(n-1)p + 1\big]
\end{aligned}
$$

## Função Geradora de Momentos

De forma que,

$$
\begin{aligned}
\operatorname{Var}(X)
&= M_X''(0) - [M_X'(0)]^2\\[6pt] 
&= n p \big[(n-1)p + 1\big] - (np)^2 \\[6pt]
&= n p \big[(n-1)p + 1 - n p\big] \\[6pt]
&= n p (1 - p)
\end{aligned}
$$





## Função Geradora de Momentos

::: {.callout-note title="Exemplo 04: Distribuição Geométrica"}
Seja $X \sim Geo(p)$. Encontre sua função geradora de momentos e a partir dela, encontre $E(X)$ e $\operatorname{Var}(X)$.
:::

. . . 

**Solução:** Seja $X \sim \text{Geo}(p)$, cuja função de probabilidade é

$$
P(X = k) = p(1-p)^{k-1}, \qquad k = 1,2,3,\dots
$$


Por definição, temos que,

$$
M_X(t)
= E(e^{tX})
= \sum_{k=1}^{\infty} e^{tk} p(1-p)^{k-1} = p \sum_{k=1}^{\infty} (e^t)^k (1-p)^{k-1}
$$



## Função Geradora de Momentos


Reorganizando,

$$
M_X(t)
= p e^t \sum_{k=1}^{\infty} \big[(1-p)e^{t}\big]^{\,k-1}
$$

A soma é uma série geométrica com razão 

$$
r=(1-p)e^{t}
$$

A série converge se e somente se:

$$
|(1-p)e^{t}| < 1
$$



## Função Geradora de Momentos

Usando a soma da série geométrica, 

$$
\sum_{k=0}^{\infty} r^k = \frac{1}{1-r},
$$

temos:

$$
M_X(t) = \frac{p e^t}{1 - (1-p)e^{t}}
$$

## Função Geradora de Momentos

Portanto,

$$
\boxed{M_X(t) = \frac{p e^{t}}{1-(1-p)e^{t}}, \quad \text{para } t < \ln\Bigg(\frac{1}{1-p}\Bigg)}
$$




Note que, pela @tip-FGM, $M_X(0) = \dfrac{p e^{0}}{1-(1-p)e^{0}} = \dfrac{p}{1-(1-p)} =  \dfrac{p}{p} =1$


. . .


Sabemos que:

$$
E(X) = M_X'(0)
$$


## Função Geradora de Momentos

Vamos então encontrar a derivada de primeira ordem de 

$$
M_X(t) = \frac{p e^{t}}{1 - (1-p)e^{t}}
$$

Use regra do quociente, temos

$$
M_X'(t)
= \frac{p e^t \big[1 - (1-p)e^{t}\big] - p e^{t}\big[-(1-p)e^{t}\big]}{\big[1 - (1-p)e^{t}\big]^2}
$$


Simplificando o numerador:

$$
p e^{t}\left[1 - (1-p)e^{t} + (1-p)e^{t}\right]
= p e^{t}
$$





## Função Geradora de Momentos

Portanto,

$$
M_X'(t) = \frac{p e^{t}}{\left[1 - (1-p)e^{t}\right]^2}
$$

Avaliando em $t=0$:

- $e^0=1$
- $1 - (1-p)e^{0} = 1-(1-p) = p$


Logo,

$$E(X) = M_X'(0)
= \frac{p}{p^2}
= \frac{1}{p}$$



## Função Geradora de Momentos


Encontrando $\operatorname{Var}(X)$, usamos,

$$
\operatorname{Var}(X) = M_X''(0) - \Big[M_X'(0)\Big]^2
$$

A derivada segunda é dada por

$$
M_X''(t)
= \frac{p e^{t}\big[1+(1-p)e^{t}\big]}{\left[1-(1-p)e^{t}\right]^3}
$$

de forma que

$$M_X''(0)
= \frac{p(1+(1-p))}{p^3}
= \frac{p(2-p)}{p^3}
= \frac{2-p}{p^2}$$


## Função Geradora de Momentos

e, portanto, 


$$\operatorname{Var}(X)
= M_X''(0) - \Big[M_X'(0)\Big]^2
= \frac{2-p}{p^2} - \left(\frac{1}{p}\right)^2
= \frac{2-p-1}{p^2}
= \frac{1-p}{p^2}$$


. . .


Veja que neste exemplo, a função geradora de momentos de $X ∼ Geo(p)$ não está definida para todo $t \in \mathbb{R}$, mas está bem definida para $t < \ln\Big(\frac{1}{1-p}\Big)$. E como $\ln\Big(\frac{1}{1-p}\Big) > 0$, a função geradora de momentos está bem definida para $t$ em uma vizinhança de zero.



## Função Geradora de Momentos

::: {.callout-note title="Exemplo 05: Distribuição de Poisson"}
Seja $X \sim Poisson(\lambda)$. Encontre sua função geradora de momentos e a partir dela, encontre $E(X)$ e $\operatorname{Var}(X)$.
:::

. . . 

**Solução:** Seja $X \sim Poisson(\lambda)$, cuja função de probabilidade é

$$
P(X = k) = \frac{e^{-\lambda}\lambda^k}{k!}, \quad k = 0,1,2,\ldots
$$

Assim, a função geradora de momentos (fgm) de $X$ é dada por,

$$
M_X(t) = \sum_{k=0}^\infty e^{tk}\,\frac{e^{-\lambda}\lambda^k}{k!}
       = e^{-\lambda} \sum_{k=0}^\infty \frac{(\lambda e^t)^k}{k!}
$$



## Função Geradora de Momentos

A série

$$
\sum_{k=0}^\infty \frac{(\lambda e^t)^k}{k!}
$$

é a expansão em série de Taylor de $e^{\lambda e^t}$. Assim,

$$
M_X(t) = e^{-\lambda} e^{\lambda e^t}
       = \exp\{\lambda(e^t - 1)\}
$$

## Função Geradora de Momentos


Portanto, a fgm de $X$ é
$$
\boxed{M_X(t) = \exp\big(\lambda(e^t - 1)\big)}
$$

Note que, pela @tip-FGM, $M_X(0) = \exp\big(\lambda(e^0 - 1)\big) = \exp\big(0\big) =  1$


. . . 


Vamos calcular as derivadas de ordem primeira e ordem segunda para calcular os respectivos momentos:


Pela regra da cadeia,

$$
M_X'(t) = \exp\big(\lambda(e^t - 1)\big)\cdot \lambda e^t
        = \lambda e^t\, M_X(t)
$$



## Função Geradora de Momentos


Logo,

$$
E(X) = M_X'(0) = \lambda e^0\, M_X(0) = \lambda \cdot 1 \cdot 1 = \lambda
$$

. . .


A segunda derivada de $M_X(t)$ é dada por:

$$
\begin{aligned}
M_X''(t) 
&= \frac{d}{dt}\big[\lambda e^t\, M_X(t)\big] \\
&= \lambda e^t\, M_X(t) + \lambda e^t\, M_X'(t) \\
&= \lambda e^t\, M_X(t) + \lambda e^t\,[\lambda e^t\, M_X(t)] \\
&= \lambda e^t\, M_X(t)\,\big[1 + \lambda e^t\big]
\end{aligned}
$$



## Função Geradora de Momentos


Agora, avaliando em $t=0$:

$$
\begin{aligned}
M_X''(0) = \lambda e^0\, M_X(0)\,\big[1 + \lambda e^0\big] = \lambda \cdot 1 \cdot 1 \cdot (1+\lambda) = \lambda(1+\lambda)
\end{aligned}
$$

Portanto,

$$
E(X^2) = M_X''(0) = \lambda(1+\lambda)
$$


Assim,

$$
\begin{aligned}
\text{Var}(X) 
&= M_X''(0) - \Big[M_X'(0)\Big]^2 \\
&= \lambda(1+\lambda) - \lambda^2 \\
&= \lambda
\end{aligned}
$$



## Função Geradora de Momentos

::: {.callout-note title="Exemplo 06: Distribuição Exponencial"}
Seja $X \sim exp(\lambda)$. Encontre sua função geradora de momentos e a partir dela, encontre $E(X)$ e $\operatorname{Var}(X)$.
:::

. . . 

**Solução:** Seja $X \sim exp(\lambda)$, cuja função densidade é

$$
f(x) = \lambda e^{-\lambda x}, \quad x > 0
$$


Neste caso, a função geradora de momentos (fgm) de $X$ é definida por
$$
M_X(t) = E(e^{tX}) = \int_0^\infty e^{tx}\,\lambda e^{-\lambda x}\,dx = \int_0^\infty \lambda e^{-(\lambda - t)x} \, dx
$$

válida para $t < \lambda$.



## Função Geradora de Momentos


Assim, como

$$
\int_0^\infty \lambda e^{-(\lambda - t)x} =  \lambda \frac{e^{-(\lambda - t)x}}{-(\lambda - t)} \Bigg|_0^{\infty} = \frac{\lambda}{\lambda - t}, \,\,\,\, \lambda - t > 0
$$

Logo,

$$
\boxed{M_X(t) = \frac{\lambda}{\lambda - t}, \qquad t < \lambda}
$$


Note que, pela @tip-FGM, $ M_X(0) = \frac{\lambda}{\lambda - 0} =  1$


## Função Geradora de Momentos


Derivando,

$$
M_X'(t) = \frac{\lambda}{(\lambda - t)^2}
$$

Portanto,

$$
E(X) = M_X'(0) = \frac{\lambda}{\lambda^2} = \frac{1}{\lambda}
$$


## Função Geradora de Momentos


A segunda derivada é dada por,


$$
M_X''(t) = \frac{2\lambda}{(\lambda - t)^3}
$$

Logo,

$$
E(X^2) = M_X''(0) = \frac{2\lambda}{\lambda^3} = \frac{2}{\lambda^2}
$$


De forma que,


$$
\text{Var}(X)
= E(X^2) - \Big[E(X) \Big]^2
= \frac{2}{\lambda^2} - \frac{1}{\lambda^2}
= \frac{1}{\lambda^2}
$$




## Função Geradora de Momentos

::: {.callout-note title="Exemplo 07: Distribuição Gamma"}
Seja $X \sim Gamma(\alpha, \lambda)$. Encontre sua função geradora de momentos.
:::


. . . 

**Solução:** Seja $X \sim Gamma(\alpha, \lambda)$, cuja função densidade é

$$
f(x) = \frac{\lambda^\alpha}{\Gamma(\alpha)}x^{\alpha-1}e^{-\lambda x},
\qquad x>0
$$


A função geradora de momentos é


$$
M_X(t)
= \int_0^\infty e^{tx}
\frac{\lambda^\alpha}{\Gamma(\alpha)}x^{\alpha-1}e^{-\lambda x}\,dx
= \frac{\lambda^\alpha}{\Gamma(\alpha)}
\int_0^\infty x^{\alpha-1}e^{-(\lambda - t)x}\,dx
$$



## Função Geradora de Momentos


Usamos agora o resultado da função Gamma:

$$
\int_0^\infty x^{\alpha-1}e^{-bx}\,dx
= \frac{\Gamma(\alpha)}{b^\alpha}, \qquad b>0
$$

Aqui, $b = \lambda - t$ (precisamos de $\lambda - t > 0$, isto é, $t<\lambda$). Logo,

$$
\int_0^\infty x^{\alpha-1}e^{-(\lambda - t)x}\,dx
= \frac{\Gamma(\alpha)}{(\lambda - t)^\alpha}
$$



Portanto,

$$
M_X(t)
= \frac{\lambda^\alpha}{\Gamma(\alpha)}
\cdot
\frac{\Gamma(\alpha)}{(\lambda - t)^\alpha}
= \left(\frac{\lambda}{\lambda - t}\right)^\alpha,
\qquad t<\lambda
$$






## Função Geradora de Momentos

::: {.callout-note title="Exemplo 07: Distribuição Normal Padrão"}
Seja $X \sim N(0, 1)$. Encontre sua função geradora de momentos.
:::


. . . 

**Solução:** Seja $X \sim N(0, 1)$, cuja função densidade é

$$
f(x) = \frac{1}{\sqrt{2\pi}} e^{-x^2/2}, \qquad x \in \mathbb{R}
$$


Queremos encontrar a função geradora de momentos (fgm) dada por

$$
M_X(t)
= \int_{-\infty}^{\infty} e^{tx}\,\frac{1}{\sqrt{2\pi}} e^{-x^2/2}\,dx
= \frac{1}{\sqrt{2\pi}}\int_{-\infty}^{\infty}
\exp\Big(tx - \frac{x^2}{2}\Big)\,dx
$$



## Função Geradora de Momentos

Vamos usar a técnica de completar quadrados no expoente,
$$
\begin{aligned}
tx - \frac{x^2}{2}
&= -\frac{1}{2}\big(x^2 - 2tx\big) = -\frac{1}{2}\big(x^2 - 2tx + t^2 - t^2\big) \\
&= -\frac{1}{2}\big[(x - t)^2 - t^2\big] = -\frac{(x - t)^2}{2} + \frac{t^2}{2}
\end{aligned}
$$


Assim, 

$$\small
M_X(t)
= \frac{1}{\sqrt{2\pi}}\int_{-\infty}^{\infty}
\exp\!\left(-\frac{(x - t)^2}{2} + \frac{t^2}{2}\right)\,dx
= \exp\!\left(\frac{t^2}{2}\right)
\frac{1}{\sqrt{2\pi}}\int_{-\infty}^{\infty}
\exp\!\left(-\frac{(x - t)^2}{2}\right)\,dx
$$




## Função Geradora de Momentos


A integral

$$
\frac{1}{\sqrt{2\pi}}\int_{-\infty}^{\infty}
\exp\!\left(-\frac{(x - t)^2}{2}\right)\,dx
$$
é a área total sob a curva de uma normal $N(t,1)$, e portanto é igual a $1$.

Logo,
$$
\boxed{M_X(t) = \exp\!\left(\frac{t^2}{2}\right), \qquad t \in \mathbb{R}}
$$




## Função Geradora de Momentos

::: {.callout-note title="Proposição 01: Transformações Lineares"}
Seja $X$ uma variável aleatória com função geradora de momentos $M_X$. Seja $Y = aX + b$. Então, a função geradora de momentos de $Y$ é dada por

$$M_Y(t) = e^{bt} M_X(at)$$ 
:::


. . .


**Demonstração:**


$$M_Y(t) = E(e^{tY}) = E(e^{t(aX+b)}) = E\Big(e^{atX} e^{bt} \Big) = e^{bt} E\Big(e^{atX}\Big) =  e^{bt} M_X(at)$$ 





## Função Geradora de Momentos

::: {.callout-note title="Exemplo 07: Distribuição Normal Padrão"}
Seja $X \sim N(\mu, \sigma^2)$. Encontre sua função geradora de momentos.
:::


. . . 

**Solução:** Se $X \sim N(\mu, \sigma^2)$, então $X = \sigma Z + \mu$, em que $Z\sim N(0,1)$, com função geradora de momentos dada por $M_X(t) = \exp\!\left(\frac{t^2}{2}\right)$. Assim,

$$M_X(t) = e^{\mu t} M_X(\sigma t) = e^{\mu t} \exp\!\left(\frac{(\sigma t)^2}{2}\right) = e^{\mu t} \exp\!\left(\frac{\sigma^2 t^2}{2}\right)$$


## Função Geradora de Momentos

Logo, 


Se $X \sim N(\mu, \sigma^2)$, então 


$$\boxed{M_X(t)= \exp \Bigg(\mu t + \frac{\sigma^2 t^2}{2} \Bigg)}$$





## Função Geradora de Momentos

::: {.callout-note title="Teorema 02"}
Se duas variáveis aleatórias têm funções geradoras de momentos que existem, e são iguais, então elas têm a mesma função de distribuição.
:::


. . .


A demonstração desse Teorema será omitida, pois ela usa conceitos não estudados neste curso. Veja que o Teorema 02 nos mostra que se duas variáveis aleatórias tem mesma função geradora de momentos então estas variáveis aleatórias são identicamente distribuídas. 


Isso significa que podemos identificar a distribuição de uma variável aleatória a partir da sua função geradora de momentos, assim como identificamos a distribuição da
variável aleatória a partir da sua função de distribuição, função densidade ou função de probabilidade.



## Função Geradora de Momentos


::: {.callout-note title="Exemplo 08"}
Seja $X \sim Gamma(\alpha, \lambda)$. Considere também $Y = cX,\,\, c\in \mathbb{R}$. Mostre, a partir da função geradora de momentos, que $Y \sim Gamma(\alpha, \lambda/c)$.
:::


. . . 

**Solução:** Por definição,

$$
M_Y(t) = E(e^{tY}) = E(e^{t(cX)}) = E(e^{(ct)X})
$$

Logo,

$$
M_Y(t) = M_X(ct) = \left(\frac{\lambda}{\lambda - ct}\right)^{\alpha},
\qquad ct < \lambda \;\; \Rightarrow \;\; t < \frac{\lambda}{c}
$$



## Função Geradora de Momentos


ou seja,


$$
M_Y(t)
= \left(\frac{\lambda}{\lambda - ct}\right)^{\alpha}
= \left(\frac{\lambda/c}{(\lambda/c) - t}\right)^{\alpha},
\qquad t < \frac{\lambda}{c}.
$$



Veja que trata-se da função geradora de momentos de uma variável aleatória com distribuição $Gamma(\alpha,\lambda/c)$. Logo, $Y \sim Gamma(\alpha,\lambda/c)$.



## Função Geradora de Momentos

::: {.callout-note title="Teorema 03: Soma de variáveis independentes"}
Sejam $X_1, X_2, \cdots, X_n$ variáveis aleatórias independentes e funções geradoras de momentos, respectivamente, iguais a $M_{X_j}(t), \,\, j = 1, 2, \cdots, n$ para $t$ em alguma vizinhança de zero. Se $Y = X_1 + X_2 + \cdots + X_n$, então a função geradora de momentos de $Y$ existe e é dada por:

$$M_Y(t) =  \prod_{j = 1}^n M_{X_j}(t)$$
:::


. . .


**Demonstração:** Pela definição, temos

$$
\begin{aligned}
M_Y(t) &= E(e^{t(X_1 + X_2 + \cdots + X_n)}) \\ &= E(e^{tX_1}e^{tX_2} \cdots e^{tX_n}) \\ &= E(e^{tX_1})E(e^{tX_2})\cdots E(e^{tX_n})
\end{aligned}
$$
e daí segue o resultado desejado. 


## Função Geradora de Momentos


::: {.callout-note title="Exemplo 09"}
Sejam $X_1, X_2, \cdots, X_n$ variáveis aleatórias independentes com distribuição Bernoulli de parâmetro $p$. Se $Y = X_1 + X_2 + \cdots + X_n$, mostre, a partir da função geradora de momentos, que $Y =  \sim Binomial(n,p)$.
:::


. . .


**Solução:** Temos que se $X_1, X_2, \cdots, X_n$ são variáveis aleatórias independentes com distribuição Bernoulli de parâmetro $p$, então

$$
M_{X_j}(t) = 1 - p + p e^{t}, \quad j = 1,2, \cdots n
$$

## Função Geradora de Momentos


Então, pelo **Teorema 03**, temos


$$
M_Y(t) = \prod_{j = 1}^n M_{X_j}(t) = \big(1 - p + p e^{t}\big)^n
$$

que corresponde à função geradora de momentos de uma variável aleatória Binomial com parâmetros $n$ e $p$. Logo, temos que $X_1 + X_2 + \cdots + X_n \sim Binomial(n,p)$.


## Função Característica

A **função característica** é uma das ferramentas mais importantes da Teoria das Probabilidades.  


. . .


Ela desempenha um papel similar ao da função geradora de momentos (fgm), mas com vantagens significativas: **sempre existe**, determina unicamente a distribuição e facilita o estudo de somas de variáveis aleatórias.


. . .

::: {.callout-note title="Definição 03: Variáveis Aleatórias Complexas"}
Dizemos que uma variável aleatória $X$ é complexa, se pode ser escrita como

$$X = X^a + i \,X^b$$

em que $i = \sqrt{-1}$ e $X^a$ e $X^b$ são variáveis aleatórias reais.
:::




## Função Característica

Para o valor esperado de $X$, no caso complexo, exigimos que as esperanças das duas partes sejam finitas. Assim, temos:

$$E(X) = E(X^a) + i \, E(X^b)$$

em que $E(X^a)$ e $E(X^b)$ são finitas.


. . .



Para efeitos práticos, podemos trabalhar funções complexas como se estivéssemos com funções reais.


. . .


As propriedades usuais de números complexos serão usadas. Sendo $z = a + i\, b$, denotaremos por $|z|$ seu módulo e por $\bar{z}$ seu conjugado.



## Função Característica


::: {.callout-note title="Definição 04: Função Característica"}
A função característica de uma variável aleatória $X$ é definida por
$$
\varphi_X(t) = E\!\left(e^{itX}\right) = E\Big[\cos(tX)\Big] + i\, E\Big[\, \text{sen}(tX)\Big],
\qquad t \in \mathbb{R},
$$
onde $i = \sqrt{-1}.$
:::



## Função Característica

### Observações importantes

1. **Sempre existe**  
   Diferentemente da fgm, que pode divergir,  
   $$
   |e^{itX}| = 1 \quad \Rightarrow \quad |E(e^{itX})| \le 1.
   $$
   Portanto, $\varphi_X(t)$ está sempre bem definida.

2. **Determina unicamente a distribuição**  
   Se $\varphi_X(t) = \varphi_Y(t)$ para todo $t$, então  
   $$
   X \stackrel{d}{=} Y.
   $$  
   (versão “complexa” do teorema da unicidade da fgm)
   
   
## Função Característica

### Observações importantes



3. **É sempre contínua**  
   Aliás, é uniformemente contínua em toda a reta real.

4. **Normalização**  
   Como $E(e^{it\cdot 0}) = 1$, temos
   $$
   \varphi_X(0) = 1
   $$
   
   
## Função Característica


### Relação com Momentos

Se $X$ possui momentos até ordem $k$, então:


$$
\varphi_X^{(n)}(0) = i^n E(X^n),
\qquad n = 1,2,\dots,k.
$$




Em particular:


- $E(X) = -i\,\varphi_X'(0)$  
- $\text{Var}(X) = -\varphi_X''(0) - \left[\varphi_X'(0)\right]^2$




## Função Característica

### Propriedade fundamental: soma de variáveis independentes

Se $X$ e $Y$ são independentes:


$$
\varphi_{X+Y}(t)
= E(e^{it(X+Y)})
= E(e^{itX})\,E(e^{itY})
= \varphi_X(t)\,\varphi_Y(t).
$$

Esse resultado é crucial, por exemplo, para demonstrar o **Teorema Central do Limite**.



## Função Característica



::: {.callout-note title="Exemplo 10"}
Considere uma variável aleatória $X$ com distribuição Uniforme Contínua no intervalo $[a,b]$. Encontre sua função característica.
:::


. . .


**Solução:** Considere $X \sim \text{U}(a,b)$, com $a<b$. Temos que, sua função densidade é dada por


$$
f(x) = \frac{1}{b-a}, \qquad a \le x \le b
$$



Assim, a função característica de $X$ é


$$
\varphi_X(t) = E(e^{itX})
= \int_{-\infty}^{\infty} e^{itx} f(x)\,dx
$$




## Função Característica

Como $f(x)$ é não nula apenas em $[a,b]$:


$$
\varphi_X(t)
= \int_a^b e^{itx}\,\frac{1}{b-a}\,dx
= \frac{1}{b-a} \int_a^b e^{itx}\,dx.
$$

mudança de variável: $u = itx \Rightarrow du = it \, dx$ ou $dx = \frac{1}{it} du$. Quando $x = a \Rightarrow u = ita$, quando $x = b \Rightarrow u = itb$. Assim, 


$$
\varphi_X(t)
= \frac{1}{b-a}
\left[\frac{1}{it}e^{u}\right]_{ita}^{itb}
= \frac{e^{itb} - e^{ita}}{it \big(b-a\big)},
\qquad t \ne 0
$$



## Função Característica



::: {.callout-note title="Exemplo 11"}
Seja $X \sim Gamma(\alpha, \lambda)$. Encontre sua função característica.
:::


. . .


**Solução:** Se $X \sim Gamma(\alpha, \lambda)$. Temos que sua função densidade é


$$
f(x) = \frac{\lambda^\alpha}{\Gamma(\alpha)}x^{\alpha-1}e^{-\lambda x},
\qquad x>0
$$


Assim, 

A função característica de $X$ é
$$
\varphi_X(t) = E(e^{itX})
= \int_0^\infty e^{itx} f(x)\,dx
$$



## Função Característica

Substituindo a densidade:


$$
\varphi_X(t)
= \int_0^\infty e^{itx}
\frac{\lambda^\alpha}{\Gamma(\alpha)}x^{\alpha-1}e^{-\lambda x}\,dx
= \frac{\lambda^\alpha}{\Gamma(\alpha)}
\int_0^\infty x^{\alpha-1} e^{-(\lambda - it)x}\,dx
$$


Sabemos que,


$$
\int_0^\infty x^{\alpha-1}e^{-bx}\,dx = \frac{\Gamma(\alpha)}{b^\alpha}
$$

Assim,



$$
\varphi_X(t)
= \frac{\lambda^\alpha}{\Gamma(\alpha)}
\cdot
\frac{\Gamma(\alpha)}{(\lambda - it)^\alpha}
= \left(\frac{\lambda}{\lambda - it}\right)^\alpha
$$



## Função Característica

### Observações importantes


- A função característica **generaliza** a fgm para o plano complexo.

- Sempre que a fgm existir, vale:
  $$
  \varphi_X(t) = M_X(it)
  $$
  
- Ambas determinam a distribuição, mas a função característica é **mais poderosa**, pois **sempre existe** e se comporta melhor em operações como soma de variáveis independentes.



## Função Característica



::: {.callout-note title="Exemplo 11"}
Seja $X \sim Binomial(n, p)$. Encontre sua função característica.
:::


. . .


**Solução:** Se $X \sim Binomial(n, p)$, sua função geradora de momentos é dada por

$$
M_X(t) = (1-p + p e^{t})^n
$$


Portanto, segue que

$$
\varphi_X(t) = M_X(it) = (1-p + p e^{it})^n
$$



## Função Característica



::: {.callout-note title="Exemplo 12"}
Seja $X \sim N(\mu, \sigma^2)$. Encontre sua função característica.
:::


. . .


**Solução:** Se $X \sim N(\mu, \sigma^2)$, sua função geradora de momentos é dada por


$$M_X(t)= \exp \Bigg(\mu t + \frac{\sigma^2 t^2}{2} \Bigg)$$


Portanto, segue que

$$
\varphi_X(t) = M_X(it) = \exp \Bigg(it\mu - \frac{\sigma^2 t^2}{2} \Bigg)
$$
