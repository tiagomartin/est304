[
  {
    "objectID": "programacao/semana-6.html",
    "href": "programacao/semana-6.html",
    "title": "Semana 06",
    "section": "",
    "text": "Outras Distribuições Contínuas"
  },
  {
    "objectID": "programacao/semana-6.html#slides",
    "href": "programacao/semana-6.html#slides",
    "title": "Semana 06",
    "section": "",
    "text": "Outras Distribuições Contínuas"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Probabilidade II",
    "section": "",
    "text": "Página dedicada à disciplina EST304 - Probabilidade I\n\n\nA disciplina Probabilidade II aprofunda o estudo de modelos probabilísticos contínuos e suas aplicações formais, desenvolvendo ferramentas essenciais para análise rigorosa de incerteza em contextos mais gerais. São abordadas as principais distribuições contínuas de probabilidade, os métodos de transformação de variáveis aleatórias (particularmente no caso unidimensional) e o uso de funções geradoras, com destaque para a Função Geradora de Momentos e a Função Característica, como instrumentos teóricos potentes para derivar propriedades e resultados assintóticos. A disciplina também introduz a avaliação de integrais em múltiplas dimensões, o formalismo de vetores aleatórios contínuos, bem como a dedução e interpretação de distribuições marginais e condicionais, estabelecendo assim a base matemática necessária para estudos posteriores em inferência estatística, teoria assintótica e modelos estocásticos.\n\n\nEmenta\nDistribuições Contínuas; Transformações de Variáveis Aleatórias Unidimensionais; Função Geradora de Momentos, Função Característica; Integrais Multiplas; Vetores Aleatórios; Distribuições Marginais e Condicionais.\n\n\nConteúdo Programático\n\nOutras Distribuições Contínuas: Gama, Qui-Quadrado, Beta. Weibull, Log-Normal e Exponencial Dupla. Distribuição de Valores Extremos.\nTransformação de Variáveis Unidimensionais: Caso Discreto. Caso Contínuo. A Transformação Integral. Aplicações: Geração de Amostras Aleatórias.\nFunção Geradora de Momentos: Função Característica.\nIntegral Dupla e Tripla: Volumes como Integrais Interligadas. Integrais Duplas e Integrais Iteradas. Integrais Triplas.Mudança de Variáveis em Integrais Múltiplas. Jacobianos.\nVetores Aleatórios: Função de Distribuição conjunta Distribuições Marginais. Função de Densidade Conjunta.Distribuições Condicionais. Independência Estocástica.\n\n\n\nHorário de Aulas\nNeste semestre, as aulas da disciplina serão ministradas no LABEST II.\n\n\n\nDia\nHorário\nLocal\n\n\n\n\nTerça-feira\n19:00 - 20:40\nLABEST II\n\n\nQuinta-feira\n21:00 - 22:40\nLABEST II\n\n\n\n\n\nAvaliações\nA nota final do semestre será computada da seguinte forma:\n\nDuas provas valendo 70 pontos, distribuídos da seguinte forma:\n\n\\(1^a\\) Prova: 35 pontos a ser realizada em 29/01/26\n\\(2^a\\) Prova: 35 pontos a ser realizada em 26/02/26\n\nListas de exercícios a serem entregues ao longo do semestre no valor de 30 pontos.\n\n\n\n\n\n\n\n\n\nReferências Bibliográficas\n\nCasella, George, and Roger L. Berger. 2011. Inferência Estatística. Cengage Learning.\n\n\nCosta, Giovani Glaucio de Oliveira. 2012. Curso de Estatística Inferencial e Probabilidade: Teoria e Prática. 1st ed. São Paulo: Editora Atlas.\n\n\nDantas, C. A. B. 1997. Probabilidade: Um Curso Introdutório. Editora USP.\n\n\nJohnson, N. L., Samuel Kotz, and N. Balakrishnan. 1994. Continuous Univariate Distributions, Volume 1. Wiley-Interscience.\n\n\n———. 1995. Continuous Univariate Distributions, Volume 2. Wiley-Interscience.\n\n\nMeyer, P. L. 2009. Probabilidade: Aplicações à Estatística. 2nd ed. Editora LTC.\n\n\nMood, A., F. Graybill, and D. Boes. 1974. Introduction to the Theory of Statistics. 3rd ed. Singapore: McGraw-Hill.\n\n\nRoss, Sheldon. 2010. Probabilidade: Um Curso Moderno Com Aplicações. 8th ed. Bookman.\n\n\nRoss, Sheldon M. 2001. A First Course in Probability. 6th ed. Prentice Hall.\n\n\n———. 2006. Introduction to Probability Models. 9th ed. Academic Press."
  },
  {
    "objectID": "aulas.html",
    "href": "aulas.html",
    "title": "Aulas",
    "section": "",
    "text": "Outras Distribuições Contínuas\nTransformação de Variáveis Unidimensionais\nFunção Geradora de Momentos\nIntegral Dupla e Tripla\nVetores Aleatórios",
    "crumbs": [
      "Aulas"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "aulas/outras_distr_continuas/outras_dist_continuas.html#função-gamma",
    "href": "aulas/outras_distr_continuas/outras_dist_continuas.html#função-gamma",
    "title": "Outras Distribuições Contínuas",
    "section": "Função Gamma",
    "text": "Função Gamma\nA função Gamma é uma das funções especiais mais importantes em probabilidade e estatística. Ela aparece naturalmente em várias distribuições contínuas fundamentais:\n\nDistribuição Gamma\n\n\\[f(x |\\alpha, \\lambda) = \\dfrac{\\lambda e^{-\\lambda x}(\\lambda x)^{\\alpha-1}}{\\Gamma(\\alpha)}\\]\n\nDistribuição Qui-quadrado\n\n\\[f(x | n) = \\frac{1}{2^{n/2} \\Gamma(n/2)} x^{\\frac{n}{2} -1} e^{- \\frac{x}{2}}\\]"
  },
  {
    "objectID": "aulas/outras_distr_continuas/outras_dist_continuas.html#função-gamma-1",
    "href": "aulas/outras_distr_continuas/outras_dist_continuas.html#função-gamma-1",
    "title": "Outras Distribuições Contínuas",
    "section": "Função Gamma",
    "text": "Função Gamma\n\nDistribuição Beta\n\n\\[f(x |\\alpha, \\beta) = \\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}x^{\\alpha-1} (1-x)^{\\beta-1} \\]\n\nDistribuição t de Student\n\n\\[ f(x |\\nu) = \\frac{\\Gamma [(\\nu+1)/2]}{\\Gamma (\\nu/2)} \\frac{1}{\\sqrt{\\nu\\pi}} \\left( 1 + \\frac{x^2}{\\nu} \\right)^{-(\\nu+1)/2} \\]"
  },
  {
    "objectID": "aulas/outras_distr_continuas/outras_dist_continuas.html#função-gamma-2",
    "href": "aulas/outras_distr_continuas/outras_dist_continuas.html#função-gamma-2",
    "title": "Outras Distribuições Contínuas",
    "section": "Função Gamma",
    "text": "Função Gamma\n\nDistribuição F de Snedecor\n\n\\[ f(x |\\nu_1, \\nu_2) = \\frac{\\Gamma [(\\nu_1+\\nu_2)/2] }{\\Gamma \\left( \\frac{\\nu_1}{2} \\right) \\Gamma \\left( \\frac{\\nu_2}{2} \\right) } \\left( \\frac{\\nu_1}{\\nu_2} \\right)^{\\nu_1/2} \\frac{x^{(\\nu_1-2)/2}}{[1+(\\nu_1/\\nu_2)x]^{(\\nu_1+\\nu_2)/2}} \\]"
  },
  {
    "objectID": "aulas/outras_distr_continuas/outras_dist_continuas.html#função-gamma-3",
    "href": "aulas/outras_distr_continuas/outras_dist_continuas.html#função-gamma-3",
    "title": "Outras Distribuições Contínuas",
    "section": "Função Gamma",
    "text": "Função Gamma\nDefinimos a função gamma como\n\\[\\Gamma(\\alpha) = \\int_{0}^{\\infty} e^{-y} \\, y^{\\alpha-1} \\, dy, \\,\\,\\,\\,\\,\\,\\,\\ \\alpha &gt; 0\\]\nVale destacar os seguintes resultados:\n\n\\(\\Gamma(\\alpha) = (\\alpha - 1)\\Gamma(\\alpha - 1)\\);\n\\(\\Gamma(n) = (n-1)!\\), \\(n\\) inteiro positivo;\n\\(\\Gamma(1/2) = \\sqrt{(\\pi)}\\)"
  },
  {
    "objectID": "aulas/outras_distr_continuas/outras_dist_continuas.html#função-gamma-4",
    "href": "aulas/outras_distr_continuas/outras_dist_continuas.html#função-gamma-4",
    "title": "Outras Distribuições Contínuas",
    "section": "Função Gamma",
    "text": "Função Gamma\nDemonstração\n\nResolvendo a integral por partes, fazendo \\(u = y^{\\alpha - 1}\\) e \\(dv = e^{-y} dy\\), temos\n\n\\[\n\\begin{eqnarray}\n\\Gamma(\\alpha) &=& - e^{-y} y^{\\alpha-1}\\Big|_0^\\infty + \\int_0^\\infty e^{-y} (\\alpha - 1) y^{\\alpha-2} \\, dy \\\\ &=& (\\alpha - 1)\\int_0^\\infty e^{-y} y^{\\alpha-2} \\, dy \\\\ &=& (\\alpha - 1)\\Gamma(\\alpha - 1)\n\\end{eqnarray}\n\\]"
  },
  {
    "objectID": "aulas/outras_distr_continuas/outras_dist_continuas.html#função-gamma-5",
    "href": "aulas/outras_distr_continuas/outras_dist_continuas.html#função-gamma-5",
    "title": "Outras Distribuições Contínuas",
    "section": "Função Gamma",
    "text": "Função Gamma\n\nPara valores inteiros positivos de \\(\\alpha\\), por exemplo \\(\\alpha = n\\), temos\n\n\\[\n\\begin{eqnarray}\n\\Gamma(n) &=& (n-1)\\Gamma(n-1) \\\\ &=& (n-1)(n-2)\\Gamma(n-2) \\\\ &=& \\cdots \\\\ &=& (n-1)(n-2)\\cdots 3\\cdot 2\\cdot \\Gamma(1)\n\\end{eqnarray}\n\\]\nComo\n\\[\n\\begin{eqnarray}\n\\Gamma(1) &=& \\int_{0}^{\\infty} e^{-y} \\, y^{1-1} \\, dy = \\int_{0}^{\\infty} e^{-y} dy = \\lim_{t \\to +\\infty} \\int_{0}^{t} e^{-t} dt = \\lim_{t \\to +\\infty} (-e^{-t} + e^0) = 1\n\\end{eqnarray}\n\\]\ntemos que \\(\\Gamma(n) = (n-1)!\\)."
  },
  {
    "objectID": "aulas/outras_distr_continuas/outras_dist_continuas.html#função-gamma-6",
    "href": "aulas/outras_distr_continuas/outras_dist_continuas.html#função-gamma-6",
    "title": "Outras Distribuições Contínuas",
    "section": "Função Gamma",
    "text": "Função Gamma\n\nUma forma alternativa de definir a função gamma, às vezes útil, é dada por:\n\n\\[ \\Gamma(\\alpha) = 2\\int_0^\\infty e^{-y^2} y^{2\\alpha-1} dy\\] Temos que \\(\\Gamma(\\alpha) = \\int_{0}^{\\infty} e^{-y} \\, y^{\\alpha-1} \\, dy\\). Fazendo \\(y = u^2\\), temos que \\(dy = 2udu\\). Assim,\n\\[\n\\begin{eqnarray}\n\\Gamma(\\alpha) &=& \\int_{0}^{\\infty} e^{-y} \\, y^{\\alpha-1} \\, dy = \\int_{0}^{\\infty} e^{-u^2} \\, u^{2\\alpha-2} \\, 2udu = 2\\int_{0}^{\\infty} e^{-u^2}u^{2\\alpha-1}du\n\\end{eqnarray}\n\\]\nSe provarmos que \\(\\left[ \\Gamma\\left( \\frac{1}{2}\\right) \\right]^2 = \\pi\\), então segue que \\(\\Gamma\\left( \\frac{1}{2}\\right) = \\sqrt{\\pi}\\)."
  },
  {
    "objectID": "aulas/outras_distr_continuas/outras_dist_continuas.html#função-gamma-7",
    "href": "aulas/outras_distr_continuas/outras_dist_continuas.html#função-gamma-7",
    "title": "Outras Distribuições Contínuas",
    "section": "Função Gamma",
    "text": "Função Gamma\nPartindo de:\n\\[ \\left[ \\Gamma(\\alpha) \\right]^2 = 2\\int_0^\\infty x^{2\\alpha-1}e^{-x^2} dx\\quad 2\\int_0^\\infty y^{2\\alpha-1}e^{-y^2} dy\\]\no que resulta,\n\\[ \\left[ \\Gamma(\\alpha) \\right]^2 = 4\\int_0^\\infty\\int_0^\\infty x^{2\\alpha-1}  y^{2\\alpha-1} e^{-(x^2+y^2)} dx dy\\]"
  },
  {
    "objectID": "aulas/outras_distr_continuas/outras_dist_continuas.html#função-gamma-8",
    "href": "aulas/outras_distr_continuas/outras_dist_continuas.html#função-gamma-8",
    "title": "Outras Distribuições Contínuas",
    "section": "Função Gamma",
    "text": "Função Gamma\nTomando \\(\\alpha = \\dfrac{1}{2}\\), temos \\(2 \\alpha - 1 = 0\\) e assim,\n\\[\n\\begin{eqnarray}\n\\displaystyle \\left[ \\Gamma \\left({\\frac {1}{2}}\\right) \\right]^2 = 4\\int_0^\\infty\\int_0^\\infty  e^{-(x^2+y^2)} dx dy\n\\end{eqnarray}\n\\]\n\nPara resolvermos esta integral dupla, podemos recorrer à técnica de coordenadas polares, na qual pontos \\((x,y)\\) são referenciados no sitema de coordenadas \\((r,\\theta)\\), sendo não negativo e \\(\\theta\\) variando de 0 a \\(2\\pi\\).\n\n\nAssim, tomamos \\(x=r \\cos\\theta\\) e \\(y=r \\text{ sen}\\,\\,\\theta\\), de forma que,\n\\[x^2+y^2 = r^2 \\cos^2\\theta +r^2 \\text{ sen}^2\\theta = r^2\\]"
  },
  {
    "objectID": "aulas/outras_distr_continuas/outras_dist_continuas.html#função-gamma-9",
    "href": "aulas/outras_distr_continuas/outras_dist_continuas.html#função-gamma-9",
    "title": "Outras Distribuições Contínuas",
    "section": "Função Gamma",
    "text": "Função Gamma\nA unidade infinitesimal de área \\(dx\\text{ }dy\\) corresponde à unidade infinitesimal de área \\(r\\text{ }dr\\text{ }d\\theta\\) no sistema de coordenadas polares.\n\nAlém disso, integrar com tanto como variando de zero a infinito corresponde a integrar no primeiro quadrante, que, no sistema de coordenadas polares, consiste em integrar com \\(\\theta\\) variando de 0 a \\(\\pi /2\\) e variando de 0 a infinito.\n\n\nAssim,\n\\[\n\\small\n\\begin{eqnarray}\n\\left[ \\Gamma\\left( \\frac{1}{2}\\right) \\right]^2 &=& 4\\int_0^{\\pi/2}\\int_0^\\infty e^{-r^2} r\\text{ } dr d\\theta = 4\\int_0^{\\pi/2} d\\theta \\int_0^\\infty e^{-r^2} r\\text{ } dr = 4\\int_0^{\\pi/2} d\\theta  \\left[ -\\frac{e^{-r^2}}{2} \\right]_0^\\infty \\\\ &=& 4\\int_0^{\\pi/2} d\\theta \\left( \\frac{1}{2} \\right)  = 2\\int_0^{\\pi/2}1\\text{ } d\\theta   = 2 \\left. \\theta \\right|_0^{\\pi/2} = \\pi\n\\end{eqnarray}\n\\]"
  },
  {
    "objectID": "aulas/outras_distr_continuas/outras_dist_continuas.html#função-gamma-10",
    "href": "aulas/outras_distr_continuas/outras_dist_continuas.html#função-gamma-10",
    "title": "Outras Distribuições Contínuas",
    "section": "Função Gamma",
    "text": "Função Gamma"
  },
  {
    "objectID": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-gamma",
    "href": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-gamma",
    "title": "Outras Distribuições Contínuas",
    "section": "Distribuição Gamma",
    "text": "Distribuição Gamma\nDizemos que \\(X\\) segue uma distribuição Gamma com parâmetros \\(\\alpha &gt; 0\\) e \\(\\lambda &gt; 0\\) se sua função densidade é dada por\n\\[\nf(x | \\alpha, \\lambda)=\n\\begin{cases}\n\\dfrac{\\lambda e^{-\\lambda x}(\\lambda x)^{\\alpha-1}}{\\Gamma(\\alpha)}, & x\\ge0\\\\[6pt]\n0, & x&lt;0\n\\end{cases}\n\\]\nNotação: \\(X \\sim Gamma(\\alpha, \\lambda)\\). Nessa parametrização:\n\n\\(\\alpha\\) = parâmetro de forma\n\\(\\dfrac{1}{\\lambda}\\) = parâmetro de escala"
  },
  {
    "objectID": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-gamma-1",
    "href": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-gamma-1",
    "title": "Outras Distribuições Contínuas",
    "section": "Distribuição Gamma",
    "text": "Distribuição Gamma\nSe \\(X \\sim Gamma(\\alpha, \\lambda)\\), então\n\\[\n\\begin{eqnarray}\nE[X] &=& \\frac{1}{\\Gamma(\\alpha)} \\int_0^\\infty x \\lambda  e^{-\\lambda x} (\\lambda x)^{\\alpha-1}\\,dx \\\\ &=& \\frac{1}{\\Gamma(\\alpha)} \\int_0^\\infty e^{-\\lambda x} (\\lambda x)^{\\alpha}\\, dx \\\\ &=& \\frac{1}{\\lambda \\Gamma(\\alpha)} \\int_0^\\infty e^{-u} (u)^{\\alpha}\\, du \\\\ &=& \\frac{\\Gamma(\\alpha+1)}{\\lambda \\,\\Gamma(\\alpha)} = \\frac{\\alpha \\Gamma(\\alpha)}{\\lambda \\,\\Gamma(\\alpha)} = \\dfrac{\\alpha}{\\lambda}\n\\end{eqnarray}\n\\]"
  },
  {
    "objectID": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-gamma-2",
    "href": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-gamma-2",
    "title": "Outras Distribuições Contínuas",
    "section": "Distribuição Gamma",
    "text": "Distribuição Gamma\n\\[\n\\begin{eqnarray}\nE[X^2] &=& \\frac{1}{\\Gamma(\\alpha)} \\int_0^\\infty x^2 \\lambda  e^{-\\lambda x} (\\lambda x)^{\\alpha-1}\\,dx = \\frac{1}{\\Gamma(\\alpha)} \\int_0^\\infty x^2 \\lambda e^{-\\lambda x} \\lambda^{\\alpha -1} x^{\\alpha-1}\\, dx \\\\ &=& \\frac{1}{\\Gamma(\\alpha)} \\int_0^\\infty e^{-\\lambda x} \\lambda^{\\alpha}x^{\\alpha+1}\\, dx = \\frac{\\lambda^{\\alpha}}{\\Gamma(\\alpha)} \\int_0^\\infty e^{-\\lambda x} x^{\\alpha+1}\\, dx \\\\ &=& \\frac{\\lambda^{\\alpha}}{\\Gamma(\\alpha)} \\int_0^\\infty e^{-u} \\left(\\dfrac{u}{\\lambda}\\right)^{\\alpha+1}\\,\\left(\\dfrac{1}{\\lambda}\\right) du  = \\frac{\\lambda^{\\alpha}}{\\lambda^{\\alpha +2}\\Gamma(\\alpha)} \\int_0^\\infty e^{-u} u^{\\alpha+1}\\, du \\\\ &=& \\frac{\\lambda^{\\alpha}\\Gamma(\\alpha+2)}{\\lambda^{\\alpha +2} \\,\\Gamma(\\alpha)} = \\frac{(\\alpha+1) \\Gamma(\\alpha+1)}{\\lambda^2 \\,\\Gamma(\\alpha)} =  \\frac{(\\alpha+1)\\alpha \\Gamma(\\alpha)}{\\lambda^2 \\,\\Gamma(\\alpha)} =  \\dfrac{\\alpha (\\alpha+1)}{\\lambda^2}\n\\end{eqnarray}\n\\]"
  },
  {
    "objectID": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-gamma-3",
    "href": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-gamma-3",
    "title": "Outras Distribuições Contínuas",
    "section": "Distribuição Gamma",
    "text": "Distribuição Gamma\nDe forma que,\n\\[\n\\begin{eqnarray}\nVar(X) &=& E(X^2) - \\left[E(X)\\right]^2 \\\\ &=& \\dfrac{\\alpha (\\alpha+1)}{\\lambda^2} - \\left[\\dfrac{\\alpha}{\\lambda}\\right]^2 \\\\ &=& \\dfrac{\\alpha^2 + \\alpha}{\\lambda^2} - \\dfrac{\\alpha^2}{\\lambda^2} \\\\ &=& \\dfrac{\\alpha^2 + \\alpha - \\alpha^2}{\\lambda^2} = \\dfrac{\\alpha}{\\lambda^2}\n\\end{eqnarray}\n\\]"
  },
  {
    "objectID": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-gamma-4",
    "href": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-gamma-4",
    "title": "Outras Distribuições Contínuas",
    "section": "Distribuição Gamma",
    "text": "Distribuição Gamma\nLogo, se \\(X \\sim Gamma(\\alpha, \\lambda)\\), então\n\\[\nf(x | \\alpha, \\lambda)=\n\\begin{cases}\n\\dfrac{\\lambda e^{-\\lambda x}(\\lambda x)^{\\alpha-1}}{\\Gamma(\\alpha)}, & x\\ge0\\\\[6pt]\n0, & x&lt;0\n\\end{cases}\n\\]\ncom\n\\[E(X) =  \\dfrac{\\alpha}{\\lambda} \\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\, \\text{       e       } \\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,Var(X) = \\dfrac{\\alpha}{\\lambda^2}\\]"
  },
  {
    "objectID": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-gamma-5",
    "href": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-gamma-5",
    "title": "Outras Distribuições Contínuas",
    "section": "Distribuição Gamma",
    "text": "Distribuição Gamma\nA Função de Distribuição Acumulada da distribuição gamma é intratável analiticamente.\n\\[\nF(x\\mid\\alpha,\\lambda)\n= \\int_{0}^{x} \\frac{\\lambda e^{-\\lambda u}(\\lambda u)^{\\alpha-1}}{\\Gamma(\\alpha)}\\,du,\n\\qquad x\\ge 0.\n\\]\n\n\n\n\n\n\n\n\n\nPara \\(\\alpha\\) inteiro positivo, a equação acima pode ser integrada por partes, resultando em\n\\[\nF(x\\mid\\alpha,\\lambda) =\n\\begin{cases}\n1 - \\sum_{k=0}^{\\alpha-1}\\frac{\\lambda^k}{k!} x^k e^{-\\lambda x}, & x &gt; 0\\\\[6pt]\n0, & x \\leq 0\n\\end{cases}\n\\]\nA expressão acima é a soma de termos da Poisson com média \\(\\lambda x\\)."
  },
  {
    "objectID": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribução-gamma",
    "href": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribução-gamma",
    "title": "Outras Distribuições Contínuas",
    "section": "Distribução Gamma",
    "text": "Distribução Gamma\n\n\nFigure 1: Distribuição Gama — variação do shape (α) com λ fixo."
  },
  {
    "objectID": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribução-gamma-1",
    "href": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribução-gamma-1",
    "title": "Outras Distribuições Contínuas",
    "section": "Distribução Gamma",
    "text": "Distribução Gamma\n\n\nFigure 2: Distribuição Gama — variação da taxa (λ) com α fixo."
  },
  {
    "objectID": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-gamma-6",
    "href": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-gamma-6",
    "title": "Outras Distribuições Contínuas",
    "section": "Distribuição Gamma",
    "text": "Distribuição Gamma\nA distribuição Gamma tem muitas aplicações reais, especialmente quando estudamos tempos até ocorrência de eventos.\n\n\nTempo até falha / confiabilidade de sistemas: modelagem do tempo de vida de componentes mecânicos, eletrônicos, etc.\n\n\n\n\nTempo até ocorrência de eventos em Poisson: Se eventos acontecem segundo um processo de Poisson com taxa \\(\\lambda\\):\n\no tempo até o primeiro evento é Exponencial (Gamma(1, \\(\\lambda\\)))\no tempo até o \\(k\\)-ésimo evento é Gamma(k,\\(\\lambda\\))\n\n\n\n\n\nModelagem de tempos de espera em filas: em teoria de filas (M/M/1, G/G/1 etc.), tempos de serviço ou tempos de atendimento podem ser modelados como Gamma."
  },
  {
    "objectID": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-gamma-7",
    "href": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-gamma-7",
    "title": "Outras Distribuições Contínuas",
    "section": "Distribuição Gamma",
    "text": "Distribuição Gamma\n\nPriori conjugada em Bayesiana: é priori conjugada para o parâmetro de taxa de uma Exponencial ou Poisson. Então em inferência Bayesiana, a Gamma aparece o tempo todo como priori para \\(\\lambda\\)\n\n\n\nHidrologia / clima: modelagem de chuvas acumuladas (precipitação) ao longo de certo intervalo: o total acumulado de chuva frequentemente é bem modelado por Gamma.\n\n\n\n\nBiologia / Epidemiologia: tempo até infecção, tempo até recuperação, duração de hospitalização, tempos de permanência podem ser modelados por Gamma (ou Weibull, que é próxima).\n\n\n\n\nAnálise de risco / Seguros: valores positivos e assimétricos (como sinistros) podem ser modelados com Gamma."
  },
  {
    "objectID": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-gamma-8",
    "href": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-gamma-8",
    "title": "Outras Distribuições Contínuas",
    "section": "Distribuição Gamma",
    "text": "Distribuição Gamma\nExemplo 01: Suponha que o tempo gasto por um estagiário selecionado aleatoriamente para realizar uma tarefa em uma empresa tem uma distribuição gamma com média \\(20\\,\\, \\text{minutos}\\) e variância de \\(80\\,\\, \\text{minutos}^2\\)\n\nQuais são os parâmetros da distribuição gamma utilizada? R: \\(\\lambda = 0,25\\) e \\(\\alpha = 5\\)\nQual é a probabilidade de um estagiário realizar a tarefa em no máximo 24 minutos? R: \\(0,7149\\)\nQual é a probabilidade de um estagiário passar entre 20 e 40 minutos realizando a tarefa? R: \\(0,5595\\)"
  },
  {
    "objectID": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-gamma-9",
    "href": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-gamma-9",
    "title": "Outras Distribuições Contínuas",
    "section": "Distribuição Gamma",
    "text": "Distribuição Gamma\nExemplo 02: A duração do atendimento de cada cliente no caixa de um supermercado tem distribuição Gamma com parâmetro de forma \\(\\alpha = 12\\) e parâmetro de taxa \\(\\lambda = 2\\)\n\nQual a média e variância da duração dos atendimentos? R: \\(E(x) = 6\\) e \\(Var(X) = 3\\)\nQual a probabilidade de um atendimento durar menos de 5 minutos? R: \\(0,3032\\)"
  },
  {
    "objectID": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-qui-quadrado",
    "href": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-qui-quadrado",
    "title": "Outras Distribuições Contínuas",
    "section": "Distribuição Qui-Quadrado",
    "text": "Distribuição Qui-Quadrado\nUm caso particular da distribuição Gamma, quando tomamos \\(\\alpha = \\dfrac{n}{2}\\) e \\(\\lambda = \\dfrac{1}{2}\\), onde \\(n\\) é um inteiro positivo.\n\nAssim, para \\(\\alpha = \\dfrac{n}{2}\\) e \\(\\lambda = \\dfrac{1}{2}\\),\n\\[\n\\begin{eqnarray}\nf(x) = \\dfrac{\\frac{1}{2} e^{-\\frac{x}{2}}\\left(\\frac{x}{2}\\right)^{\\frac{n}{2}-1}}{\\Gamma \\left(\\frac{n}{2}\\right)} =  \\dfrac{\\frac{e^{-\\frac{x}{2}}}{2} \\left(\\frac{x ^{\\frac{n}{2}-1}}{2^{\\frac{n}{2}-1}}\\right)}{\\Gamma \\left(\\frac{n}{2}\\right)} = \\dfrac{\\frac{e^{-\\frac{x}{2}} x ^{\\frac{n}{2}-1}}{2^{\\frac{n}{2}}}}{\\Gamma \\left(\\frac{n}{2}\\right)} = \\dfrac{1}{2^{\\frac{n}{2}} \\Gamma \\left(\\frac{n}{2}\\right)} e^{-\\frac{x}{2}} x ^{\\frac{n}{2}-1}\n\\end{eqnarray}\n\\]"
  },
  {
    "objectID": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-qui-quadrado-1",
    "href": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-qui-quadrado-1",
    "title": "Outras Distribuições Contínuas",
    "section": "Distribuição Qui-Quadrado",
    "text": "Distribuição Qui-Quadrado\nLogo, se \\(X \\sim \\chi^2_n\\), então\n\\[\nf(x | \\alpha, \\lambda)=\n\\begin{cases}\n\\dfrac{1}{2^{\\frac{n}{2}} \\Gamma \\left(\\frac{n}{2}\\right)} e^{-\\frac{x}{2}} x ^{\\frac{n}{2}-1}, & x\\ge0\\\\[6pt]\n0, & x&lt;0\n\\end{cases}\n\\]\ncom\n\\[E(X) =  n \\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\, \\text{       e       } \\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,Var(X) = 2n\\]"
  },
  {
    "objectID": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-qui-quadrado-2",
    "href": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-qui-quadrado-2",
    "title": "Outras Distribuições Contínuas",
    "section": "Distribuição Qui-Quadrado",
    "text": "Distribuição Qui-Quadrado\nA Função de Distribuição Acumulada da distribuição qui-quadrado é intratável analiticamente.\n\\[\nF(x\\mid n)\n= \\int_{0}^{x} \\dfrac{1}{2^{\\frac{n}{2}} \\Gamma \\left(\\frac{n}{2}\\right)} e^{-\\frac{x}{2}} x ^{\\frac{n}{2}-1} \\,dx,\n\\qquad x\\ge 0.\n\\]\n\n\n\n\n\n\n\n\n\nPara \\(n\\) inteiro positivo par, então existe forma como série finita:\n\\[\nF(x\\mid n) = 1 - e^{-x/2}\\sum_{k=0}^{\\frac{n}{2}-1}\\frac{\\left(\\frac{x}{2}\\right)^k}{k!}\n\\]\nou seja, quando \\(n\\) é par, ela pode ser escrita como uma soma finita de termos tipo Poisson."
  },
  {
    "objectID": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-qui-quadrado-3",
    "href": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-qui-quadrado-3",
    "title": "Outras Distribuições Contínuas",
    "section": "Distribuição Qui-Quadrado",
    "text": "Distribuição Qui-Quadrado\n\n\nFigure 3: Densidades qui-quadrado para vários graus de liberdade n."
  },
  {
    "objectID": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-qui-quadrado-4",
    "href": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-qui-quadrado-4",
    "title": "Outras Distribuições Contínuas",
    "section": "Distribuição Qui-Quadrado",
    "text": "Distribuição Qui-Quadrado\nA distribuição qui-quadrado é central em inferência estatística porque ela surge naturalmente quando somamos quadrados de variáveis Normais padrão:\n\\[\\chi^2_n = \\displaystyle{\\sum_{i=1}^n} Z_i^2, \\,\\,\\,\\,\\, Z_i \\sim N(0,1)\\]\n\nLogo, ela está por trás dos testes mais usados com dados categóricos, ANOVA, variâncias e na estruturação das distribuições t e F."
  },
  {
    "objectID": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-beta",
    "href": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-beta",
    "title": "Outras Distribuições Contínuas",
    "section": "Distribuição Beta",
    "text": "Distribuição Beta\nDiz-se que uma variável aleatória tem distribução Beta, se sua função densidade é dada por\n\\[\nf(x \\mid a,b)=\n\\begin{cases}\n\\dfrac{1}{B(a,b)} x^{\\,a-1} (1-x)^{\\,b-1}, & 0&lt;x&lt;1\\\\[6pt]\n0, & \\text{caso contrário}\n\\end{cases}\n\\]\nem que a função Beta é dada por\n\\[B(a,b)=\\int_0^1 x^{\\,a-1} (1-x)^{\\,b-1}\\,dx\\]"
  },
  {
    "objectID": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-beta-1",
    "href": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-beta-1",
    "title": "Outras Distribuições Contínuas",
    "section": "Distribuição Beta",
    "text": "Distribuição Beta\nAlém disso, a função Beta tem uma conexão muito importante com a função Gamma\n\\[B(a,b) = \\dfrac{\\Gamma(a) \\Gamma(b)}{\\Gamma(a+b)}\\]"
  },
  {
    "objectID": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-beta-2",
    "href": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-beta-2",
    "title": "Outras Distribuições Contínuas",
    "section": "Distribuição Beta",
    "text": "Distribuição Beta\nSe \\(X \\sim Beta(a, b)\\), então\n\\[\n\\begin{eqnarray}\nE[X] &=& \\int_0^1 x \\dfrac{1}{B(a,b)} x^{\\,a-1} (1-x)^{\\,b-1} &=& \\dfrac{1}{B(a,b)}  \\int_0^1 x^a(1-x)^{b-1} = \\dfrac{B(a+1,b)}{B(a,b)}\n\\end{eqnarray}\n\\]\nUsando a relação \\(B(a,b) = \\dfrac{\\Gamma(a) \\Gamma(b)}{\\Gamma(a+b)}\\), temos\n\\[\n\\begin{eqnarray}\n\\dfrac{B(a+1,b)}{B(a,b)} &=& \\dfrac{\\dfrac{\\Gamma(a+1)\\Gamma(b)}{\\Gamma(a+b + 1)}}{\\dfrac{\\Gamma(a)\\Gamma(b)}{\\Gamma(a+b)}} = \\dfrac{\\Gamma(a+1)\\Gamma(b)}{\\Gamma(a+b + 1)} \\times \\dfrac{\\Gamma(a+b)}{\\Gamma(a)\\Gamma(b)} = \\dfrac{\\Gamma(a+1)\\Gamma(a+b)}{\\Gamma(a+b + 1)\\Gamma(a)}=\n\\end{eqnarray}\n\\]"
  },
  {
    "objectID": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-beta-3",
    "href": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-beta-3",
    "title": "Outras Distribuições Contínuas",
    "section": "Distribuição Beta",
    "text": "Distribuição Beta\n\\(= \\dfrac{a\\Gamma(a)\\Gamma(a+b)}{(a+b)\\Gamma(a+b)\\Gamma(a)} = \\dfrac{a}{a+b}\\)\n\n\\[\n\\begin{eqnarray}\nE[X^2] &=& \\int_0^1 x^2 \\dfrac{1}{B(a,b)} x^{\\,a-1} (1-x)^{\\,b-1} = \\dfrac{1}{B(a,b)}  \\int_0^1 x^{a+1}(1-x)^{b-1} = \\dfrac{B(a+2,b)}{B(a,b)} \\\\ &=& \\dfrac{\\dfrac{\\Gamma(a+2)\\Gamma(b)}{\\Gamma(a+b + 2)}}{\\dfrac{\\Gamma(a)\\Gamma(b)}{\\Gamma(a+b)}} = \\dfrac{\\Gamma(a+2)\\Gamma(b)\\Gamma(a+b)}{\\Gamma(a+b + 2)\\Gamma(a)\\Gamma(b)} = \\dfrac{(a+1)\\Gamma(a+1)\\Gamma(a+b)}{(a+b+1)\\Gamma(a+b + 1)\\Gamma(a)} \\\\ &=& \\dfrac{(a+1)a\\Gamma(a)\\Gamma(a+b)}{(a+b+1)(a+b)\\Gamma(a+b)\\Gamma(a)} = \\dfrac{(a+1)a}{(a+b+1)(a+b)}\n\\end{eqnarray}\n\\]"
  },
  {
    "objectID": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-beta-4",
    "href": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-beta-4",
    "title": "Outras Distribuições Contínuas",
    "section": "Distribuição Beta",
    "text": "Distribuição Beta\nDe forma que,\n\\[\n\\begin{eqnarray}\nVar(X) &=& E(X^2) - \\left[E(X)\\right]^2 \\\\ &=& \\dfrac{(a+1)a}{(a+b+1)(a+b)}-\\left[\\dfrac{a}{a+b}\\right]^2 = \\dfrac{a^2 + a}{(a+b+1)(a+b)}-\\dfrac{a^2}{(a+b)^2} \\\\ &=& \\dfrac{(a^2 + a)(a+b) - a^2(a+b+1)}{(a+b+1)(a+b)^2} = \\dfrac{a^3 + a^2b+a^2 +ab- a^3-a^2b-a^2}{(a+b+1)(a+b)^2} \\\\ &=& \\dfrac{ab}{(a+b+1)(a+b)^2}\n\\end{eqnarray}\n\\]"
  },
  {
    "objectID": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-beta-5",
    "href": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-beta-5",
    "title": "Outras Distribuições Contínuas",
    "section": "Distribuição Beta",
    "text": "Distribuição Beta\nLogo, se \\(X \\sim Beta(a,b)\\), então\n\\[\nf(x \\mid a,b)=\n\\begin{cases}\n\\dfrac{1}{B(a,b)} x^{\\,a-1} (1-x)^{\\,b-1}, & 0&lt;x&lt;1\\\\[6pt]\n0, & \\text{caso contrário}\n\\end{cases}\n\\]\ncom\n\\[E(X) =  \\dfrac{a}{a+b} \\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\, \\text{       e       } \\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,Var(X) = \\dfrac{ab}{(a+b+1)(a+b)^2}\\]"
  },
  {
    "objectID": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-beta-6",
    "href": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-beta-6",
    "title": "Outras Distribuições Contínuas",
    "section": "Distribuição Beta",
    "text": "Distribuição Beta\nA Função de Distribuição Acumulada da distribuição Beta é intratável analiticamente.\n\\[\nF(x| a,b)=\\dfrac{1}{B(a,b)}\\int_0^x t^{a-1}(1-t)^{b-1}dt.\n\\qquad 0&lt; x &lt; 1.\n\\]"
  },
  {
    "objectID": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-beta-7",
    "href": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-beta-7",
    "title": "Outras Distribuições Contínuas",
    "section": "Distribuição Beta",
    "text": "Distribuição Beta\n\n\nFigure 4: Distribuição Beta — variação de a (shape1) com b fixo."
  },
  {
    "objectID": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-beta-8",
    "href": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-beta-8",
    "title": "Outras Distribuições Contínuas",
    "section": "Distribuição Beta",
    "text": "Distribuição Beta\n\n\nFigure 5: Distribuição Beta — variação de b (shape2) com a fixo."
  },
  {
    "objectID": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-beta-9",
    "href": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-beta-9",
    "title": "Outras Distribuições Contínuas",
    "section": "Distribuição Beta",
    "text": "Distribuição Beta\n\n\nFigure 6: Distribuição Beta — simetria quando α = β."
  },
  {
    "objectID": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-beta-10",
    "href": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-beta-10",
    "title": "Outras Distribuições Contínuas",
    "section": "Distribuição Beta",
    "text": "Distribuição Beta\nA distribuição Beta é usada para modelar proporções (variáveis contínuas entre 0 e 1). É uma das distribuições mais importantes em estatística aplicada.\n\n\nModelagem de proporções: proporção de sucesso, taxa de clique (CTR) em marketing digital, fração de tempo ativo de um equipamento, percentual de umidade, pureza, concentração, etc.\n\n\n\n\nInferência Bayesiana: é a prior conjugada da Bernoulli e da Binomial → se o parâmetro de interesse é uma probabilidade \\(p\\), a priori Beta é natural.\n\n\n\n\nModelo para incerteza em probabilidades: incerteza sobre \\(p\\) em “probabilidade de sucesso” antes de observar dados, nossa crença sobre \\(p\\) é Beta"
  },
  {
    "objectID": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-beta-11",
    "href": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-beta-11",
    "title": "Outras Distribuições Contínuas",
    "section": "Distribuição Beta",
    "text": "Distribuição Beta\nExemplo 03: A porcentagem de impurezas por lote, em determinado produto químico, é uma variável aleatória com distribuição Beta com parâmetros \\(a = 3\\) e \\(b = 2\\). Um lote com mais de \\(40\\%\\) de impurezas não pode ser vendido.\n\nQual é a probabilidade de que um lote, selecionado ao acaso, não possa ser vendido por causa do excesso de impurezas? R: \\(0,8208\\)\nQuantos lotes, em média, são selecionados, ao acaso, até que se encontre um que não pode ser vendido por causa do excesso de impurezas? R: \\(1,2183\\)\nQual é a porcentagem média de impurezas nos lotes desse produto químico? R: \\(0,60\\)"
  },
  {
    "objectID": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-beta-12",
    "href": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-beta-12",
    "title": "Outras Distribuições Contínuas",
    "section": "Distribuição Beta",
    "text": "Distribuição Beta\nExemplo 04: O teor de gordura no leite de um rebanho bovino é uma variável com distribuição Beta com parâmetros \\(a = 2\\) e \\(b = 5\\).\n\nQual o teor médio de gordura no leite? R: \\(0,2857\\)\nQual o percentual de amostras que terá teor de gordura menor que \\(10\\%\\)? R: \\(0,1143\\)"
  },
  {
    "objectID": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-weibull",
    "href": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-weibull",
    "title": "Outras Distribuições Contínuas",
    "section": "Distribuição Weibull",
    "text": "Distribuição Weibull\nUma variável aleatória tem distribuição de Weibull com parâmetros \\(\\beta &gt; 0\\) e \\(\\alpha &gt; 0\\) se sua função densidade para \\(x \\geq 0\\) é dada por\n\\[\nf(x|\\alpha, \\beta) =\n\\begin{cases}\n\\dfrac{\\beta}{\\alpha}\n\\left(\\dfrac{x}{\\alpha}\\right)^{\\beta - 1}\n\\exp\\!\\left[-\\left(\\dfrac{x}{\\alpha}\\right)^{\\beta}\\right],\n& x\\geq0\\\\[10pt]\n0, & x&lt;0\n\\end{cases}\n\\]\nNesta parametrização,\n\n\\(\\beta\\) - parâmetro de forma\n\\(\\alpha\\) - parâmetro de escala"
  },
  {
    "objectID": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-weibull-1",
    "href": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-weibull-1",
    "title": "Outras Distribuições Contínuas",
    "section": "Distribuição Weibull",
    "text": "Distribuição Weibull\nSe \\(X \\sim Weibull(\\alpha, \\beta)\\), então\n\\[\n\\begin{eqnarray}\nE[X] &=& \\int_0^\\infty x \\dfrac{\\beta}{\\alpha} \\left(\\dfrac{x}{\\alpha}\\right)^{\\beta - 1} \\exp\\!\\left[-\\left(\\dfrac{x}{\\alpha}\\right)^{\\beta}\\right]dx = \\int_0^\\infty \\alpha u \\dfrac{\\beta}{\\alpha} \\left(\\dfrac{\\alpha u}{\\alpha}\\right)^{\\beta - 1} \\exp\\!\\left[-\\left(\\dfrac{\\alpha u}{\\alpha}\\right)^{\\beta}\\right] \\alpha  \\,\\,\\,du \\\\&=& \\int_0^\\infty  u \\,\\,\\beta \\,\\, u^{\\beta - 1} \\exp\\!\\left[-u^{\\beta}\\right] \\alpha \\,\\,\\,du = \\int_0^\\infty  \\beta \\,\\, u^{\\beta } \\exp\\!\\left[-u^{\\beta}\\right] \\alpha \\,\\,\\,du \\\\ &=& \\int_0^\\infty  \\beta \\,\\, v \\exp\\!\\left[-v\\right] \\alpha \\,\\,\\,\\dfrac{1}{\\beta} v^{\\frac{1}{\\beta}-1} dv = \\alpha \\int_0^\\infty  \\exp\\!\\left[-v\\right] v^{\\frac{1}{\\beta}} \\,\\, dv = \\alpha \\Gamma\\left( \\dfrac{1}{\\beta} + 1\\right)\n\\end{eqnarray}\n\\]"
  },
  {
    "objectID": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-weibull-2",
    "href": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-weibull-2",
    "title": "Outras Distribuições Contínuas",
    "section": "Distribuição Weibull",
    "text": "Distribuição Weibull\n\\[\n\\small\n\\begin{eqnarray}\nE[X^2] &=& \\int_0^\\infty x^2 \\dfrac{\\beta}{\\alpha} \\left(\\dfrac{x}{\\alpha}\\right)^{\\beta - 1} \\exp\\!\\left[-\\left(\\dfrac{x}{\\alpha}\\right)^{\\beta}\\right]dx = \\int_0^\\infty (\\alpha u)^2 \\dfrac{\\beta}{\\alpha} \\left(\\dfrac{\\alpha u}{\\alpha}\\right)^{\\beta - 1} \\exp\\!\\left[-\\left(\\dfrac{\\alpha u}{\\alpha}\\right)^{\\beta}\\right] \\alpha  \\,\\,\\,du \\\\&=& \\int_0^\\infty  \\alpha^2u^2 \\,\\,\\dfrac{\\beta}{\\alpha} \\,\\, u^{\\beta - 1} \\exp\\!\\left[-u^{\\beta}\\right] \\alpha \\,\\,\\,du = \\int_0^\\infty  \\alpha^2 \\,\\,\\beta \\,\\, u^{\\beta +1} \\exp\\!\\left[-u^{\\beta}\\right]\\,\\,\\,du \\\\ &=& \\int_0^\\infty  \\alpha^2 \\,\\,\\beta \\,\\, u^{\\beta} \\,\\, u \\,\\, \\exp\\!\\left[-u^{\\beta}\\right]\\,\\,\\,du = \\int_0^\\infty \\alpha^2 \\beta \\,\\, v \\,\\, v^{\\frac{1}{\\beta}}\\exp\\!\\left[-v\\right] \\,\\,\\dfrac{1}{\\beta} v^{\\frac{1}{\\beta}-1} dv \\\\ &=& \\alpha^2 \\int_0^\\infty  v^{\\frac{1}{\\beta} + 1} \\exp\\!\\left[-v\\right] v^{\\frac{1}{\\beta}-1} \\,\\, dv = \\alpha^2 \\int_0^\\infty  v^{\\frac{2}{\\beta}} \\exp\\!\\left[-v\\right] \\,\\, dv = \\alpha^2 \\Gamma\\left( \\dfrac{2}{\\beta} + 1\\right)\n\\end{eqnarray}\n\\]"
  },
  {
    "objectID": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-weibull-3",
    "href": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-weibull-3",
    "title": "Outras Distribuições Contínuas",
    "section": "Distribuição Weibull",
    "text": "Distribuição Weibull\nDe forma que,\n\\[\n\\begin{eqnarray}\nVar(X) &=& E(X^2) - \\left[E(X)\\right]^2 \\\\ &=& \\alpha^2 \\Gamma\\left( \\dfrac{2}{\\beta} + 1\\right)-\\left[\\alpha \\Gamma\\left( \\dfrac{1}{\\beta} + 1\\right)\\right]^2 = \\alpha^2 \\Gamma\\left( \\dfrac{2}{\\beta} + 1\\right)- \\alpha^2\\left[ \\Gamma\\left( \\dfrac{1}{\\beta} + 1\\right)\\right]^2\\\\ &=& \\alpha^2\\left\\{\\Gamma\\left( \\dfrac{2}{\\beta} + 1\\right) - \\left[\\Gamma\\left( \\dfrac{1}{\\beta} + 1\\right)\\right]^2\\right\\}\n\\end{eqnarray}\n\\]"
  },
  {
    "objectID": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-weibull-4",
    "href": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-weibull-4",
    "title": "Outras Distribuições Contínuas",
    "section": "Distribuição Weibull",
    "text": "Distribuição Weibull\nSe \\(X \\sim Weibull(\\alpha, \\beta)\\), então\n\\[\nf(x|\\alpha, \\beta) =\n\\begin{cases}\n\\dfrac{\\beta}{\\alpha}\n\\left(\\dfrac{x}{\\alpha}\\right)^{\\beta - 1}\n\\exp\\!\\left[-\\left(\\dfrac{x}{\\alpha}\\right)^{\\beta}\\right],\n& x\\geq0\\\\[10pt]\n0, & x&lt;0\n\\end{cases}\n\\]\ncom\n\\[E(X) = \\alpha \\Gamma\\left( \\dfrac{1}{\\beta} + 1\\right) \\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\, \\text{       e       } \\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,Var(X) = \\alpha^2\\left\\{\\Gamma\\left( \\dfrac{2}{\\beta} + 1\\right) - \\left[\\Gamma\\left( \\dfrac{1}{\\beta} + 1\\right)\\right]^2\\right\\}\\]"
  },
  {
    "objectID": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-weibull-5",
    "href": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-weibull-5",
    "title": "Outras Distribuições Contínuas",
    "section": "Distribuição Weibull",
    "text": "Distribuição Weibull\nObservação: Se \\(X \\sim Weibull(\\alpha, 1)\\), ou seja, fazendo \\(\\beta = 1\\), temos\n\\[f(x) = \\dfrac{1}{\\alpha} \\left(\\dfrac{x}{\\alpha}\\right)^{1 - 1} \\exp\\!\\left[-\\left(\\dfrac{x}{\\alpha}\\right)^{1}\\right] = \\dfrac{1}{\\alpha}  \\exp\\!\\left(-\\dfrac{x}{\\alpha}\\right), \\,\\,\\,\\,\\,\\, x\\geq 0\\]\nFazendo \\(\\lambda = \\dfrac{1}{\\lambda}\\), temos \\(f(x) = \\lambda e^{-\\lambda x}\\). Logo \\(X \\sim Exp(\\lambda)\\)."
  },
  {
    "objectID": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-weibull-6",
    "href": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-weibull-6",
    "title": "Outras Distribuições Contínuas",
    "section": "Distribuição Weibull",
    "text": "Distribuição Weibull\nUma das vantagens da distribuição Weibull é a possibilidade de encontrar a sua função de distribuição parametrizada por \\(\\alpha\\) e \\(\\beta\\). Dessa forma fica fácil fazer cálculos de probabilidades.\n\nPor definição, \\(F(x) = P(X ≤ x)\\). Para \\(x \\leq 0\\) claramente \\(F(x) = 0\\). Para \\(x &gt; 0\\), temos:\n\\[F(x) = \\int_0^x \\dfrac{\\beta}{\\alpha}\\left(\\dfrac{t}{\\alpha}\\right)^{\\beta - 1} \\exp\\!\\left[-\\left(\\dfrac{t}{\\alpha}\\right)^{\\beta}\\right] \\,\\,dt\\]\nMudança de variável: \\(u=\\left(\\frac{t}{\\alpha}\\right)^{\\beta} \\;\\;\\Rightarrow\\;\\; du=\\frac{\\beta}{\\alpha}\\left(\\frac{t}{\\alpha}\\right)^{\\beta-1}dt.\\) Quando \\(t=0\\), \\(u=0\\); quando \\(t=x\\), \\(u=(x/\\alpha)^{\\beta}\\)."
  },
  {
    "objectID": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-weibull-7",
    "href": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-weibull-7",
    "title": "Outras Distribuições Contínuas",
    "section": "Distribuição Weibull",
    "text": "Distribuição Weibull\nLogo,\n\\[\nF(x)=\\int_{0}^{(x/\\alpha)^{\\beta}} e^{-u}\\,du = \\Big[-e^{-u}\\Big]_{0}^{(x/\\alpha)^{\\beta}} = 1-\\exp\\!\\left[-\\left(\\frac{x}{\\alpha}\\right)^{\\beta}\\right]\n\\]\nPortanto,\n\\[\nF(x|\\alpha, \\beta)=\n\\begin{cases}\n1-\\exp\\!\\left[-\\left(\\dfrac{x}{\\alpha}\\right)^{\\beta}\\right], & x&gt; 0,\\\\[8pt]\n0, & x\\le0.\n\\end{cases}\n\\]"
  },
  {
    "objectID": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-weibull-8",
    "href": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-weibull-8",
    "title": "Outras Distribuições Contínuas",
    "section": "Distribuição Weibull",
    "text": "Distribuição Weibull\n\n\nFigure 7: Distribuição Weibull — variação de β (forma) com α fixo."
  },
  {
    "objectID": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-weibull-9",
    "href": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-weibull-9",
    "title": "Outras Distribuições Contínuas",
    "section": "Distribuição Weibull",
    "text": "Distribuição Weibull\n\n\nFigure 8: Distribuição Weibull — variação de α (escala) com β fixo."
  },
  {
    "objectID": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-weibull-10",
    "href": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-weibull-10",
    "title": "Outras Distribuições Contínuas",
    "section": "Distribuição Weibull",
    "text": "Distribuição Weibull\nA Weibull é uma das distribuições mais usadas em engenharia para modelar tempo até falha/tempo de vida. Ela é extremamente flexível porque o parâmetro de forma \\(\\beta\\) permite que o risco aumente, diminua ou seja constante ao longo do tempo.\n\n\nEngenharia de Confiabilidade:\n\nvida útil de componentes mecânicos e eletrônicos]\nfadiga de materiais\ntempo até quebra de peças, motores, rolamentos, cabos, soldas\n\n\n\n\n\nAnálise de Risco/Manutenção:\n\nplanejamento de manutenção preventiva\nprever quando um equipamento deve ser substituído"
  },
  {
    "objectID": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-weibull-11",
    "href": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-weibull-11",
    "title": "Outras Distribuições Contínuas",
    "section": "Distribuição Weibull",
    "text": "Distribuição Weibull\n\nAnálise de sobrevivência/biomedicina:\n\ntempo até ocorrência de um evento (recorrência de doença, óbito, falha de tratamento)\nalternativa mais flexível que exponencial\n\n\n\n\nMeteorologia e Clima:\n\ndistribuição de vento (velocidade do vento normalmente é modelada por Weibull)\n\n\n\n\n\nPesquisa operacional:\n\ntempo até falha em sistemas complexos\nmodelagem em teoria de filas com tempos de serviço não-exponenciais"
  },
  {
    "objectID": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-weibull-12",
    "href": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-weibull-12",
    "title": "Outras Distribuições Contínuas",
    "section": "Distribuição Weibull",
    "text": "Distribuição Weibull\nExemplo 05: Uma empresa realiza treinamentos periódicos com seus funcionários. Em cada grupo de treinamento é passado uma tarefa desafio individual e os funcionários do grupo podem fazê-la durante o tempo que precisarem. O tempo que dura a tarefa desafio, em horas, pode ser considerada uma variável aleatória de Weibull com \\(\\alpha = 2\\) e \\(\\beta = 0,4\\).\n\nEm média, quanto tempo dura a tarefa desafio em um treinamento? R: \\(6,6467\\) \\((\\approx 6h38min)\\)\nQual a probabilidade da tarefa desafio durar menos de 8 horas? R: \\(0,8247\\)\nA tarefa desafio já está sendo realizada há 2 horas. Qual a probabilidade dela acabar nas próximas 2 horas? R: \\(0,2735\\)\nQual o menor tempo \\(t\\), em horas, para o qual podemos dizer que \\(95\\%\\) das tarefas desafio duram menos que \\(t\\)? R: \\(31,06615\\) \\((\\approx 31h04min)\\)"
  },
  {
    "objectID": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-weibull-13",
    "href": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-weibull-13",
    "title": "Outras Distribuições Contínuas",
    "section": "Distribuição Weibull",
    "text": "Distribuição Weibull\nExemplo 06: Uma loja quer saber quanto tempo suas lâmpadas LED duram, para decidir garantia. Ela testa várias lâmpadas funcionando 24 horas por dia e anota o tempo até queimarem. Os dados mostram que quanto mais antiga a lâmpada, mais chance ela tem de queimar a qualquer momento, ou seja, os dados seguem uma distribuição de Weibull com parâmetros \\(\\alpha = 200\\) e \\(\\beta = 1,4\\).\n\nQual a probabilidade de uma lâmpada queimar antes de 150 dias? R: \\(0,4875\\)\nQual a probabilidade de uma lâmpada queimar entre 100 e 250 dias? R: \\(0,4296\\)"
  },
  {
    "objectID": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-log-normal",
    "href": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-log-normal",
    "title": "Outras Distribuições Contínuas",
    "section": "Distribuição Log-Normal",
    "text": "Distribuição Log-Normal\nUma variável \\(X\\) tem distribuição Log-Normal quando\n\\[\\ln(X) \\sim N(\\mu, \\sigma^2)\\]\nAssim, \\(X \\sim Log-Normal(\\mu, \\sigma^2)\\) se sua função densidade é dada por\n\\[\nf(x|\\mu, \\sigma) =\n\\begin{cases}\n\\dfrac{1}{x\\, \\sigma\\, \\sqrt{2\\pi}} \\,\\exp\\!\\left[-\\dfrac{1}{2}\\left(\\dfrac{\\ln x - \\mu}{\\sigma}\\right)^2\\right],\n& x&gt;0\\\\[10pt]\n0, & x\\leq 0\n\\end{cases}\n\\]"
  },
  {
    "objectID": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-log-normal-1",
    "href": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-log-normal-1",
    "title": "Outras Distribuições Contínuas",
    "section": "Distribuição Log-Normal",
    "text": "Distribuição Log-Normal\n\\[\n\\begin{eqnarray}\nE(X) &=& \\int_{-\\infty}^{\\infty} x \\,\\dfrac{1}{x\\, \\sigma\\, \\sqrt{2\\pi}} \\,\\exp\\!\\left[-\\dfrac{1}{2}\\left(\\dfrac{\\ln x - \\mu}{\\sigma}\\right)^2\\right]\\,dx \\\\&=& \\int_{0}^{\\infty}  \\,\\dfrac{1}{\\sigma \\,\\sqrt{2\\pi}}\\exp\\!\\left[-\\dfrac{1}{2}\\left(\\dfrac{\\ln x - \\mu}{\\sigma}\\right)^2\\right]\\,dx\n\\end{eqnarray}\n\\] Mudança de variável: \\(z = \\ln(x) \\Rightarrow x = e^z, dx = e^z \\, dz\\). Quando \\(x \\in (0, \\infty) \\Rightarrow z \\in (-\\infty,\\infty)\\)\n\\[\n\\begin{eqnarray}\n&=& \\int_{-\\infty}^{\\infty} \\dfrac{1}{\\sigma\\, \\sqrt{2\\pi}} \\,\\exp\\!\\left[-\\dfrac{1}{2}\\left(\\dfrac{z - \\mu}{\\sigma}\\right)^2\\right]\\, e^z \\,dz \\\\&=& \\int_{-\\infty}^{\\infty}  \\,\\dfrac{1}{\\sigma \\,\\sqrt{2\\pi}}\\exp\\!\\left[-\\dfrac{1}{2}\\left(\\dfrac{z - \\mu}{\\sigma}\\right)^2 + z\\right]\\,dz\n\\end{eqnarray}\n\\]"
  },
  {
    "objectID": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-log-normal-2",
    "href": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-log-normal-2",
    "title": "Outras Distribuições Contínuas",
    "section": "Distribuição Log-Normal",
    "text": "Distribuição Log-Normal\nCompletando o quadrado do expoente:\n\\[\n\\begin{eqnarray}\nz-\\dfrac{(z-\\mu)^2}{2\\sigma^2} &=& -\\dfrac{1}{2\\sigma^2}\\Big[(z-\\mu)^2-2\\sigma^2 z\\Big] = -\\dfrac{1}{2\\sigma^2}\\Big[z^2-2(\\mu+\\sigma^2)z+\\mu^2\\Big] \\\\ &=&  -\\dfrac{1}{2\\sigma^2}\\big[\\big(z-(\\mu+\\sigma^2)\\big)^2 - (\\mu+\\sigma^2)^2+\\mu^2\\big] \\\\ &=& -\\dfrac{\\big(z-(\\mu+\\sigma^2)\\big)^2}{2\\sigma^2} +\\dfrac{(\\mu+\\sigma^2)^2-\\mu^2}{2\\sigma^2} \\\\ &=& -\\dfrac{\\big(z-(\\mu+\\sigma^2)\\big)^2}{2\\sigma^2} +\\mu+\\dfrac{\\sigma^2}{2}\n\\end{eqnarray}\n\\]"
  },
  {
    "objectID": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-log-normal-3",
    "href": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-log-normal-3",
    "title": "Outras Distribuições Contínuas",
    "section": "Distribuição Log-Normal",
    "text": "Distribuição Log-Normal\nAssim,\n\\[\\small\n\\begin{eqnarray}\n\\int_{-\\infty}^{\\infty}  \\,\\dfrac{1}{\\sigma \\,\\sqrt{2\\pi}}\\exp\\!\\left[-\\dfrac{1}{2}\\left(\\dfrac{z - \\mu}{\\sigma}\\right)^2 + z\\right]\\,dz &=& \\int_{-\\infty}^{\\infty}  \\,\\dfrac{1}{\\sigma \\,\\sqrt{2\\pi}}\\exp\\!\\left[-\\dfrac{\\big(z-(\\mu+\\sigma^2)\\big)^2}{2\\sigma^2} +\\mu+\\dfrac{\\sigma^2}{2}\\right]\\,dz \\\\&=& e^{\\mu+\\dfrac{\\sigma^2}{2}}\n\\int_{-\\infty}^{\\infty} \\dfrac{1}{\\sigma \\,\\sqrt{2\\pi}} \\exp\\!\\left[-\\frac{\\big(z-(\\mu+\\sigma^2)\\big)^2}{2\\sigma^2}\\right]\\,dz\n\\end{eqnarray}\n\\]\nA integral é 1 (é uma densidade Normal). Logo,\n\\[\n\\boxed{E(X)=\\exp\\!\\left(\\mu+\\frac{\\sigma^2}{2}\\right)}\n\\]"
  },
  {
    "objectID": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-log-normal-4",
    "href": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-log-normal-4",
    "title": "Outras Distribuições Contínuas",
    "section": "Distribuição Log-Normal",
    "text": "Distribuição Log-Normal\nDa mesma forma,\n\\[\n\\begin{eqnarray}\nE(X^2) &=& \\int_{-\\infty}^{\\infty} x^2 \\,\\dfrac{1}{x\\, \\sigma\\, \\sqrt{2\\pi}} \\,\\exp\\!\\left[-\\dfrac{1}{2}\\left(\\dfrac{\\ln x - \\mu}{\\sigma}\\right)^2\\right]\\,dx \\\\&=& \\int_{0}^{\\infty}  \\,\\dfrac{x}{\\sigma \\,\\sqrt{2\\pi}}\\exp\\!\\left[-\\dfrac{1}{2}\\left(\\dfrac{\\ln x - \\mu}{\\sigma}\\right)^2\\right]\\,dx\n\\end{eqnarray}\n\\]\nMudança de variável: \\(z = \\ln(x) \\Rightarrow x = e^z, dx = e^z \\, dz\\). Quando \\(x \\in (0, \\infty) \\Rightarrow z \\in (-\\infty,\\infty)\\)"
  },
  {
    "objectID": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-log-normal-5",
    "href": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-log-normal-5",
    "title": "Outras Distribuições Contínuas",
    "section": "Distribuição Log-Normal",
    "text": "Distribuição Log-Normal\n\\[\n\\begin{eqnarray}\n&=& \\int_{-\\infty}^{\\infty} \\dfrac{e^z}{\\sigma\\, \\sqrt{2\\pi}} \\,\\exp\\!\\left[-\\dfrac{1}{2}\\left(\\dfrac{z - \\mu}{\\sigma}\\right)^2\\right]\\, e^z \\,dz \\\\&=& \\int_{-\\infty}^{\\infty} \\dfrac{1}{\\sigma\\, \\sqrt{2\\pi}} \\,\\exp\\!\\left[-\\dfrac{1}{2}\\left(\\dfrac{z - \\mu}{\\sigma}\\right)^2\\right]\\, e^{2z} \\,dz \\\\&=& \\int_{-\\infty}^{\\infty}  \\,\\dfrac{1}{\\sigma \\,\\sqrt{2\\pi}}\\exp\\!\\left[-\\dfrac{1}{2}\\left(\\dfrac{z - \\mu}{\\sigma}\\right)^2 + 2z\\right]\\,dz\n\\end{eqnarray}\n\\]"
  },
  {
    "objectID": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-log-normal-6",
    "href": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-log-normal-6",
    "title": "Outras Distribuições Contínuas",
    "section": "Distribuição Log-Normal",
    "text": "Distribuição Log-Normal\nCompletando o quadrado do expoente:\n\\[\n\\begin{eqnarray}\n2z-\\dfrac{(z-\\mu)^2}{2\\sigma^2} &=& -\\dfrac{1}{2\\sigma^2}\\Big[(z-\\mu)^2-4\\sigma^2 z\\Big] = -\\dfrac{1}{2\\sigma^2}\\Big[z^2-2(\\mu+2\\sigma^2)z+\\mu^2\\Big] \\\\ &=&  -\\dfrac{1}{2\\sigma^2}\\big[\\big(z-(\\mu+2\\sigma^2)\\big)^2 - (\\mu+2\\sigma^2)^2+\\mu^2\\big] \\\\ &=& -\\dfrac{\\big(z-(\\mu+2\\sigma^2)\\big)^2}{2\\sigma^2} +\\dfrac{(\\mu+2\\sigma^2)^2-\\mu^2}{2\\sigma^2} \\\\ &=& -\\dfrac{\\big(z-(\\mu+2\\sigma^2)\\big)^2}{2\\sigma^2} +2\\mu+ 2\\sigma^2\n\\end{eqnarray}\n\\]"
  },
  {
    "objectID": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-log-normal-7",
    "href": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-log-normal-7",
    "title": "Outras Distribuições Contínuas",
    "section": "Distribuição Log-Normal",
    "text": "Distribuição Log-Normal\nAssim,\n\\[\\small\n\\begin{eqnarray}\n\\int_{-\\infty}^{\\infty}  \\,\\dfrac{1}{\\sigma \\,\\sqrt{2\\pi}}\\exp\\!\\left[-\\dfrac{1}{2}\\left(\\dfrac{z - \\mu}{\\sigma}\\right)^2 + 2z\\right]\\,dz &=& \\int_{-\\infty}^{\\infty}  \\,\\dfrac{1}{\\sigma \\,\\sqrt{2\\pi}}\\exp\\!\\left[-\\dfrac{\\big(z-(\\mu+ 2\\sigma^2)\\big)^2}{2\\sigma^2} +2\\mu+2\\sigma^2\\right]\\,dz \\\\&=& e^{2\\mu+2\\sigma^2}\n\\int_{-\\infty}^{\\infty} \\dfrac{1}{\\sigma \\,\\sqrt{2\\pi}} \\exp\\!\\left[-\\frac{\\big(z-(\\mu+2\\sigma^2)\\big)^2}{2\\sigma^2}\\right]\\,dz\n\\end{eqnarray}\n\\]\nA integral é 1 (é uma densidade Normal). Logo,\n\\[\n\\boxed{E(X^2)=\\exp\\!\\left(2\\mu+2\\sigma^2\\right)}\n\\]"
  },
  {
    "objectID": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-log-normal-8",
    "href": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-log-normal-8",
    "title": "Outras Distribuições Contínuas",
    "section": "Distribuição Log-Normal",
    "text": "Distribuição Log-Normal\nDe forma que,\n\\[\n\\begin{eqnarray}\nVar(X) &=& E(X^2) - \\left[E(X)\\right]^2 \\\\ &=& \\exp\\!\\left( 2\\mu+2\\sigma^2 \\right) - \\left[\\exp\\!\\left(\\mu+\\frac{\\sigma^2}{2}\\right)\\right]^2 \\\\ &=& \\exp\\!\\left(\\,2\\mu+\\sigma^2\\right)\\big(e^{\\sigma^2}-1\\big)\n\\end{eqnarray}\n\\]\nAssim,\n\\[\n\\boxed{Var(X)=\\exp\\!\\left(\\,2\\mu+\\sigma^2\\right)\\big(e^{\\sigma^2}-1\\big)}\n\\]"
  },
  {
    "objectID": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-log-normal-9",
    "href": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-log-normal-9",
    "title": "Outras Distribuições Contínuas",
    "section": "Distribuição Log-Normal",
    "text": "Distribuição Log-Normal\nSe \\(X \\sim Log-Normal(\\mu, \\sigma^2)\\), então\n\\[\nf(x|\\mu, \\sigma) =\n\\begin{cases}\n\\dfrac{1}{x\\, \\sigma\\, \\sqrt{2\\pi}} \\,\\exp\\!\\left[-\\dfrac{1}{2}\\left(\\dfrac{\\ln x - \\mu}{\\sigma}\\right)^2\\right],\n& x&gt;0\\\\[10pt]\n0, & x\\leq 0\n\\end{cases}\n\\]\ncom\n\\[E(X) = \\exp\\!\\left(\\mu+\\frac{\\sigma^2}{2}\\right) \\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\, \\text{       e       } \\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,Var(X) = \\exp\\!\\left(\\,2\\mu+\\sigma^2\\right)\\big(e^{\\sigma^2}-1\\big)\\]"
  },
  {
    "objectID": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-log-normal-10",
    "href": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-log-normal-10",
    "title": "Outras Distribuições Contínuas",
    "section": "Distribuição Log-Normal",
    "text": "Distribuição Log-Normal\nA distribuição Log-Normal não possui forma fechada para a função de distribuição acumulada. No entanto,\n\\[F_X(x)=P(X\\le x)=\\Phi\\!\\left(\\frac{\\ln x - \\mu}{\\sigma}\\right), \\quad x&gt;0,\\]\nonde \\(\\Phi(\\cdot)\\) é a FDA da Normal padrão."
  },
  {
    "objectID": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-log-normal-11",
    "href": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-log-normal-11",
    "title": "Outras Distribuições Contínuas",
    "section": "Distribuição Log-Normal",
    "text": "Distribuição Log-Normal\n\n\nFigure 9: Distribuição Lognormal — variação de μ (com σ fixo)"
  },
  {
    "objectID": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-log-normal-12",
    "href": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-log-normal-12",
    "title": "Outras Distribuições Contínuas",
    "section": "Distribuição Log-Normal",
    "text": "Distribuição Log-Normal\n\n\nFigure 10: Distribuição Lognormal — variação de σ (com μ fixo)"
  },
  {
    "objectID": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-log-normal-13",
    "href": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-log-normal-13",
    "title": "Outras Distribuições Contínuas",
    "section": "Distribuição Log-Normal",
    "text": "Distribuição Log-Normal\n\n\nFigure 11: Distribuição Lognormal — FDC variando μ (σ fixo)"
  },
  {
    "objectID": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-log-normal-14",
    "href": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-log-normal-14",
    "title": "Outras Distribuições Contínuas",
    "section": "Distribuição Log-Normal",
    "text": "Distribuição Log-Normal\n\n\nFigure 12: Distribuição Lognormal — FDC variando σ (μ fixo)"
  },
  {
    "objectID": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-log-normal-15",
    "href": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-log-normal-15",
    "title": "Outras Distribuições Contínuas",
    "section": "Distribuição Log-Normal",
    "text": "Distribuição Log-Normal\nA distribuição Lognormal aparece quando um processo multiplicativo gera a variável observada. Como \\(\\ln(X)\\) é Normal, isso quer dizer que \\(X\\) nasce como produto de vários pequenos fatores aleatórios. Por isso, a Lognormal é muito comum em:\n\n\nEconomia e Finanças: distribuição de valores de ações, tempo até certos eventos econômicos, modelos de volatilidade e retornos acumulados\n\n\n\n\nBiologia e Medicina: tempos de incubação de doenças, tempos de sobrevivência em certos contextos, concentração (positiva) de substâncias em fluidos biológicos"
  },
  {
    "objectID": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-log-normal-16",
    "href": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-log-normal-16",
    "title": "Outras Distribuições Contínuas",
    "section": "Distribuição Log-Normal",
    "text": "Distribuição Log-Normal\n\nEngenharias/Confiabilidade: vida útil de componentes quando o desgaste é multiplicativo, dureza de materiais, diâmetro de partículas (tamanho de grãos)\n\n\n\nCiências Ambientais: distribuição de poluentes positivos (\\(CO_2\\), etc.), precipitação acumulada em certos modelos"
  },
  {
    "objectID": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-log-normal-17",
    "href": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-log-normal-17",
    "title": "Outras Distribuições Contínuas",
    "section": "Distribuição Log-Normal",
    "text": "Distribuição Log-Normal\nExemplo 07: A duração do atendimento de cada cliente no caixa de um supermercado tem distribuição Lognormal com parâmetro de locação \\(\\mu = 1,5\\) e parâmetro de dispersão \\(\\sigma = 0,5\\).\n\nQual a média e variância da duração dos atendimentos? R: \\(E(X) = 5,0784\\) e \\(Var(x) = 7,3251\\)\nQual a probabilidade de um atendimento durar menos de 5 minutos? R: \\(0,5871\\)"
  },
  {
    "objectID": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-log-normal-18",
    "href": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-log-normal-18",
    "title": "Outras Distribuições Contínuas",
    "section": "Distribuição Log-Normal",
    "text": "Distribuição Log-Normal\nExemplo 08: Considere o tempo de reparo \\(X\\) (em horas) de equipamentos. Admita que \\[\n\\ln(X) \\sim N(\\mu,\\sigma^2), \\qquad \\mu=0,2\\,\\,\\,\\,\\text{e}\\,\\,\\,\\, \\sigma=0,6.\n\\]\nResponda:\n\nCalcule \\(P(X \\le 1,5)\\). R: \\(0,6331\\)\n\nCalcule \\(P(1 \\le X \\le 3)\\). R: \\(0,5625\\)"
  },
  {
    "objectID": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-exponencial-dupla",
    "href": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-exponencial-dupla",
    "title": "Outras Distribuições Contínuas",
    "section": "Distribuição Exponencial Dupla",
    "text": "Distribuição Exponencial Dupla\nTambém conhecida como Distribuição de Laplace, é formada por reflexão da distribuição exponencial em torno da média.\n\nSe \\(X \\sim Laplace(a, b)\\), então sua função densidade é dada por\n\\[\nf(x)= \\frac{1}{2b}\\, \\exp\\!\\left(-\\frac{|x-a|}{b}\\right), \\qquad -\\infty &lt; x &lt; \\infty,\\; -\\infty &lt; a &lt; \\infty,\\; b &gt; 0.\n\\]\nNesta parametrização:\n\n\\(a\\): parâmetro de localização (a média e mediana da distribuição).\n\\(b\\): parâmetro de escala (controla a dispersão; quanto maior \\(b\\), mais “achatada” é a curva)."
  },
  {
    "objectID": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-exponencial-dupla-1",
    "href": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-exponencial-dupla-1",
    "title": "Outras Distribuições Contínuas",
    "section": "Distribuição Exponencial Dupla",
    "text": "Distribuição Exponencial Dupla\n\nPara \\(x \\ge a\\), exponencial decaindo para a direita\n\n\\[\nf(x)= \\frac{1}{2b}\\, \\exp\\!\\left(-\\frac{x-a}{b}\\right)\n\\]\n\nPara \\(x &lt; a\\), exponencial decaindo para a esquerda\n\n\\[\nf(x)= \\frac{1}{2b}\\, \\exp\\!\\left(\\frac{x-a}{b}\\right)\n\\]"
  },
  {
    "objectID": "aulas/outras_distr_continuas/outras_dist_continuas.html#aparte-funções-pares-e-ímpares",
    "href": "aulas/outras_distr_continuas/outras_dist_continuas.html#aparte-funções-pares-e-ímpares",
    "title": "Outras Distribuições Contínuas",
    "section": "Aparte: Funções Pares e Ímpares",
    "text": "Aparte: Funções Pares e Ímpares\nDefinições:\n\nUma função \\(f(x)\\) é par se\n\n\\[  f(-x) = f(x), \\quad \\forall x\\]\n→ Gráfico simétrico em relação ao eixo y. Exemplos: \\(f(x) = x^2, \\cos x, e^{-x^2}\\)\n\nUma função \\(f(x)\\) é ímpar se\n\n\\[f(-x) = -f(x), \\quad \\forall x\\] → Gráfico simétrico em relação à origem. Exemplos: \\(f(x) = x^3, \\sin x, x e^{-x^2}\\)"
  },
  {
    "objectID": "aulas/outras_distr_continuas/outras_dist_continuas.html#aparte-funções-pares-e-ímpares-1",
    "href": "aulas/outras_distr_continuas/outras_dist_continuas.html#aparte-funções-pares-e-ímpares-1",
    "title": "Outras Distribuições Contínuas",
    "section": "Aparte: Funções Pares e Ímpares",
    "text": "Aparte: Funções Pares e Ímpares\nPara integrais em intervalos simétricos \\([-a, a]\\):\n\nSe \\(f(x)\\) é par:\n\n\\[\\int_{-a}^{a} f(x)\\,dx = 2\\int_{0}^{a} f(x)\\,dx\\]\n\nSe \\(f(x)\\) é ímpar:\n\n\\[\\int_{-a}^{a} f(x)\\,dx = 0\\]"
  },
  {
    "objectID": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-exponencial-dupla-2",
    "href": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-exponencial-dupla-2",
    "title": "Outras Distribuições Contínuas",
    "section": "Distribuição Exponencial Dupla",
    "text": "Distribuição Exponencial Dupla\nVamos encontrar \\(E(X)\\). Para isso, tomamos \\(Z = X-a\\). Logo, \\(Z \\sim Laplace(0,b)\\) com\n\\[\nf_Z(z)= \\dfrac{1}{2b}\\, \\exp\\!\\left(-\\frac{|z|}{b}\\right)\n\\] e \\(E(X) = E(Z + a) = E(Z) + a\\)\n\\[\n\\begin{eqnarray}\nE(Z) &=& \\int_{-\\infty}^{\\infty} z\\,f_Z(z)\\,dz =\\frac{1}{2b}\\int_{-\\infty}^{\\infty} z\\,e^{-|z|/b}\\,dz \\\\ &=& \\frac{1}{2b}\\left(\\int_{-\\infty}^{0} z\\,e^{z/b}\\,dz+\\int_{0}^{\\infty} z\\,e^{-z/b}\\,dz\\right)\n\\end{eqnarray}\n\\]"
  },
  {
    "objectID": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-exponencial-dupla-3",
    "href": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-exponencial-dupla-3",
    "title": "Outras Distribuições Contínuas",
    "section": "Distribuição Exponencial Dupla",
    "text": "Distribuição Exponencial Dupla\nNote que o integrando \\(z\\,e^{-|z|/b}\\) é ímpar, portanto as áreas se cancelam:\n\\[E(Z)=0\\]\nde forma que,\n\\[E(X)= E(Z+a) = E(Z) + a = a\\]\nLogo,\n\\[\\boxed{E(X)=a}\\]"
  },
  {
    "objectID": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-exponencial-dupla-4",
    "href": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-exponencial-dupla-4",
    "title": "Outras Distribuições Contínuas",
    "section": "Distribuição Exponencial Dupla",
    "text": "Distribuição Exponencial Dupla\nCálculo de \\(Var(Z)=E(Z^2)\\) (pois \\((E(Z)=0\\)):\n\\[\n\\begin{eqnarray}\nE(Z^2)&=&\\int_{-\\infty}^{\\infty} z^{2}\\,f_Z(z)\\,dz=\\int_{-\\infty}^{\\infty} z^{2}\\,\\frac{1}{2b} \\,e^{-|z|/b}\\,dz\n\\end{eqnarray}\n\\]\nComo o integrando é par,\n\\[\n\\begin{eqnarray}\nE(Z^2)&=&\\int_{0}^{\\infty} z^{2} \\frac{1}{b} e^{-z/b}\\,dz\n\\end{eqnarray}\n\\]\nFaça a mudança \\(y=z/b\\Rightarrow z=by,\\ dz=b\\,dy\\)"
  },
  {
    "objectID": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-exponencial-dupla-5",
    "href": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-exponencial-dupla-5",
    "title": "Outras Distribuições Contínuas",
    "section": "Distribuição Exponencial Dupla",
    "text": "Distribuição Exponencial Dupla\n\\[\n\\begin{eqnarray}\nE(Z^2)&=&\\int_{0}^{\\infty} z^{2} \\,\\frac{1}{b}\\ e^{-z/b}\\,dz \\\\ &=&\\int_{0}^{\\infty}  (b y)^{2}\\,\\frac{1}{b}\\, e^{-y}\\,b\\,dy\n= b^{2}\\int_{0}^{\\infty} y^{2} e^{-y}\\,dy\n\\end{eqnarray}\n\\]\nPela definição da função Gama,\n\\[\\int_{0}^{\\infty} y^{2} e^{-y}\\,dy=\\Gamma(3)=2!\\]\nPortanto,\n\\[E(Z^2)=b^{2}\\cdot 2 = 2b^{2}\\]"
  },
  {
    "objectID": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-exponencial-dupla-6",
    "href": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-exponencial-dupla-6",
    "title": "Outras Distribuições Contínuas",
    "section": "Distribuição Exponencial Dupla",
    "text": "Distribuição Exponencial Dupla\nComo \\(Var(X)=Var(Z)=E(Z^2)\\),\n\\[\\boxed{Var(X)=2b^{2}}\\]"
  },
  {
    "objectID": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-exponencial-dupla-7",
    "href": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-exponencial-dupla-7",
    "title": "Outras Distribuições Contínuas",
    "section": "Distribuição Exponencial Dupla",
    "text": "Distribuição Exponencial Dupla\nSe \\(X \\sim Laplace(a, b)\\), então\n\\[\nf(x)= \\frac{1}{2b}\\, \\exp\\!\\left(-\\frac{|x-a|}{b}\\right), \\qquad -\\infty &lt; x &lt; \\infty,\\; -\\infty &lt; a &lt; \\infty,\\; b &gt; 0.\n\\]\ncom\n\\[E(X) = a \\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\, \\text{       e       } \\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,Var(X) = 2b^{2}\\]"
  },
  {
    "objectID": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-exponencial-dupla-8",
    "href": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-exponencial-dupla-8",
    "title": "Outras Distribuições Contínuas",
    "section": "Distribuição Exponencial Dupla",
    "text": "Distribuição Exponencial Dupla\nSe \\(X \\sim Laplace(a,b)\\), então,\n\\[F(x)=P(X\\le x)=\\int_{-\\infty}^{x} f(t)\\,dt = \\int_{-\\infty}^{x} \\frac{1}{2b}e^{-\\frac{|t-a|}{b}}\\,dt\\]\nComo a função é definida por módulos, dividimos o cálculo em dois casos.\n\nCaso 1: \\(x &lt; a\\)\nNeste intervalo, \\(|t-a|=a-t\\). Logo\n\\[F(x)=\\int_{-\\infty}^{x}\\frac{1}{2b}e^{-(a-t)/b}\\,dt\n=\\frac{1}{2b}e^{-a/b}\\int_{-\\infty}^{x}e^{t/b}\\,dt\\]"
  },
  {
    "objectID": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-exponencial-dupla-9",
    "href": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-exponencial-dupla-9",
    "title": "Outras Distribuições Contínuas",
    "section": "Distribuição Exponencial Dupla",
    "text": "Distribuição Exponencial Dupla\nCalculando a integral:\n\\[\\int_{-\\infty}^{x} e^{t/b}\\,dt = \\Big[b\\,e^{t/b}\\Big]_{-\\infty}^{x} = b\\,e^{x/b} - b\\,\\lim_{t\\to -\\infty} e^{t/b} = b\\,e^{x/b}\\]\nUma vez que \\(\\lim_{t\\to -\\infty} e^{t/b}=0\\). Então:\n\\[F(x)=\\frac{1}{2b}e^{-a/b}\\,b\\,e^{x/b} =\\dfrac{1}{2} e^{\\Big({\\dfrac{x-a}{b}}\\Big)}, \\qquad x&lt;a\\]"
  },
  {
    "objectID": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-exponencial-dupla-10",
    "href": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-exponencial-dupla-10",
    "title": "Outras Distribuições Contínuas",
    "section": "Distribuição Exponencial Dupla",
    "text": "Distribuição Exponencial Dupla\nCaso 2: \\(x \\ge a\\)\nAqui \\(|t-a|=t-a\\). Assim:\n\\[F(x)=\\int_{-\\infty}^{a}\\frac{1}{2b}e^{-(a-t)/b}\\,dt + \\int_{a}^{x}\\frac{1}{2b}e^{-(t-a)/b}\\,dt\\]\nA primeira integral resulta em:\n\\[\\scriptsize\\int_{-\\infty}^{a} \\frac{1}{2b}\\, e^{-(a-t)/b}\\,dt = \\frac{e^{-a/b}}{2b}\\int_{-\\infty}^{a} e^{t/b}\\,dt = \\frac{e^{-a/b}}{2b}\\,\\Big[b\\,e^{t/b}\\Big]_{-\\infty}^{a} = \\frac{e^{-a/b}}{2b}\\,\\Big(b\\,e^{a/b}-0\\Big) = \\frac{1}{2}\\,e^{-a/b}e^{a/b} = \\frac{1}{2}\\]"
  },
  {
    "objectID": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-exponencial-dupla-11",
    "href": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-exponencial-dupla-11",
    "title": "Outras Distribuições Contínuas",
    "section": "Distribuição Exponencial Dupla",
    "text": "Distribuição Exponencial Dupla\nPara a segunda integral, fazemos a substituição \\(u = t - a  \\Rightarrow  du = dt\\). Quando \\(t = a \\Rightarrow u = 0\\) e quando \\(t = x \\Rightarrow u = x - a\\).\n\\[\n\\begin{eqnarray}\n\\int_{a}^{x}\\dfrac{1}{2b}e^{-(t-a)/b}\\,dt &=& \\dfrac{1}{2b}\\int_{0}^{x-a} e^{-u/b}\\,du = \\dfrac{1}{2b}\\Big[-b\\,e^{-u/b}\\Big]_{0}^{x-a}\\\\\n&=& \\dfrac{1}{2b}\\Big[-b\\,e^{-(x-a)/b} + b\\Big] = \\dfrac{1}{2b}\\cdot b(1 - e^{-(x-a)/b}) \\\\&=& \\dfrac{1}{2}\\left(1 - e^{-\\dfrac{x-a}{b}}\\right)\n\\end{eqnarray}\n\\]"
  },
  {
    "objectID": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-exponencial-dupla-12",
    "href": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-exponencial-dupla-12",
    "title": "Outras Distribuições Contínuas",
    "section": "Distribuição Exponencial Dupla",
    "text": "Distribuição Exponencial Dupla\nSomando:\n\\[ F(x)=\\frac{1}{2}+\\frac{1}{2}(1-e^{-(x-a)/b}) =1 - \\dfrac{1}{2} e^{-\\Big(\\dfrac{x-a}{b}\\Big)}, \\qquad x\\ge a\\] Logo, se \\(X \\sim Laplace(a,b)\\), então\n\\[\nF(x)=\n\\begin{cases}\n\\dfrac{1}{2} e^{\\Big({\\dfrac{x-a}{b}}\\Big)}, & x&lt;a, \\\\[8pt]\n1 - \\dfrac{1}{2} e^{-\\Big(\\dfrac{x-a}{b}\\Big)}, & x\\ge a.\n\\end{cases}\n\\]"
  },
  {
    "objectID": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-exponencial-dupla-13",
    "href": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-exponencial-dupla-13",
    "title": "Outras Distribuições Contínuas",
    "section": "Distribuição Exponencial Dupla",
    "text": "Distribuição Exponencial Dupla\n\n\nFigure 13: Distribuição Laplace — diferentes parâmetros (função densidade)"
  },
  {
    "objectID": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-exponencial-dupla-14",
    "href": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-exponencial-dupla-14",
    "title": "Outras Distribuições Contínuas",
    "section": "Distribuição Exponencial Dupla",
    "text": "Distribuição Exponencial Dupla\n\n\nFigure 14: Distribuição Laplace — diferentes parâmetros (função de distribuição acumulada)"
  },
  {
    "objectID": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-exponencial-dupla-15",
    "href": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-exponencial-dupla-15",
    "title": "Outras Distribuições Contínuas",
    "section": "Distribuição Exponencial Dupla",
    "text": "Distribuição Exponencial Dupla\nA distribuição Laplace (também chamada de exponencial dupla) aparece em diversas áreas onde há simetria, mas com caudas mais pesadas que a normal.\n\n\nFigure 15: Comparação lado a lado: Laplace(0, 1/√2) vs Normal(0,1)"
  },
  {
    "objectID": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-exponencial-dupla-16",
    "href": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-exponencial-dupla-16",
    "title": "Outras Distribuições Contínuas",
    "section": "Distribuição Exponencial Dupla",
    "text": "Distribuição Exponencial Dupla\n\n\nNo painel esquerdo (escala linear): a Laplace tem pico mais alto e mais pontudo no centro.\nNo painel direito (escala log): as caudas da Laplace decaem mais lentamente que as da Normal — ou seja, a Laplace tem caudas mais pesadas."
  },
  {
    "objectID": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-exponencial-dupla-17",
    "href": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-exponencial-dupla-17",
    "title": "Outras Distribuições Contínuas",
    "section": "Distribuição Exponencial Dupla",
    "text": "Distribuição Exponencial Dupla\n\n\nFigure 16: CDF: Normal(0,1) vs Laplace(0, 1/√2)"
  },
  {
    "objectID": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-exponencial-dupla-18",
    "href": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-exponencial-dupla-18",
    "title": "Outras Distribuições Contínuas",
    "section": "Distribuição Exponencial Dupla",
    "text": "Distribuição Exponencial Dupla\n\nModelagem de Erros e Ruídos: Em muitos fenômenos, os erros são simétricos, mas com maior probabilidade de valores extremos do que uma normal permitiria.\n\n\n\nEconomia e Finanças: Em séries de retornos de ativos financeiros, é comum observar distribuições simétricas com caudas longas.\n\n\n\n\nEngenharia de Processos e Controle: Modelagem de diferenças entre medições ou resíduos em sistemas físicos. Muito usada em filtros robustos e em controle adaptativo, pois é menos sensível a outliers do que a Normal.\n\n\n\n\nProcessamento de Sinais e Imagens: Em processamento de imagem, a distribuição Laplace descreve coeficientes de transformadas, especialmente onde há bordas ou ruídos bruscos. Também aparece em compressão de imagem e remoção de ruído impulsivo."
  },
  {
    "objectID": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-exponencial-dupla-19",
    "href": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-exponencial-dupla-19",
    "title": "Outras Distribuições Contínuas",
    "section": "Distribuição Exponencial Dupla",
    "text": "Distribuição Exponencial Dupla\n\nCiência de Dados e Estatística Robusta: O modelo Laplace é base para a Regressão de Mínima Soma de Desvios Absolutos (LAD), também chamada regressão mediana. O estimador de máxima verossimilhança sob erro Laplace é o estimador da mediana, o que torna a Laplace uma escolha natural para modelos robustos a outliers.\n\n\n\nPrivacidade Diferencial: Em Ciência de Dados, o Mecanismo Laplace é usado para adicionar ruído calibrado e proteger dados sensíveis. A distribuição Laplace permite controlar a quantidade de ruído adicionada de acordo com o parâmetro de privacidade \\(\\epsilon\\)."
  },
  {
    "objectID": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-exponencial-dupla-20",
    "href": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-exponencial-dupla-20",
    "title": "Outras Distribuições Contínuas",
    "section": "Distribuição Exponencial Dupla",
    "text": "Distribuição Exponencial Dupla\nExemplo 09: Considere que o retorno financeiro de dado investimento possa ser modelado de acordo com um distribuição de Laplace, com parâmetros \\(a = \\, R\\$ \\, 1 \\text{milhão}\\) e \\(b = 20 \\, \\text{mil}\\).\n\nCalcule a probabilidade do retorno ser superior a \\(R\\$ \\, 1.020.000,00\\). R: \\(0,1839\\)\nCalcule a probabilidade do retorno ser inferior a \\(R\\$ \\, 940.000,00\\). R: \\(0,0249\\)\nCalcule a probabilidade do retorno ficar entre \\(R\\$ \\, 940.000,00\\) e \\(R\\$ \\, 1.020.000,00\\). R: \\(0,7912\\)"
  },
  {
    "objectID": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-exponencial-dupla-21",
    "href": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-exponencial-dupla-21",
    "title": "Outras Distribuições Contínuas",
    "section": "Distribuição Exponencial Dupla",
    "text": "Distribuição Exponencial Dupla\nExemplo 10: Uma câmera digital utilizada em um ambiente industrial está sujeita a reflexos de luz intensa e interferências elétricas, que produzem ruído impulsivo nas imagens. Esse ruído pode ser modelado pela distribuição de Laplace, cuja função densidade de probabilidade é dada por:\n\\[f(x) = \\frac{1}{2b}\\, e^{-\\frac{|x - a|}{b}}, \\quad -\\infty &lt; x &lt; \\infty,\\]\nonde: - \\(a\\) é o valor médio (esperado) do ruído, - \\(b\\) é o parâmetro de escala que controla a dispersão.\nEm uma determinada medição, assume-se que:\n\\[a = 0, \\quad b = 10\\]"
  },
  {
    "objectID": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-exponencial-dupla-22",
    "href": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-exponencial-dupla-22",
    "title": "Outras Distribuições Contínuas",
    "section": "Distribuição Exponencial Dupla",
    "text": "Distribuição Exponencial Dupla\n\nCalcule a probabilidade de que a perturbação no brilho de um pixel seja maior que 20 unidades em valor absoluto (ou seja, (|X| &gt; 20)). R: \\(0,1353\\)\nCalcule a probabilidade de que a variação do brilho esteja entre -15 e 15 unidades. R: \\(0,7769\\)\nCompare esse comportamento com o ruído Gaussiano \\(N(0, 10^2)\\). O ruído Laplaciano gera mais ou menos pixels “fora da faixa normal” (\\(|X| &gt; 20\\))? Explique o motivo, relacionando ao formato das caudas."
  },
  {
    "objectID": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-exponencial-dupla-23",
    "href": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-exponencial-dupla-23",
    "title": "Outras Distribuições Contínuas",
    "section": "Distribuição Exponencial Dupla",
    "text": "Distribuição Exponencial Dupla\n\nRuído Laplaciano (a=0, b=10) e comparação com Normal(0,10)"
  },
  {
    "objectID": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-de-valores-extremos",
    "href": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-de-valores-extremos",
    "title": "Outras Distribuições Contínuas",
    "section": "Distribuição de Valores Extremos",
    "text": "Distribuição de Valores Extremos\nEm muitas situações práticas, não estamos interessados na média ou no comportamento típico dos dados, mas sim nos eventos extremos, os maiores ou menores valores observados.\nAssim, temos duas possibilidades:\n\nDistribuição do Menor Valor Extremo\nDistribuição do Maior Valor Extremo"
  },
  {
    "objectID": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-de-valores-extremos-1",
    "href": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-de-valores-extremos-1",
    "title": "Outras Distribuições Contínuas",
    "section": "Distribuição de Valores Extremos",
    "text": "Distribuição de Valores Extremos\nA Distribuição de Gumbel é um caso particular da teoria dos valores extremos. Ela descreve o comportamento assintótico de máximos ou mínimos em grandes amostras.\n\n\nDistribuição do Maior Valor Extremo (Gumbel Máximo)\n\nUsada para modelar valores máximos observados, por exemplo:\n\na maior temperatura anual,\na maior cheia de um rio,\no maior prejuízo financeiro."
  },
  {
    "objectID": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-de-valores-extremos-2",
    "href": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-de-valores-extremos-2",
    "title": "Outras Distribuições Contínuas",
    "section": "Distribuição de Valores Extremos",
    "text": "Distribuição de Valores Extremos\nA função de distribuição acumulada (FDC) é:\n\\[F(x) = \\exp\\!\\left[-\\,e^{-\\Big(\\dfrac{x-\\mu}{\\sigma}\\Big)}\\right], \\qquad x \\in \\mathbb{R},\\]\nonde:\n\n\\(\\mu\\) é o parâmetro de localização,\n\\(\\sigma &gt; 0\\) é o parâmetro de escala."
  },
  {
    "objectID": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-de-valores-extremos-3",
    "href": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-de-valores-extremos-3",
    "title": "Outras Distribuições Contínuas",
    "section": "Distribuição de Valores Extremos",
    "text": "Distribuição de Valores Extremos\nA função densidade é:\n\\[\\small f(x \\mid \\mu, \\sigma)\n= \\frac{1}{\\sigma}\n\\exp\\!\\left[\n-\\left(\n\\frac{x - \\mu}{\\sigma}\n\\right)\n- \\exp\\!\\left(\n-\\frac{x - \\mu}{\\sigma}\n\\right)\n\\right],\n\\qquad\n-\\infty &lt; x &lt; \\infty,\\;\n-\\infty &lt; \\mu &lt; \\infty,\\;\n\\sigma &gt; 0\\]\nÉ possível demonstrar que,\n\\[E(X) = \\mu + \\gamma\\sigma,\\]\nonde \\(\\gamma = 0.57721566\\) é conhecida como constante de Euler–Mascheroni.\ne,\n\\[Var(X) = \\dfrac{\\pi^2}{6}\\sigma^2\\]"
  },
  {
    "objectID": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-de-valores-extremos-4",
    "href": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-de-valores-extremos-4",
    "title": "Outras Distribuições Contínuas",
    "section": "Distribuição de Valores Extremos",
    "text": "Distribuição de Valores Extremos\nSe \\(X \\sim Gumbel_{\\text{max}}(\\mu, \\sigma)\\), então\n\\[\nf(x) = \\frac{1}{\\sigma} \\exp\\!\\left[-\\left(\\frac{x - \\mu}{\\sigma}\\right)- \\exp\\!\\left(-\\frac{x - \\mu}{\\sigma}\\right)\\right]\n\\]\ncom\n\\[E(X) = \\mu + \\gamma\\sigma \\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\, \\text{       e       } \\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,Var(X) = \\dfrac{\\pi^2}{6}\\sigma^2\\]"
  },
  {
    "objectID": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-de-valores-extremos-5",
    "href": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-de-valores-extremos-5",
    "title": "Outras Distribuições Contínuas",
    "section": "Distribuição de Valores Extremos",
    "text": "Distribuição de Valores Extremos\n\n\nFigure 17: Gumbel — Maior Valor Extremo: diferentes parâmetros"
  },
  {
    "objectID": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-de-valores-extremos-6",
    "href": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-de-valores-extremos-6",
    "title": "Outras Distribuições Contínuas",
    "section": "Distribuição de Valores Extremos",
    "text": "Distribuição de Valores Extremos\n\nDistribuição do Menor Valor Extremo (Gumbel Mínimo)\n\nUsada para modelar valores mínimos, por exemplo:\n\na menor temperatura do inverno,\na menor resistência de um material antes da falha,\no menor tempo até um evento extremo."
  },
  {
    "objectID": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-de-valores-extremos-7",
    "href": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-de-valores-extremos-7",
    "title": "Outras Distribuições Contínuas",
    "section": "Distribuição de Valores Extremos",
    "text": "Distribuição de Valores Extremos\nA função de distribuição acumulada é:\n\\[F(x) = 1 - \\exp\\!\\left[-\\,e^{\\Big(\\dfrac{x-\\mu}{\\sigma}\\Big)}\\right], \\qquad x \\in \\mathbb{R}\\] A função densidade correspondente é:\n\\[\n\\small\nf(x|\\mu, \\sigma) = \\dfrac{1}{\\sigma}\\exp\\!\\left[\\Big(\\dfrac{x - \\mu}{\\sigma}\\Big)- \\exp\\!\\left(\\dfrac{x - \\mu}{\\sigma}\\right)\\right],\n\\qquad\n-\\infty &lt; x &lt; \\infty,\\;\n-\\infty &lt; \\mu &lt; \\infty,\\;\n\\sigma &gt; 0\n\\]"
  },
  {
    "objectID": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-de-valores-extremos-8",
    "href": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-de-valores-extremos-8",
    "title": "Outras Distribuições Contínuas",
    "section": "Distribuição de Valores Extremos",
    "text": "Distribuição de Valores Extremos\nÉ possível demonstrar que,\n\\[E(X) = \\mu - \\gamma\\sigma,\\]\nonde \\(\\gamma = 0.57721566\\) é a constante de Euler–Mascheroni.\ne,\n\\[Var(X) = \\dfrac{\\pi^2}{6}\\sigma^2\\]\n\nObserve que a variância é a mesma; apenas o sinal da média muda."
  },
  {
    "objectID": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-de-valores-extremos-9",
    "href": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-de-valores-extremos-9",
    "title": "Outras Distribuições Contínuas",
    "section": "Distribuição de Valores Extremos",
    "text": "Distribuição de Valores Extremos\nSe \\(X \\sim Gumbel_{\\text{min}}(\\mu, \\sigma)\\), então\n\\[\nf(x) = \\dfrac{1}{\\sigma}\\exp\\!\\left[\\Big(\\dfrac{x - \\mu}{\\sigma}\\Big)- \\exp\\!\\left(\\dfrac{x - \\mu}{\\sigma}\\right)\\right]\n\\]\ncom\n\\[E(X) = \\mu - \\gamma\\sigma \\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\, \\text{       e       } \\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,Var(X) = \\dfrac{\\pi^2}{6}\\sigma^2\\]"
  },
  {
    "objectID": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-de-valores-extremos-10",
    "href": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-de-valores-extremos-10",
    "title": "Outras Distribuições Contínuas",
    "section": "Distribuição de Valores Extremos",
    "text": "Distribuição de Valores Extremos\n\n\nFigure 18: Gumbel — Menor Valor Extremo: diferentes parâmetros"
  },
  {
    "objectID": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-de-valores-extremos-11",
    "href": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-de-valores-extremos-11",
    "title": "Outras Distribuições Contínuas",
    "section": "Distribuição de Valores Extremos",
    "text": "Distribuição de Valores Extremos\nRelação entre as duas formas\nNote que o Gumbel do mínimo é obtido invertendo o sinal de \\(x\\):\n\\[\nX_{\\text{mínimo}} \\sim \\text{Gumbel Mínimo}(\\mu, \\sigma)\n\\quad \\Leftrightarrow \\quad\n-Y \\sim \\text{Gumbel Máximo}(-\\mu, \\sigma),\n\\]\nou seja, o Gumbel mínimo é simplesmente o reflexo do Gumbel máximo em torno do eixo vertical."
  },
  {
    "objectID": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-de-valores-extremos-12",
    "href": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-de-valores-extremos-12",
    "title": "Outras Distribuições Contínuas",
    "section": "Distribuição de Valores Extremos",
    "text": "Distribuição de Valores Extremos\n\n\nFigure 19: Comparação entre Gumbel Máximo e Gumbel Mínimo"
  },
  {
    "objectID": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-de-valores-extremos-13",
    "href": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-de-valores-extremos-13",
    "title": "Outras Distribuições Contínuas",
    "section": "Distribuição de Valores Extremos",
    "text": "Distribuição de Valores Extremos\n\n\nO Gumbel Máximo tem cauda longa à direita → eventos de máximos (chuvas, enchentes, picos).\nO Gumbel Mínimo tem cauda longa à esquerda → eventos de mínimos (temperaturas mínimas, falhas).\nParâmetro \\(\\mu\\): define o centro.\nParâmetro \\(\\sigma\\): define a dispersão e intensidade dos extremos."
  },
  {
    "objectID": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-de-valores-extremos-14",
    "href": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-de-valores-extremos-14",
    "title": "Outras Distribuições Contínuas",
    "section": "Distribuição de Valores Extremos",
    "text": "Distribuição de Valores Extremos\nExemplo 11: Considere que a temperatura de operação para um determinado processo industrial possa ser modelada através de uma distribuição do Menor Valor Extremo, com parâmetros \\(\\mu = 30^o \\,C\\) e \\(\\sigma = 2\\). Valores de temperatura abaixo de \\(20^o\\,C\\) são raros, porém quando acontecem, inviabilizam o processo. Determine a probabilidade disso ocorrer, bem como a temperatura média do processo. R: \\(0,0067\\) \\(E(X) = 28,85\\)"
  },
  {
    "objectID": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-de-valores-extremos-15",
    "href": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-de-valores-extremos-15",
    "title": "Outras Distribuições Contínuas",
    "section": "Distribuição de Valores Extremos",
    "text": "Distribuição de Valores Extremos\nExemplo 12: Uma equipe de engenheiros hidrólogos está estudando o nível máximo anual do Rio Paraopeba (em metros acima do leito) para dimensionar obras de contenção. Com base em 30 anos de registros históricos, o nível máximo anual pode ser modelado por uma Distribuição de Gumbel do Máximo com parâmetros:\n\\[\\mu = 6 \\,\\,\\text{m} \\,\\,\\,\\text{       e       } \\,\\,\\,\\sigma = 0,4\\]\nQuer-se saber:\n\nQual é a probabilidade de o nível máximo anual ultrapassar 7 metros? R: \\(0,0789\\)\nQual é o nível esperado (médio) das cheias anuais?R: \\(6,23 \\, \\text{m}\\)"
  },
  {
    "objectID": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-de-valores-extremos-16",
    "href": "aulas/outras_distr_continuas/outras_dist_continuas.html#distribuição-de-valores-extremos-16",
    "title": "Outras Distribuições Contínuas",
    "section": "Distribuição de Valores Extremos",
    "text": "Distribuição de Valores Extremos\n\n\n\n\nFigure 20: Distribuição de Gumbel (Maior Valor Extremo) — Exemplo: nível máximo anual do Rio Paraopeba"
  },
  {
    "objectID": "cronograma.html",
    "href": "cronograma.html",
    "title": "Cronograma da Disciplina",
    "section": "",
    "text": "Esta página contém um esboço dos tópicos, conteúdos e tarefas para o semestre. Este cronograma será atualizado conforme o semestre avança.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSemana\nData\nTópico\nArtigo\nSlides\nEC\nLE\nScript\nMC\nProjeto\n\n\n\n\n6\nTer, 11/11\nOutras Distribuições Contínuas\n\n\n\n\n\n\n\n\n\n\nQui, 13/11\nOutras Distribuições Contínuas",
    "crumbs": [
      "Cronograma da Disciplina"
    ]
  },
  {
    "objectID": "plano.html",
    "href": "plano.html",
    "title": "Plano de Ensino",
    "section": "",
    "text": "Leia com atenção o plano de ensino da disciplina que será oferecida neste período. Nele estão as regras do jogo.\n\nPlano de ensino",
    "crumbs": [
      "Plano de Ensino"
    ]
  },
  {
    "objectID": "exercicios/lista01.html",
    "href": "exercicios/lista01.html",
    "title": "Lista de exercícios 01: Revisão de Probabilidade I",
    "section": "",
    "text": "Data de entrega: 25 de novembro de 2025\n\n\n\nA demanda diária de arroz em um supermercado (em centenas de quilos) é uma variável aleatória contínua \\(X\\) com função densidade de probabilidade\n\n\\[\nf_X(x)=\n\\begin{cases}\n\\dfrac{2x}{3}, & 0&lt;x&lt;1,\\\\\\\\\n1-\\dfrac{x}{3}, & 1&lt;x&lt;3,\\\\\\\\\n0, & \\text{caso contrário.}\n\\end{cases}\n\\]\nConsidere as questões a seguir.\n\nMostre que \\(f_x(x)\\) é uma função densidade de probabilidade para a demanda de arroz.\nQual a probabilidade de, em um dia escolhido ao acaso, se vender mais que 150 kg de arroz?\nEm 30 dias, quanto o gerente do supermercado espera vender?\nDetermine a função de distribuição acumulada de \\(X\\).\nQual é a quantidade de arroz que deve ser deixada à disposição do público diariamente para que não falte arroz com \\(95\\%\\) de probabilidade?\nQual é a demanda mediana de arroz?\nE a demanda modal?\n\n\n\nA temperatura \\(T\\) de destilação do petróleo é crucial na determinação da qualidade final do produto. Suponha que \\(T\\) seja considerada uma v.a. com distribuição uniforme no intervalo de 150 a 300. Suponha que o custo para produzir um galão de petróleo seja \\(C_1 u.m.\\). Se o óleo é destilado a uma temperatura inferior a 200, o produto obtido é vendido a \\(C_2 u.m.\\); se a temperatura for superior a 200, o produto é vendido a \\(C_3 u.m.\\).\n\n\nFazer o gráfico da f.d.p de \\(T\\).\nQual o lucro esperado por galão?\n\n\n\nO diâmetro \\(X\\) de rolamentos de esferas fabricados por certa fábrica tem distribuição \\(N(0,6140;\\ (0,0025)^2)\\). O lucro \\(T\\) de cada esfera depende de seu diâmetro e\n\n\n\\(T=0,10\\) se a esfera é boa \\((0,6100 &lt; X &lt; 0,6180)\\)\n\\(T=0,05\\) se a esfera é recuperável \\((0,6080 &lt; X &lt; 0,6100)\\) ou \\((0,6180 &lt; X &lt; 0,6200)\\)\n\\(T=-0,10\\) se a esfera é defeituosa \\((X &lt; 0,6080 \\text{ ou } X &gt; 0,6200)\\).\n\nDetermine \\(E[T]\\).\n\n\nEm uma determinada localidade, a renda em 1000 u.m. é uma v.a. \\(X\\) com função densidade de probabilidade: \\[\nf_X(x)=\n\\begin{cases}\n\\dfrac{x+1}{10}, & 0&lt;x&lt;2,\\\\[6pt]\n1-\\dfrac{18-3x}{40}, & 2&lt;x&lt;6,\\\\[6pt]\n0, & \\text{c.c.}\n\\end{cases}\n\\]\n\n\nMostre que \\(f_X(x)\\) é uma função densidade de probabilidade para \\(X\\).\nDetermine a função de distribuição acumulada de \\(X\\).\nEscolhida uma pessoa ao acaso, qual é a probabilidade de sua renda exceder 3.000 u.m.?\nDetermine a renda média nessa localidade.\nDetermine a renda mediana nessa localidade.\nDetermine o 1º e o 3º quartis da variável renda.\n\n\n\nAs notas de Probabilidade dos alunos de determinada universidade seguem a distribuição normal, com média \\(6{,}4\\) e desvio-padrão \\(0{,}8\\). O professor atribui graus A, B e C, da seguinte forma:\n\n\nC, para notas inferiores a \\(5\\)\nB, para notas entre \\(5\\) e \\(7{,}5\\)\nA, para notas superiores a \\(7{,}5\\)\n\n\nQual a probabilidade de um aluno receber conceito A?\n\nQual a probabilidade de um aluno receber conceito B?\n\nQual a probabilidade de um aluno receber conceito C?\n\nSe a turma tem 80 alunos, quantos devem receber cada conceito, em média?\n\n\n\nSuponha que o número de milhas que um carro percorre antes que sua bateria sofra desgaste tenha distribuição Exponencial com média de 10.000 milhas. Se uma pessoa deseja fazer uma viagem de 5.000 milhas com uma bateria já usada por 8.000 milhas, qual é a probabilidade de terminar a viagem sem ter que trocar a bateria?\n\n\n\nO tempo de vida dos pneus de certo fabricante tem distribuição Exponencial, com duração média de 50.000 km.\n\n\nDetermine a probabilidade de que um pneu deste fabricante dure mais que 50.000 km.\nQual é o tempo de vida que o fabricante deve garantir de forma que, no máximo, 1% dos compradores utilizem a garantia?\nVocê acha que a distribuição exponencial é adequada a esta situação? Justifique.\n\n\n\nO número de clientes chegando a um certo estabelecimento comercial segue a distribuição de Poisson. Em média, chegam 10 clientes a cada hora.\n\n\nDetermine a probabilidade de que o tempo até a chegada do primeiro cliente exceda 5 minutos.\nDetermine a probabilidade de que o tempo entre chegadas sucessivas de dois clientes quaisquer exceda 5 minutos.\nDetermine a probabilidade de que o tempo até a chegada do quinto cliente exceda 30 minutos.\nDetermine a probabilidade de que chegue algum cliente nos próximos 30 minutos, uma vez que nenhum cliente chegou na última hora.\nDetermine o tempo médio entre chegadas sucessivas. Este é um bom valor preditivo?\nDetermine o tempo mediano entre chegadas sucessivas.\nDetermine o tempo médio até a chegada do quinto cliente. Este é um bom valor preditivo?\n\n\n\nSuponha-se que um fusível tenha uma duração de vida \\(X\\), a qual pode ser considerada uma variável aleatória contínua com distribuição Exponencial. Existem dois processos pelos quais o fusível pode ser fabricado. O processo I apresenta uma duração de vida esperada de 100 horas, enquanto o processo II apresenta uma duração de vida esperada de 150 horas. Suponha-se que o processo II seja duas vezes mais custoso que o processo I, que custa 3,00 u.m. por fusível. Admita-se, além disso, que se um fusível durar menos que 200 horas, uma multa de 20 u.m. seja lançada sobre o fabricante. Qual processo deve ser empregado de forma a se minimizar o custo esperado?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n10.Mostre que se \\(X\\) é uma variável aleatória contínua, com distribuição \\(Uniforme(a,b)\\), então:\n\\[E[X] = \\frac{a+b}{2} \\quad \\text{e} \\quad V[X] = \\frac{(b-a)^2}{12}.\\]\n\n11.Mostre que se \\(X \\sim Exponencial(\\lambda)\\), então:\n\\[E[X] = \\frac{1}{\\lambda} \\quad \\text{e} \\quad V[X] = \\frac{1}{\\lambda^2}.\\]\n\n12.Mostre que se \\(X \\sim Exponencial(\\lambda)\\), então \\(P(X &gt; t + s \\mid X &gt; t) = P(X &gt; s)\\), ou seja, a distribuição Exponencial goza da propriedade de “Falta de Memória”.\n\n\n\n\n\n\n\n\n\nUma fábrica produz 10 recipientes de vidro por dia. Deve-se supor que exista uma probabilidade constante \\(p = 0,1\\) de produzir um recipiente defeituoso. Antes que esses recipientes sejam estocados, eles são inspecionados e os defeituosos são separados. Admita que exista uma probabilidade constante \\(r = 0,1\\) de que um recipiente defeituoso seja mal classificado. Faça \\(X\\) igual ao número de recipientes classificados como defeituosos ao fim de um dia de produção. (Admita que todos os recipientes fabricados em um dia sejam inspecionados naquele dia.)\n\n\nCalcule \\(P(X = 3)\\) e \\(P(X &gt; 3)\\).\nObtenha a expressão de \\(P(X = k)\\).\n\n\n\nO número de navios petroleiros, digamos \\(N\\), que chegam a determinada refinaria, cada dia, tem distribuição de Poisson, com parâmetro \\(\\lambda = 2\\). As atuais instalações do porto podem atender a três petroleiros por dia. Se mais de três petroleiros aportarem por dia, os excedentes a três deverão seguir para outro porto.\n\na)Em um dia, qual é a probabilidade de se ter de mandar petroleiros para outro porto?\nb)De quanto deverão as atuais instalações ser aumentadas para permitir manobrar todos os petroleiros, em aproximadamente \\(90\\%\\) dos dias?\nc)Qual é o número esperado de petroleiros a chegarem por dia?\nd)Qual é o número mais provável de petroleiros a chegarem por dia?\ne)Qual é o número esperado de petroleiros a serem atendidos diariamente?\nf)Qual é o número esperado de petroleiros que voltarão a outros portos diariamente?\n\n\nA probabilidade de um bem-sucedido lançamento de foguete é igual a 0,8. Suponha que tentativas de lançamento sejam feitas até que tenham ocorrido 3 lançamentos bem-sucedidos. Qual é a probabilidade de que exatamente 6 tentativas sejam necessárias? Qual é a probabilidade de que menos de 6 tentativas sejam necessárias?\n\n\n\nNa situação descrita no Probl. 15, suponha que as tentativas de lançamento sejam feitas até que três lançamentos bem-sucedidos, consecutivos, ocorram. Responda às questões que surgiram no problema anterior, neste caso.\n\n\n17.Considere novamente a situação descrita no Probl. 15. Suponha que cada tentativa de lançamento custe \\(R\\$ 25.000,00\\). Além disso, um lançamento falho acarrete um custo adicional de \\(R\\$ 5.000,00\\). Calcule o custo esperado, para a situação apresentada."
  },
  {
    "objectID": "exercicios/lista02.html",
    "href": "exercicios/lista02.html",
    "title": "Revisão Probabilidade I",
    "section": "",
    "text": "Definição. Uma variável aleatória é uma função que associa a cada resultado do experimento um número real.\n\nDiscretas: suporte finito/enumerável (ex.: 0,1,2,…). Caracterizam-se por função de probabilidade \\(P(X=x_i) =p(x_i) =p_i\\). Uma função de probabilidade satisfaz \\(0 \\le p_i \\le 1\\) e \\(\\sum_{i=1} p_i = 1\\)\nContínuas: suporte intervalar. Caracterizam-se por densidade \\(f(x)\\) tal que \\(P(a&lt;X&lt; b)=\\int_a^b f(x)\\,dx\\). Uma função densidade satisfaz \\(f(x) \\ge 0\\) e \\(\\int_{-\\infty}^{\\infty} f(x)\\, dx = 1\\).\n\nA função de distribuição é \\(F(x)=P(X\\le x)\\). Em ambos os casos, \\(F(x)\\) é não-decrescente e \\(\\lim_{x\\to-\\infty}F(x)=0\\), \\(\\lim_{x\\to\\infty}F(x)=1\\)."
  },
  {
    "objectID": "exercicios/lista02.html#exercício",
    "href": "exercicios/lista02.html#exercício",
    "title": "Lista de Revisão — Probabilidade I",
    "section": "",
    "text": "(Teoria de Conjuntos) Considere os conjuntos \\(A,B,C\\) em um universo \\(U\\). Mostre que \\(A\\setminus (B\\cup C) = (A\\setminus B)\\cap (A\\setminus C)\\) usando leis algébricas de conjuntos."
  },
  {
    "objectID": "exercicios/lista02.html#exercício-1",
    "href": "exercicios/lista02.html#exercício-1",
    "title": "Lista de Revisão — Probabilidade I",
    "section": "",
    "text": "(Contagem) Quantas sequências de 7 caracteres podem ser formadas com letras maiúsculas (26) e dígitos (10) se: a) não há restrições;\nb) o primeiro caractere é letra e o último é dígito;\nc) não se permite repetição."
  },
  {
    "objectID": "exercicios/lista02.html#exercício-2",
    "href": "exercicios/lista02.html#exercício-2",
    "title": "Lista de Revisão — Probabilidade I",
    "section": "",
    "text": "(Análise Combinatória) Em uma turma com 12 estudantes, de quantas maneiras distintas podem-se escolher: a) um representante e um vice (ordem importa);\nb) um comitê de 4 alunos (ordem não importa);\nc) um comitê de 4 onde um deve ser presidente."
  },
  {
    "objectID": "exercicios/lista02.html#exercício-3",
    "href": "exercicios/lista02.html#exercício-3",
    "title": "Lista de Revisão — Probabilidade I",
    "section": "",
    "text": "(Princípio da Inclusão-Exclusão) Em uma pesquisa: 62% leem jornal \\(J\\), 48% revista \\(R\\), 27% ambos. Qual a probabilidade de um entrevistado ler pelo menos um dos dois? E de não ler nenhum?"
  },
  {
    "objectID": "exercicios/lista02.html#exercício-4",
    "href": "exercicios/lista02.html#exercício-4",
    "title": "Lista de Revisão — Probabilidade I",
    "section": "",
    "text": "(Experimento aleatório) Lance dois dados honestos. Defina o espaço amostral e os eventos \\(A=\\)“soma é 7”, \\(B=\\)“há pelo menos um 6”. Calcule \\(P(A)\\), \\(P(B)\\) e \\(P(A\\cap B)\\)."
  },
  {
    "objectID": "exercicios/lista02.html#exercício-5",
    "href": "exercicios/lista02.html#exercício-5",
    "title": "Lista de Revisão — Probabilidade I",
    "section": "",
    "text": "(Três definições de probabilidade) Explique, com um exemplo curto, as ideias de probabilidade clássica, frequentista e axiomática. Discuta quando cada abordagem é mais conveniente."
  },
  {
    "objectID": "exercicios/lista02.html#exercício-6",
    "href": "exercicios/lista02.html#exercício-6",
    "title": "Lista de Revisão — Probabilidade I",
    "section": "",
    "text": "(Propriedades Axiomáticas) Mostre que para qualquer evento \\(A\\) vale \\(P(A^c)=1-P(A)\\) e que \\(P(\\varnothing)=0\\) a partir dos axiomas de Kolmogorov."
  },
  {
    "objectID": "exercicios/lista02.html#exercício-7",
    "href": "exercicios/lista02.html#exercício-7",
    "title": "Lista de Revisão — Probabilidade I",
    "section": "",
    "text": "(Probabilidade Condicional) Em um baralho padrão, extraem-se 2 cartas sem reposição. Qual a probabilidade de a segunda carta ser um Ás dado que a primeira foi uma figura (J,Q,K)?"
  },
  {
    "objectID": "exercicios/lista02.html#exercício-8",
    "href": "exercicios/lista02.html#exercício-8",
    "title": "Lista de Revisão — Probabilidade I",
    "section": "",
    "text": "(Regra da Multiplicação) Uma fábrica tem 3 máquinas \\(M_1,M_2,M_3\\) que produzem 20%, 30% e 50% dos parafusos, com taxas de defeito de 1%, 2% e 3%. Se um parafuso é escolhido ao acaso, qual a probabilidade de ele ser defeituoso?"
  },
  {
    "objectID": "exercicios/lista02.html#exercício-9",
    "href": "exercicios/lista02.html#exercício-9",
    "title": "Lista de Revisão — Probabilidade I",
    "section": "",
    "text": "(Teorema de Bayes) No cenário do exercício anterior, sabendo que um parafuso sorteado é defeituoso, qual a probabilidade de ter sido produzido pela máquina \\(M_2\\)?"
  },
  {
    "objectID": "exercicios/lista02.html#exercício-10",
    "href": "exercicios/lista02.html#exercício-10",
    "title": "Lista de Revisão — Probabilidade I",
    "section": "",
    "text": "(Independência) Dois eventos \\(A\\) e \\(B\\) obedecem \\(P(A)=0{,}5\\), \\(P(B)=0{,}4\\) e \\(P(A\\cap B)=0{,}22\\). Verifique se \\(A\\) e \\(B\\) são independentes. Justifique."
  },
  {
    "objectID": "exercicios/lista02.html#exercício-11",
    "href": "exercicios/lista02.html#exercício-11",
    "title": "Lista de Revisão — Probabilidade I",
    "section": "",
    "text": "(V.A. Discreta) Uma v.a. \\(X\\) assume valores \\(0,1,2\\) com \\(P(X=0)=p\\), \\(P(X=1)=2p\\) e \\(P(X=2)=1-3p\\).\na) Determine o intervalo de valores de \\(p\\) para que isto defina uma f.p. válida.\nb) Calcule \\(E[X]\\) e \\(Var(X)\\) em função de \\(p\\)."
  },
  {
    "objectID": "exercicios/lista02.html#exercício-12",
    "href": "exercicios/lista02.html#exercício-12",
    "title": "Lista de Revisão — Probabilidade I",
    "section": "",
    "text": "(Função de Distribuição) Para a v.a. discreta do exercício anterior, escreva a função distribuição acumulada \\(F_X(x)\\) e esboce seu gráfico."
  },
  {
    "objectID": "exercicios/lista02.html#exercício-13",
    "href": "exercicios/lista02.html#exercício-13",
    "title": "Lista de Revisão — Probabilidade I",
    "section": "",
    "text": "(Bernoulli/Binomial) Se \\(X\\sim Bin(n,p)\\): a) interprete \\(E[X]\\) e \\(Var(X)\\);\nb) para \\(n=10\\) e \\(p=0{,}3\\), calcule \\(P(X\\le 2)\\) e \\(P(X\\ge 4)\\)."
  },
  {
    "objectID": "exercicios/lista02.html#exercício-14",
    "href": "exercicios/lista02.html#exercício-14",
    "title": "Lista de Revisão — Probabilidade I",
    "section": "",
    "text": "(Hipergeométrica) Em um lote de 50 peças, 8 são defeituosas. Sorteiam-se 6 sem reposição.\na) Qual a probabilidade de obter exatamente 2 defeituosas?\nb) Qual a esperança e a variância do número de defeituosas no sorteio?"
  },
  {
    "objectID": "exercicios/lista02.html#exercício-15",
    "href": "exercicios/lista02.html#exercício-15",
    "title": "Lista de Revisão — Probabilidade I",
    "section": "",
    "text": "(Geométrica e Pascal/Negativa)\na) Para \\(X\\sim Geom(p)\\) (número de falhas antes do primeiro sucesso, apoio \\(0,1,\\dots\\)), obtenha \\(E[X]\\) e \\(Var(X)\\).\nb) Para \\(Y\\sim NegBin(r,p)\\) (falhas antes do \\(r\\)-ésimo sucesso), calcule \\(P(Y\\le 3)\\) quando \\(r=2\\), \\(p=0{,}4\\)."
  },
  {
    "objectID": "exercicios/lista02.html#exercício-16",
    "href": "exercicios/lista02.html#exercício-16",
    "title": "Lista de Revisão — Probabilidade I",
    "section": "",
    "text": "(Poisson) Chegam em média 6 e-mails por hora, seguindo Poisson.\na) Probabilidade de receber ao menos 1 e-mail nos próximos 10 minutos.\nb) Probabilidade de receber exatamente 5 e-mails em uma hora.\nc) Esperança e variância."
  },
  {
    "objectID": "exercicios/lista02.html#exercício-17",
    "href": "exercicios/lista02.html#exercício-17",
    "title": "Lista de Revisão — Probabilidade I",
    "section": "",
    "text": "(Aproximação Binomial→Poisson) Justifique por que \\(Bin(n,p)\\) com \\(n\\) grande e \\(p\\) pequeno pode ser aproximada por \\(Poisson(\\lambda=np)\\). Aplique para \\(n=200\\), \\(p=0{,}02\\) e estime \\(P(X=0)\\)."
  },
  {
    "objectID": "exercicios/lista02.html#exercício-18",
    "href": "exercicios/lista02.html#exercício-18",
    "title": "Lista de Revisão — Probabilidade I",
    "section": "",
    "text": "(Variáveis Contínuas) Uma v.a. contínua tem f.d.p. \\(f(x)=kx\\) em \\(0\\le x\\le 2\\) e \\(0\\) caso contrário.\na) Determine \\(k\\).\nb) Calcule \\(E[X]\\), \\(Var(X)\\) e \\(P(1&lt;X&lt;1{,}5)\\)."
  },
  {
    "objectID": "exercicios/lista02.html#exercício-19",
    "href": "exercicios/lista02.html#exercício-19",
    "title": "Lista de Revisão — Probabilidade I",
    "section": "",
    "text": "(Uniforme Contínua) Se \\(X\\sim U(a,b)\\), mostre que \\(E[X]=(a+b)/2\\) e \\(Var(X)=(b-a)^2/12\\). Aplique para \\(a=2\\), \\(b=8\\)."
  },
  {
    "objectID": "exercicios/lista02.html#exercício-20",
    "href": "exercicios/lista02.html#exercício-20",
    "title": "Lista de Revisão — Probabilidade I",
    "section": "",
    "text": "(Exponencial) Para \\(X\\sim Exp(\\lambda)\\):\na) mostre a propriedade sem memória \\(P(X&gt;s+t\\mid X&gt;t)=P(X&gt;s)\\);\nb) com média \\(5\\) min, qual a probabilidade de o tempo entre chegadas exceder \\(8\\) min?"
  },
  {
    "objectID": "exercicios/lista02.html#exercício-21",
    "href": "exercicios/lista02.html#exercício-21",
    "title": "Lista de Revisão — Probabilidade I",
    "section": "",
    "text": "(Normal) Seja \\(Z\\sim N(0,1)\\). Calcule:\na) \\(P(-1{,}2&lt;Z&lt;0{,}7)\\);\nb) \\(z_{0{,}975}\\);\nc) \\(P(Z&gt;z_{0{,}9})\\)."
  },
  {
    "objectID": "exercicios/lista02.html#exercício-22",
    "href": "exercicios/lista02.html#exercício-22",
    "title": "Lista de Revisão — Probabilidade I",
    "section": "",
    "text": "(Aproximação Normal da Binomial) Para \\(X\\sim Bin(200,0{,}4)\\), estime \\(P(X\\le 70)\\) usando a aproximação normal com correção de continuidade. Compare com o valor exato (pode descrever o procedimento)."
  },
  {
    "objectID": "exercicios/lista02.html#exercício-23",
    "href": "exercicios/lista02.html#exercício-23",
    "title": "Lista de Revisão — Probabilidade I",
    "section": "",
    "text": "(Aproximação Normal da Poisson) Para \\(Y\\sim Poisson(40)\\), estime \\(P(35\\le Y\\le 50)\\) via aproximação normal com correção de continuidade."
  },
  {
    "objectID": "exercicios/lista02.html#exercício-24",
    "href": "exercicios/lista02.html#exercício-24",
    "title": "Lista de Revisão — Probabilidade I",
    "section": "",
    "text": "(Momentos) Mostre que \\(Var(X)=E[X^2]-\\{E[X]\\}^2\\). Use para calcular a variância de \\(X\\sim U(0,1)\\) sem integrar \\(x^2\\) diretamente."
  },
  {
    "objectID": "exercicios/lista02.html#exercício-25",
    "href": "exercicios/lista02.html#exercício-25",
    "title": "Lista de Revisão — Probabilidade I",
    "section": "",
    "text": "(Desigualdade de Jensen) Seja \\(\\phi\\) convexa e \\(X\\) uma v.a. com \\(E[|X|]&lt;\\infty\\). Enuncie Jensen e aplique com \\(\\phi(x)=x^2\\) para mostrar que \\(E[X]^2\\le E[X^2]\\)."
  },
  {
    "objectID": "exercicios/lista02.html#exercício-26",
    "href": "exercicios/lista02.html#exercício-26",
    "title": "Lista de Revisão — Probabilidade I",
    "section": "",
    "text": "(Momentos de Ordem Superior) Para \\(X\\sim Exp(\\lambda)\\), calcule \\(E[X^k]\\) para \\(k=1,2,3\\) usando integrais ou a função gama."
  },
  {
    "objectID": "exercicios/lista02.html#exercício-27",
    "href": "exercicios/lista02.html#exercício-27",
    "title": "Lista de Revisão — Probabilidade I",
    "section": "",
    "text": "(Transformações) Se \\(X\\sim U(0,1)\\) e \\(Y=-\\ln(1-X)\\), mostre que \\(Y\\sim Exp(1)\\). Dica: use a regra de transformação monotônica via f.d.a."
  },
  {
    "objectID": "exercicios/lista02.html#exercício-28",
    "href": "exercicios/lista02.html#exercício-28",
    "title": "Lista de Revisão — Probabilidade I",
    "section": "",
    "text": "(Independência e Covariância) Duas v.a.’s \\(X\\) e \\(Y\\) têm \\(E[X]=E[Y]=0\\), \\(Var(X)=Var(Y)=1\\) e \\(Cov(X,Y)=\\rho\\).\na) Mostre que \\(Var(X+Y)=2(1+\\rho)\\).\nb) Para quais valores de \\(\\rho\\) a variância de \\(X+Y\\) é mínima e máxima?"
  },
  {
    "objectID": "exercicios/lista02.html#exercício-29",
    "href": "exercicios/lista02.html#exercício-29",
    "title": "Lista de Revisão — Probabilidade I",
    "section": "",
    "text": "(Bayes com Testes) Um teste tem sensibilidade 0,95 e especificidade 0,98. A prevalência da doença é 3%.\na) Qual o valor preditivo positivo?\nb) E o negativo? Interprete."
  },
  {
    "objectID": "exercicios/lista02.html#exercício-30",
    "href": "exercicios/lista02.html#exercício-30",
    "title": "Lista de Revisão — Probabilidade I",
    "section": "",
    "text": "(Modelagem de Chegadas) Chegadas seguem Poisson com taxa \\(12\\)/h.\na) Distribuição e média do tempo entre chegadas.\nb) Probabilidade de esperar mais de 7 min pela próxima chegada.\nc) Tempo mediano entre chegadas."
  },
  {
    "objectID": "exercicios/lista02.html#exercício-31",
    "href": "exercicios/lista02.html#exercício-31",
    "title": "Lista de Revisão — Probabilidade I",
    "section": "",
    "text": "(Misturas) Seja \\(X\\) mistura: com prob. 0,7 vem de \\(Exp(1)\\) e com prob. 0,3 de \\(Exp(2)\\).\na) Encontre a f.d.p. de \\(X\\).\nb) Calcule \\(E[X]\\) e comente sobre a propriedade sem memória."
  },
  {
    "objectID": "exercicios/lista02.html#exercício-32",
    "href": "exercicios/lista02.html#exercício-32",
    "title": "Lista de Revisão — Probabilidade I",
    "section": "",
    "text": "(Aproximações e Limites) Para \\(n\\) grande, use o Teorema Central do Limite para aproximar \\(P\\!\\left(\\frac{1}{n}\\sum_{i=1}^n X_i \\in [\\mu-0{,}1,\\mu+0{,}1]\\right)\\) quando \\(X_i\\sim U(0,1)\\) i.i.d. Determine \\(n\\) mínimo para que a probabilidade seja ao menos \\(0{,}95\\) (use quantil normal padrão)."
  },
  {
    "objectID": "exercicios/lista02.html#bernoulli-p",
    "href": "exercicios/lista02.html#bernoulli-p",
    "title": "Revisão Probabilidade I",
    "section": "3.1 Bernoulli \\((p)\\)",
    "text": "3.1 Bernoulli \\((p)\\)\nInterpretação: 1 sucesso/fracasso em único ensaio.\n\n\n\n\n\n\n\nItem\nExpressão\n\n\n\n\nSuporte\n\\(x\\in\\{0,1\\}\\)\n\n\nParâmetro\n\\(0&lt;p&lt;1\\)\n\n\nFunção de probabilidade \\(p(x)\\)\n\\(p^x(1-p)^{1-x}\\)\n\n\nDistribuição \\(F(x)\\)\n\\(0\\) se \\(x&lt;0\\); \\(1-p\\) se \\(0\\le x&lt;1\\); \\(1\\) se \\(x\\ge1\\)\n\n\nEsperança \\(E[X]\\)\n\\(p\\)\n\n\nVariância \\(Var(X)\\)\n\\(p(1-p)\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMostrar demonstração\n\n\n\n\n\nA variável Bernoulli assume valores \\(0\\) e \\(1\\), com \\(P(X=1)=p\\) e \\(P(X=0)=1-p\\).\nEsperança\n\\[\nE(X)=0(1-p)+1\\cdot p = p.\n\\]\nSegundo momento\n\\[\nE(X^2)=0^2(1-p)+1^2\\cdot p = p.\n\\]\nVariância\n\\[\n\\mathrm{Var}(X)=E(X^2)-[E(X)]^2\n= p - p^2 = p(1-p).\n\\]\n\n\n\nExemplo: Um alarme dispara corretamente com probabilidade \\(p = 0{,}92\\). Seja \\(X\\) = 1 se o alarme dispara corretamente, 0 caso contrário.\n\nCalcule \\(P(X=1)\\) e \\(P(X=0)\\).\n\nCalcule \\(E[X]\\) e interprete.\n\nSolução:\n\n\\(P(X=1)=0{,}92\\)\n\n\\(P(X=0)=0{,}08\\)\n\n\\[\nE[X]=p=0{,}92\n\\]\nInterpretação: o alarme funciona corretamente em 92% dos acionamentos."
  },
  {
    "objectID": "exercicios/lista02.html#binomial-np",
    "href": "exercicios/lista02.html#binomial-np",
    "title": "Revisão Probabilidade I",
    "section": "3.2 Binomial \\((n,p)\\)",
    "text": "3.2 Binomial \\((n,p)\\)\nInterpretação: número de sucessos em \\(n\\) ensaios independentes, prob. \\(p\\).\n\n\n\nItem\nExpressão\n\n\n\n\nSuporte\n\\(k=0,1,\\dots,n\\)\n\n\nParâmetros\n\\(n\\in\\mathbb{N}\\), \\(0&lt;p&lt;1\\)\n\n\n\\(p(k)\\)\n\\(\\binom{n}{k}p^k(1-p)^{n-k}\\)\n\n\n\\(F(k)\\)\n\\(\\sum_{j=0}^{k}\\binom{n}{j}p^j(1-p)^{n-j}\\)\n\n\n\\(E[X]\\)\n\\(np\\)\n\n\n\\(Var(X)\\)\n\\(np(1-p)\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMostrar demonstração\n\n\n\n\n\nConsidere \\(X \\sim \\text{Binomial}(n,p)\\), isto é, \\[\nP(X = k) = \\binom{n}{k} p^k (1-p)^{n-k}, \\quad k = 0,1,\\dots,n.\n\\]\nVamos demonstrar:\n\n\\(E(X) = np\\)\n\\(\\mathrm{Var}(X) = np(1-p)\\)\n\nusando apenas as definições: \\[\nE(X) = \\sum_{k=0}^n k\\,P(X=k),\n\\qquad\n\\mathrm{Var}(X) = E(X^2) - [E(X)]^2.\n\\]\n\n\n3.2.1 1. Cálculo de \\(E(X)\\)\nPela definição: \\[\nE(X) = \\sum_{k=0}^n k\\,P(X=k)\n     = \\sum_{k=0}^n k \\binom{n}{k} p^k (1-p)^{n-k}.\n\\]\nNote que o termo com \\(k=0\\) é zero (pois tem um fator \\(k\\)), então podemos começar em \\(k=1\\): \\[\nE(X) = \\sum_{k=1}^n k \\binom{n}{k} p^k (1-p)^{n-k}.\n\\]\nUsamos agora a identidade combinatória \\[\nk \\binom{n}{k} = n \\binom{n-1}{k-1}.\n\\]\nSubstituindo: \\[\nE(X)\n= \\sum_{k=1}^n n \\binom{n-1}{k-1} p^k (1-p)^{n-k}.\n\\]\nColocamos \\(n\\) em evidência: \\[\nE(X)\n= n \\sum_{k=1}^n \\binom{n-1}{k-1} p^k (1-p)^{n-k}.\n\\]\nAgora fazemos a mudança de índice \\(j = k-1\\):\n\nquando \\(k = 1\\), \\(j = 0\\);\n\nquando \\(k = n\\), \\(j = n-1\\);\n\n\\(k = j + 1\\).\n\nEntão: \\[\nE(X)\n= n \\sum_{j=0}^{n-1} \\binom{n-1}{j} p^{j+1} (1-p)^{n-(j+1)}.\n\\]\nReorganizando as potências: \\[\np^{j+1} = p \\cdot p^j,\n\\qquad\nn-(j+1) = (n-1)-j,\n\\] obtemos: \\[\nE(X)\n= n p \\sum_{j=0}^{n-1} \\binom{n-1}{j} p^{j} (1-p)^{(n-1)-j}.\n\\]\nRepare que a soma \\[\n\\sum_{j=0}^{n-1} \\binom{n-1}{j} p^{j} (1-p)^{(n-1)-j}\n\\] é exatamente o desenvolvimento binomial de \\[\n(p + (1-p))^{n-1} = 1^{n-1} = 1.\n\\]\nPortanto: \\[\nE(X) = n p \\cdot 1 = np.\n\\]\n\n\n\n3.2.2 2. Cálculo de \\(E(X^2)\\)\nUsaremos: \\[\n\\mathrm{Var}(X) = E(X^2) - [E(X)]^2,\n\\] então precisamos de \\(E(X^2)\\).\nPela definição: \\[\nE(X^2) = \\sum_{k=0}^n k^2\\,P(X=k)\n       = \\sum_{k=0}^n k^2 \\binom{n}{k} p^k (1-p)^{n-k}.\n\\]\nTruque clássico: escrever \\[\nk^2 = k(k-1) + k.\n\\]\nEntão: \\[\nE(X^2)\n= \\sum_{k=0}^n [k(k-1) + k] \\binom{n}{k} p^k (1-p)^{n-k}\n= \\sum_{k=0}^n k(k-1)\\binom{n}{k} p^k (1-p)^{n-k}\n  + \\sum_{k=0}^n k\\binom{n}{k} p^k (1-p)^{n-k}.\n\\]\nChamemos: - \\(A = \\sum_{k=0}^n k(k-1)\\binom{n}{k} p^k (1-p)^{n-k}\\)\n- \\(B = \\sum_{k=0}^n k\\binom{n}{k} p^k (1-p)^{n-k}\\)\nLogo, \\(E(X^2) = A + B\\).\nMas repare que \\(B = E(X)\\), que já calculamos: \\[\nB = E(X) = np.\n\\]\nFalta calcular \\(A\\).\n\n\n3.2.2.1 2.1 Cálculo de \\(A\\)\nUsamos agora a identidade: \\[\nk(k-1)\\binom{n}{k} = n(n-1)\\binom{n-2}{k-2}.\n\\]\nEntão: \\[\nA = \\sum_{k=0}^n k(k-1)\\binom{n}{k} p^k (1-p)^{n-k}\n  = \\sum_{k=2}^n n(n-1)\\binom{n-2}{k-2} p^k (1-p)^{n-k}.\n\\]\nPodemos tirar \\(n(n-1)\\) em evidência: \\[\nA = n(n-1)\\sum_{k=2}^n \\binom{n-2}{k-2} p^k (1-p)^{n-k}.\n\\]\nAgora faça a mudança de índice \\(j = k-2\\):\n\nquando \\(k = 2\\), \\(j = 0\\);\n\nquando \\(k = n\\), \\(j = n-2\\);\n\n\\(k = j + 2\\).\n\nSubstituindo: \\[\nA = n(n-1)\\sum_{j=0}^{n-2} \\binom{n-2}{j} p^{j+2} (1-p)^{n-(j+2)}.\n\\]\nReorganizando as potências: \\[\np^{j+2} = p^2 p^j,\n\\qquad\nn-(j+2) = (n-2)-j,\n\\] temos: \\[\nA = n(n-1)p^2 \\sum_{j=0}^{n-2} \\binom{n-2}{j} p^{j} (1-p)^{(n-2)-j}.\n\\]\nA soma é novamente um binômio: \\[\n\\sum_{j=0}^{n-2} \\binom{n-2}{j} p^{j} (1-p)^{(n-2)-j}\n= (p + (1-p))^{n-2} = 1^{n-2} = 1.\n\\]\nPortanto: \\[\nA = n(n-1)p^2.\n\\]\n\n\n\n3.2.2.2 2.2 Conclusão para \\(E(X^2)\\)\nLembrando que: - \\(A = n(n-1)p^2\\)\n- \\(B = np\\)\ntemos: \\[\nE(X^2) = A + B = n(n-1)p^2 + np.\n\\]\n\n\n\n\n3.2.3 3. Cálculo de \\(\\mathrm{Var}(X)\\)\nAgora usamos: \\[\n\\mathrm{Var}(X) = E(X^2) - [E(X)]^2.\n\\]\nJá temos:\n\n\\(E(X^2) = n(n-1)p^2 + np\\)\n\\(E(X) = np\\)\n\nLogo: \\[\n\\mathrm{Var}(X)\n= \\big(n(n-1)p^2 + np\\big) - (np)^2.\n\\]\nAgora expandimos: \\[\n(np)^2 = n^2 p^2,\n\\] então: \\[\n\\mathrm{Var}(X)\n= n(n-1)p^2 + np - n^2 p^2.\n\\]\nNote que \\[\nn(n-1)p^2 = (n^2 - n)p^2,\n\\] então: \\[\n\\mathrm{Var}(X)\n= (n^2 - n)p^2 + np - n^2 p^2\n= -n p^2 + np\n= n p (1-p).\n\\]\n\n\n\n3.2.4 Resultado final\nPara \\(X \\sim \\text{Binomial}(n,p)\\), demonstramos a partir das definições que \\[\nE(X) = np,\n\\qquad\n\\mathrm{Var}(X) = np(1-p).\n\\]\n\n\n\n\nExemplo: A probabilidade de um cliente comprar um produto é \\(p=0{,}3\\). Em um dia, 20 clientes entram na loja. Seja \\(X\\) = número de compras.\n\nCalcule \\(P(X=8)\\).\n\nCalcule \\(E[X]\\).\n\nSolução:\n\n\\[\nP(X=8)=\\binom{20}{8}(0{,}3)^8(0{,}7)^{12}\\approx 0{,}053\n\\]\n\\[\nE[X]=np=20\\cdot 0{,}3 = 6\n\\]"
  },
  {
    "objectID": "exercicios/lista02.html#geométrica-p",
    "href": "exercicios/lista02.html#geométrica-p",
    "title": "Revisão Probabilidade I",
    "section": "3.3 Geométrica \\((p)\\)",
    "text": "3.3 Geométrica \\((p)\\)\nConvenção usada: \\(X\\) = número de ensaios até o 1º sucesso (apoio \\(1,2,\\dots\\)).\n\n\n\nItem\nExpressão\n\n\n\n\n\\(p(k)\\)\n\\(p(1-p)^{k-1}\\)\n\n\n\\(F(k)\\)\n\\(1-(1-p)^k\\)\n\n\n\\(E[X]\\)\n\\(1/p\\)\n\n\n\\(Var(X)\\)\n\\((1-p)/p^2\\)\n\n\nPropriedade\nSem memória: \\(P(X&gt;s+t\\mid X&gt;t)=P(X&gt;s)\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMostrar demonstração\n\n\n\n\n\nConsidere \\(X \\sim \\text{Geom}(p)\\) com suporte \\(k=1,2,\\dots\\), isto é, \\[\nP(X = k) = p(1-p)^{k-1}, \\quad k = 1,2,\\dots,\\quad 0&lt;p&lt;1.\n\\]\nVamos demonstrar que \\[\nE(X) = \\frac{1}{p}\n\\qquad \\text{e} \\qquad\n\\mathrm{Var}(X) = \\frac{1-p}{p^2},\n\\] usando as definições \\[\nE(X) = \\sum_{k=1}^{\\infty} k\\,P(X=k),\n\\qquad\n\\mathrm{Var}(X) = E(X^2) - [E(X)]^2.\n\\]\n\n\n3.3.1 1. Cálculo de \\(E(X)\\)\nPela definição de esperança: \\[\nE(X) = \\sum_{k=1}^{\\infty} k\\,P(X=k)\n     = \\sum_{k=1}^{\\infty} k\\,p(1-p)^{k-1}.\n\\]\nColocamos o fator \\(p\\) em evidência: \\[\nE(X) = p \\sum_{k=1}^{\\infty} k (1-p)^{k-1}.\n\\]\nPara simplificar, definimos \\[\nr = 1-p.\n\\]\nNote que \\(0&lt;r&lt;1\\). Então a soma fica \\[\nE(X) = p \\sum_{k=1}^{\\infty} k r^{k-1}.\n\\]\nAgora usamos uma identidade clássica de séries: - Sabemos que, para \\(|r|&lt;1\\), \\[\n  \\sum_{k=0}^{\\infty} r^k = \\frac{1}{1-r}.\n  \\]\nDerivando em relação a \\(r\\): \\[\n\\frac{d}{dr}\\left(\\sum_{k=0}^{\\infty} r^k\\right)\n= \\sum_{k=0}^{\\infty} k r^{k-1}\n= \\frac{d}{dr}\\left(\\frac{1}{1-r}\\right)\n= \\frac{1}{(1-r)^2}.\n\\]\nNote que o termo \\(k=0\\) é zero, então \\[\n\\sum_{k=1}^{\\infty} k r^{k-1} = \\frac{1}{(1-r)^2}.\n\\]\nVoltando à expressão de \\(E(X)\\): \\[\nE(X) = p \\cdot \\frac{1}{(1-r)^2}.\n\\]\nLembrando que \\(r = 1-p\\), então \\[\n1-r = 1 - (1-p) = p.\n\\]\nLogo: \\[\nE(X) = p \\cdot \\frac{1}{p^2} = \\frac{1}{p}.\n\\]\n\n\n\n3.3.2 2. Cálculo de \\(E(X^2)\\)\nPela definição: \\[\nE(X^2) = \\sum_{k=1}^{\\infty} k^2 P(X=k)\n       = \\sum_{k=1}^{\\infty} k^2 p(1-p)^{k-1}\n       = p \\sum_{k=1}^{\\infty} k^2 r^{k-1},\n\\] com \\(r = 1-p\\).\nEntão precisamos do valor de \\[\n\\sum_{k=1}^{\\infty} k^2 r^{k-1}.\n\\]\nUsaremos de novo a série geométrica e suas derivadas.\nJá sabemos: \\[\n\\sum_{k=0}^{\\infty} r^k = \\frac{1}{1-r}.\n\\]\n\nPrimeira derivada: \\[\n\\sum_{k=1}^{\\infty} k r^{k-1} = \\frac{1}{(1-r)^2}.\n\\]\nMultiplicando por \\(r\\): \\[\nr \\sum_{k=1}^{\\infty} k r^{k-1}\n= \\sum_{k=1}^{\\infty} k r^{k}\n= \\frac{r}{(1-r)^2}.\n\\]\nDerivando novamente: Vamos derivar a expressão \\[\n\\sum_{k=1}^{\\infty} k r^{k}\n\\] em relação a \\(r\\): \\[\n\\frac{d}{dr}\\left(\\sum_{k=1}^{\\infty} k r^{k}\\right)\n= \\sum_{k=1}^{\\infty} k^2 r^{k-1}.\n\\]\nAgora derivamos o outro lado: \\[\n\\frac{d}{dr}\\left(\\frac{r}{(1-r)^2}\\right)\n= \\frac{(1-r)^2 - r\\cdot 2(1-r)(-1)}{(1-r)^4}.\n\\]\nSimplificando o numerador: \\[\n(1-r)^2 + 2r(1-r)\n= (1 - 2r + r^2) + (2r - 2r^2)\n= 1 - r^2\n= (1-r)(1+r).\n\\]\nAssim, \\[\n\\frac{d}{dr}\\left(\\frac{r}{(1-r)^2}\\right)\n= \\frac{(1-r)(1+r)}{(1-r)^4}\n= \\frac{1+r}{(1-r)^3}.\n\\]\n\nPortanto, \\[\n\\sum_{k=1}^{\\infty} k^2 r^{k-1}\n= \\frac{1+r}{(1-r)^3}.\n\\]\nVoltando para \\(E(X^2)\\): \\[\nE(X^2) = p \\cdot \\frac{1+r}{(1-r)^3}.\n\\]\nSubstituímos \\(r=1-p\\) e \\(1-r=p\\):\n\n\\(1+r = 1 + (1-p) = 2-p\\)\n\n\\((1-r)^3 = p^3\\)\n\nEntão: \\[\nE(X^2)= p \\cdot \\frac{2-p}{p^3}\n      = \\frac{2-p}{p^2}.\n\\]\n\n\n\n3.3.3 3. Cálculo de \\(\\mathrm{Var}(X)\\)\nAgora usamos \\[\n\\mathrm{Var}(X) = E(X^2) - [E(X)]^2.\n\\]\nJá encontramos:\n\n\\(E(X) = \\dfrac{1}{p}\\)\n\\(E(X^2) = \\dfrac{2-p}{p^2}\\)\n\nLogo: \\[\n\\mathrm{Var}(X)\n= \\frac{2-p}{p^2} - \\left(\\frac{1}{p}\\right)^2\n= \\frac{2-p}{p^2} - \\frac{1}{p^2}\n= \\frac{1-p}{p^2}.\n\\]\n\n\n\n3.3.4 Resultado final\nPara \\(X \\sim \\text{Geom}(p)\\), com \\(P(X=k)=p(1-p)^{k-1}\\), \\(k=1,2,\\dots\\), temos:\n\\[\nE(X) = \\frac{1}{p},\n\\qquad\n\\mathrm{Var}(X) = \\frac{1-p}{p^2}.\n\\]\n\n\n\n\nExemplo: Uma chamada telefônica é atendida com probabilidade \\(p = 0{,}15\\). Seja \\(X\\) = número de tentativas até o primeiro atendimento.\n\nCalcule \\(P(X=4)\\).\n\nCalcule \\(P(X&gt;4)\\).\n\nDetermine \\(E[X]\\).\n\nSolução:\n\n\\[\nP(X=4)=0{,}15(0{,}85)^3 \\approx 0{,}092\n\\]\n\\[\nP(X&gt;4)=0{,}85^4 \\approx 0{,}522\n\\]\n\\[\nE[X]=\\frac{1}{0{,}15}\\approx 6{,}67\n\\]"
  },
  {
    "objectID": "exercicios/lista02.html#pascal-negativa-rp",
    "href": "exercicios/lista02.html#pascal-negativa-rp",
    "title": "Revisão Probabilidade I",
    "section": "3.4 Pascal / Negativa \\((r,p)\\)",
    "text": "3.4 Pascal / Negativa \\((r,p)\\)\nConvenção usada: \\(Y\\) = número de falhas antes do \\(r\\)-ésimo sucesso (apoio \\(0,1,\\dots\\)).\n\n\n\nItem\nExpressão\n\n\n\n\n\\(p(k)\\)\n\\(\\binom{k+r-1}{k}(1-p)^k p^r\\)\n\n\n\\(F(k)\\)\n\\(\\sum_{j=0}^{k}\\binom{j+r-1}{j}(1-p)^j p^r\\)\n\n\n\\(E[Y]\\)\n\\(r(1-p)/p\\)\n\n\n\\(Var(Y)\\)\n\\(r(1-p)/p^2\\)\n\n\n\n\n\n\n\n\n\n\n\n\nExemplo: Um pesquisador precisa de 4 pessoas que aceitem responder um questionário. Cada tentativa tem probabilidade \\(p=0{,}25\\) de sucesso. Seja \\(Y\\) = número de recusas até o 4º sucesso.\n\nCalcule \\(P(Y=6)\\).\n\nCalcule \\(E[Y]\\).\n\nSolução:\n\n\\[\nP(Y=6)=\\binom{9}{6}(0{,}75)^6 (0{,}25)^4 \\approx 0{,}050\n\\]\n\\[\nE[Y]=\\frac{4(1-0{,}25)}{0{,}25}=12\n\\]"
  },
  {
    "objectID": "exercicios/lista02.html#hipergeométrica-nkn",
    "href": "exercicios/lista02.html#hipergeométrica-nkn",
    "title": "Revisão Probabilidade I",
    "section": "3.5 Hipergeométrica \\((N,K,n)\\)",
    "text": "3.5 Hipergeométrica \\((N,K,n)\\)\nAmostragem sem reposição. \\(N\\) total, \\(K\\) sucessos na população, amostra \\(n\\).\n\n\n\n\n\n\n\nItem\nExpressão\n\n\n\n\nSuporte\n\\(k=\\max(0,n-(N-K)),\\dots,\\min(n,K)\\)\n\n\n\\(p(k)\\)\n\\(\\dfrac{\\binom{K}{k}\\binom{N-K}{n-k}}{\\binom{N}{n}}\\)\n\n\n\\(E[X]\\)\n\\(n\\,\\dfrac{K}{N}\\)\n\n\n\\(Var(X)\\)\n\\(n\\,\\dfrac{K}{N}\\!\\left(1-\\dfrac{K}{N}\\right)\\!\\dfrac{N-n}{N-1}\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMostrar demonstração\n\n\n\n\n\nConsidere:\n\nUma população finita com \\(N\\) elementos.\n\\(K\\) desses \\(N\\) elementos são “sucessos” (por exemplo, peças defeituosas).\nOs demais \\(N-K\\) são “fracassos”.\nRetiramos uma amostra sem reposição de tamanho \\(n\\).\n\nSeja \\(X\\) o número de sucessos na amostra. Dizemos que \\[\nX \\sim \\text{Hipergeométrica}(N, K, n).\n\\]\nA função de probabilidade é \\[\nP(X = k)\n= \\frac{\\binom{K}{k}\\binom{N-K}{n-k}}{\\binom{N}{n}},\n\\quad k = 0,1,\\dots,\\min(K,n).\n\\]\nQueremos demonstrar que: \\[\nE(X) = n \\frac{K}{N},\n\\qquad\n\\mathrm{Var}(X) = n\\frac{K}{N}\\left(1-\\frac{K}{N}\\right)\\frac{N-n}{N-1}.\n\\]\nEm vez de somar diretamente a f.p., vamos usar uma abordagem com variáveis indicadoras.\n\n\n3.5.1 1. Representando \\(X\\) como soma de indicadores\nImagine que a amostra de tamanho \\(n\\) é extraída em ordem: 1ª retirada, 2ª retirada, …, \\(n\\)-ésima retirada.\nDefina, para cada \\(i=1,\\dots,n\\): \\[\nY_i =\n\\begin{cases}\n1, & \\text{se o $i$-ésimo elemento sorteado é sucesso;}\\\\[4pt]\n0, & \\text{caso contrário.}\n\\end{cases}\n\\]\nEntão o número total de sucessos na amostra é \\[\nX = Y_1 + Y_2 + \\cdots + Y_n.\n\\]\nIsso é intuitivo: cada \\(Y_i\\) indica se houve sucesso naquela retirada, e a soma conta quantos sucessos houve ao todo.\n\n\n\n3.5.2 2. Esperança de \\(X\\) via linearidade\nUsando linearidade da esperança: \\[\nE(X) = E(Y_1 + Y_2 + \\cdots + Y_n)\n     = E(Y_1) + E(Y_2) + \\cdots + E(Y_n).\n\\]\nComo a população é homogênea e a amostragem é simétrica, todas as retiradas têm a mesma probabilidade de ser sucesso. Ou seja: \\[\nE(Y_1)=E(Y_2)=\\cdots=E(Y_n).\n\\]\nBasta, então, calcular \\(E(Y_1)\\).\n\n\n\n3.5.3 3. Cálculo de \\(E(Y_i)\\)\nPor definição, \\[\nE(Y_i) = P(Y_i=1).\n\\]\nMas \\(Y_i=1\\) significa que, na \\(i\\)-ésima retirada, escolhemos um elemento “sucesso”.\nComo as retiradas são todas igualmente prováveis (sem viés) e a população tem \\(K\\) sucessos em \\(N\\) elementos, temos: \\[\nP(Y_i=1) = \\frac{K}{N},\n\\quad \\text{para qualquer } i.\n\\]\nLogo, \\[\nE(Y_i) = \\frac{K}{N}.\n\\]\nPortanto, \\[\nE(X)\n= \\sum_{i=1}^n E(Y_i)\n= n \\cdot \\frac{K}{N}.\n\\]\nIsso demonstra: \\[\nE(X) = n \\frac{K}{N}.\n\\]\n\n\n\n3.5.4 4. Variância de \\(X\\): estratégia\nQueremos agora \\[\n\\mathrm{Var}(X) = \\mathrm{Var}(Y_1+\\cdots+Y_n).\n\\]\nLembre que, em geral, para variáveis quaisquer: \\[\n\\mathrm{Var}\\left(\\sum_{i=1}^n Y_i\\right)\n= \\sum_{i=1}^n \\mathrm{Var}(Y_i) + 2\\sum_{1\\le i&lt;j\\le n} \\mathrm{Cov}(Y_i, Y_j).\n\\]\nAs \\(Y_i\\) não são independentes (pois a amostragem é sem reposição), então precisamos levar em conta as covariâncias.\nPela simetria do problema:\n\nTodas as variâncias \\(\\mathrm{Var}(Y_i)\\) são iguais.\nTodas as covariâncias \\(\\mathrm{Cov}(Y_i,Y_j)\\) com \\(i \\ne j\\) são iguais.\n\nEntão podemos escrever: \\[\n\\mathrm{Var}(X)\n= n\\,\\mathrm{Var}(Y_1) + 2\\binom{n}{2}\\mathrm{Cov}(Y_1,Y_2)\n= n\\,\\mathrm{Var}(Y_1) + n(n-1)\\,\\mathrm{Cov}(Y_1,Y_2).\n\\]\nAssim, precisamos calcular:\n\n\\(\\mathrm{Var}(Y_1)\\)\n\n\\(\\mathrm{Cov}(Y_1,Y_2)\\)\n\n\n\n\n3.5.5 5. Cálculo de \\(\\mathrm{Var}(Y_1)\\)\nComo \\(Y_1\\) é uma variável indicadora (0 ou 1) com \\[\nP(Y_1=1) = \\frac{K}{N},\n\\quad\nP(Y_1=0) = 1 - \\frac{K}{N},\n\\] temos: \\[\nE(Y_1) = \\frac{K}{N},\\qquad\nE(Y_1^2) = E(Y_1) = \\frac{K}{N}\n\\] (pois \\(Y_1^2 = Y_1\\) quando \\(Y_1 \\in \\{0,1\\}\\)).\nLogo: \\[\n\\mathrm{Var}(Y_1) = E(Y_1^2) - [E(Y_1)]^2\n= \\frac{K}{N} - \\left(\\frac{K}{N}\\right)^2\n= \\frac{K}{N}\\left(1-\\frac{K}{N}\\right)\n= \\frac{K}{N}\\cdot\\frac{N-K}{N}\n= \\frac{K(N-K)}{N^2}.\n\\]\n\n\n\n3.5.6 6. Cálculo de \\(\\mathrm{Cov}(Y_1,Y_2)\\)\nPor definição: \\[\n\\mathrm{Cov}(Y_1,Y_2)\n= E(Y_1Y_2) - E(Y_1)E(Y_2).\n\\]\nJá sabemos que \\[\nE(Y_1) = E(Y_2) = \\frac{K}{N}.\n\\]\nEntão precisamos de \\(E(Y_1Y_2)\\), que é \\[\nE(Y_1Y_2) = P(Y_1=1 \\text{ e } Y_2=1),\n\\] pois \\(Y_1Y_2=1\\) somente quando ambos são 1.\n\n\n3.5.6.1 6.1 Cálculo de \\(P(Y_1=1, Y_2=1)\\)\nInterprete o sorteio em duas etapas, sem reposição:\n\nPrimeira retirada é sucesso: probabilidade \\(K/N\\).\nDada uma primeira retirada de sucesso, restam:\n\n\\(K-1\\) sucessos\nem um total de \\(N-1\\) elementos.\n\nEntão a probabilidade de a segunda retirada também ser sucesso é: \\[\n\\frac{K-1}{N-1}.\n\\]\n\nLogo: \\[\nP(Y_1=1, Y_2=1)\n= \\frac{K}{N} \\cdot \\frac{K-1}{N-1}\n= \\frac{K(K-1)}{N(N-1)}.\n\\]\nPortanto: \\[\nE(Y_1Y_2) = \\frac{K(K-1)}{N(N-1)}.\n\\]\n\n\n\n3.5.6.2 6.2 Covariância\nAgora: \\[\n\\mathrm{Cov}(Y_1,Y_2)\n= E(Y_1Y_2) - E(Y_1)E(Y_2)\n= \\frac{K(K-1)}{N(N-1)} - \\left(\\frac{K}{N}\\right)^2.\n\\]\nVamos colocar os termos no mesmo denominador. Note que \\[\n\\left(\\frac{K}{N}\\right)^2 = \\frac{K^2}{N^2}\n= \\frac{K^2(N-1)}{N^2(N-1)}.\n\\]\nE \\[\n\\frac{K(K-1)}{N(N-1)}\n= \\frac{K(K-1)N}{N^2(N-1)}.\n\\]\nEntão: \\[\n\\mathrm{Cov}(Y_1,Y_2)\n= \\frac{K(K-1)N - K^2(N-1)}{N^2(N-1)}.\n\\]\nSimplificando o numerador: \\[\nK(K-1)N - K^2(N-1)\n= K[ N(K-1) - K(N-1) ].\n\\]\nDentro dos colchetes: \\[\nN(K-1) - K(N-1)\n= (NK - N) - (KN - K)\n= NK - N - KN + K\n= -N + K\n= K - N.\n\\]\nLogo o numerador é: \\[\nK(K-N) = -K(N-K).\n\\]\nPortanto: \\[\n\\mathrm{Cov}(Y_1,Y_2)\n= \\frac{-K(N-K)}{N^2(N-1)}.\n\\]\nIsso mostra que a covariância é negativa: faz sentido, pois sem reposição, ao observar um sucesso na primeira retirada, fica ligeiramente menos provável ver outro sucesso na segunda.\n\n\n\n\n3.5.7 7. Variância de \\(X\\)\nRelembrando: \\[\n\\mathrm{Var}(X)=n\\,\\mathrm{Var}(Y_1) + n(n-1)\\,\\mathrm{Cov}(Y_1,Y_2).\n\\]\nSubstituímos os valores encontrados:\n\n\\(\\mathrm{Var}(Y_1) = \\dfrac{K(N-K)}{N^2}\\)\n\\(\\mathrm{Cov}(Y_1,Y_2) = -\\dfrac{K(N-K)}{N^2(N-1)}\\)\n\nEntão: \\[\n\\mathrm{Var}(X)\n= n \\cdot \\frac{K(N-K)}{N^2}\n+ n(n-1)\\cdot\\left(-\\frac{K(N-K)}{N^2(N-1)}\\right).\n\\]\nColocamos o fator comum \\(\\dfrac{K(N-K)}{N^2}\\) em evidência: \\[\n\\mathrm{Var}(X)\n= \\frac{K(N-K)}{N^2}\n\\left[\nn - \\frac{n(n-1)}{N-1}\n\\right].\n\\]\nAgora vamos simplificar o colchete: \\[\nn - \\frac{n(n-1)}{N-1}\n= n\\left[1 - \\frac{n-1}{N-1}\\right]\n= n\\left[\\frac{N-1 - (n-1)}{N-1}\\right]\n= n\\left[\\frac{N-n}{N-1}\\right].\n\\]\nPortanto: \\[\n\\mathrm{Var}(X)\n= \\frac{K(N-K)}{N^2}\\cdot n\\frac{N-n}{N-1}.\n\\]\nPodemos reescrever \\(K(N-K)/N^2\\) como \\[\n\\frac{K}{N}\\left(1-\\frac{K}{N}\\right).\n\\]\nAssim: \\[\n\\mathrm{Var}(X)\n= n \\frac{K}{N}\\left(1-\\frac{K}{N}\\right)\\frac{N-n}{N-1}.\n\\]\n\n\n\n3.5.8 8. Resultado final\nPara \\(X \\sim \\text{Hipergeométrica}(N,K,n)\\):\n\nEsperança: \\[\nE(X) = n \\frac{K}{N}.\n\\]\nVariância: \\[\n\\mathrm{Var}(X) = n \\frac{K}{N}\\left(1-\\frac{K}{N}\\right)\\frac{N-n}{N-1}.\n\\]\n\n\n\n\n\nExemplo: Um lote tem \\(N=80\\) peças, sendo \\(K=10\\) defeituosas. Retira-se uma amostra de \\(n=12\\) peças. Seja \\(X\\) = número de defeituosas na amostra.\n\nCalcule \\(P(X=2)\\).\n\nCalcule \\(E[X]\\).\n\nSolução:\n\n\\[\nP(X=2)=\\frac{\\binom{10}{2}\\binom{70}{10}}{\\binom{80}{12}}\n\\]\n\nResultado aproximado: 0,283\n\n\\[\nE[X]=12 \\cdot \\frac{10}{80} = 1{,}5\n\\]"
  },
  {
    "objectID": "exercicios/lista02.html#poisson-lambda",
    "href": "exercicios/lista02.html#poisson-lambda",
    "title": "Revisão Probabilidade I",
    "section": "3.6 Poisson \\((\\lambda)\\)",
    "text": "3.6 Poisson \\((\\lambda)\\)\nContagem de eventos raros em intervalo fixo.\n\n\n\nItem\nExpressão\n\n\n\n\nSuporte\n\\(k=0,1,2,\\dots\\)\n\n\n\\(p(k)\\)\n\\(e^{-\\lambda}\\lambda^k/k!\\)\n\n\n\\(F(k)\\)\n\\(\\sum_{j=0}^{k}e^{-\\lambda}\\lambda^j/j!\\)\n\n\n\\(E[X]\\)\n\\(\\lambda\\)\n\n\n\\(Var(X)\\)\n\\(\\lambda\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMostrar demonstração\n\n\n\n\n\n\n3.6.1 Distribuição de Poisson: \\(E(X)\\) e \\(\\mathrm{Var}(X)\\) passo a passo\nConsidere \\(X \\sim \\text{Poisson}(\\lambda)\\), com \\(\\lambda &gt; 0\\).\nA função de probabilidade é: \\[\nP(X=k) = e^{-\\lambda}\\frac{\\lambda^k}{k!},\\qquad k=0,1,2,\\dots\n\\]\nNosso objetivo é demonstrar: \\[\nE(X)=\\lambda,\n\\qquad\n\\mathrm{Var}(X)=\\lambda.\n\\]\nUsaremos somente as definições: \\[\nE(X)=\\sum_{k=0}^{\\infty} k\\,P(X=k), \\qquad\n\\mathrm{Var}(X)=E(X^2)-[E(X)]^2.\n\\]\n\n\n\n3.7 1. Cálculo de \\(E(X)\\)\nPela definição de esperança: \\[\nE(X)=\\sum_{k=0}^{\\infty} k\\,P(X=k)\n    =\\sum_{k=0}^{\\infty} k\\,e^{-\\lambda}\\frac{\\lambda^k}{k!}.\n\\]\nO termo \\(k=0\\) é nulo, então: \\[\nE(X)=e^{-\\lambda}\\sum_{k=1}^{\\infty} k\\frac{\\lambda^k}{k!}.\n\\]\nUsamos a identidade: \\[\nk\\frac{\\lambda^k}{k!}=\\frac{\\lambda^k}{(k-1)!}.\n\\]\nLogo: \\[\nE(X)=e^{-\\lambda}\\sum_{k=1}^{\\infty} \\frac{\\lambda^k}{(k-1)!}.\n\\]\nAgora fazemos a mudança de variável \\(j=k-1\\):\n\nquando \\(k=1\\), \\(j=0\\)\n\nquando \\(k\\to\\infty\\), \\(j\\to\\infty\\)\n\nEntão: \\[\nE(X)=e^{-\\lambda}\\sum_{j=0}^{\\infty} \\frac{\\lambda^{j+1}}{j!}\n= e^{-\\lambda}\\lambda \\sum_{j=0}^{\\infty} \\frac{\\lambda^{j}}{j!}.\n\\]\nMas: \\[\n\\sum_{j=0}^{\\infty} \\frac{\\lambda^{j}}{j!} = e^{\\lambda}.\n\\]\nPortanto: \\[\nE(X)=e^{-\\lambda}\\lambda e^{\\lambda} = \\lambda.\n\\]\nAssim demonstramos: \\[\nE(X)=\\lambda.\n\\]\n\n\n\n3.8 2. Cálculo de \\(E(X^2)\\)\nAgora usamos a definição: \\[\nE(X^2)=\\sum_{k=0}^{\\infty} k^2 P(X=k)\n      =e^{-\\lambda}\\sum_{k=0}^{\\infty} k^2 \\frac{\\lambda^k}{k!}.\n\\]\nTruque clássico: escrever \\[\nk^2 = k(k-1) + k.\n\\]\nEntão: \\[\nE(X^2)\n= e^{-\\lambda}\\sum_{k=0}^{\\infty} k(k-1)\\frac{\\lambda^k}{k!}\n+ e^{-\\lambda}\\sum_{k=0}^{\\infty} k\\frac{\\lambda^k}{k!}.\n\\]\nChamemos:\n\nPrimeiro somatório:\n\\[A = e^{-\\lambda}\\sum_{k=0}^{\\infty} k(k-1)\\frac{\\lambda^k}{k!}\\]\nSegundo somatório:\n\\[B = e^{-\\lambda}\\sum_{k=0}^{\\infty} k\\frac{\\lambda^k}{k!}\\]\n\nMas já vimos antes que \\(B = E(X) = \\lambda\\).\nVamos calcular \\(A\\).\n\n\n3.8.1 2.1 Cálculo do termo \\(A\\)\nNote que: \\[\nk(k-1)\\frac{\\lambda^k}{k!}\n= \\frac{\\lambda^k}{(k-2)!}.\n\\]\nPortanto: \\[\nA = e^{-\\lambda}\\sum_{k=2}^{\\infty}\\frac{\\lambda^k}{(k-2)!}.\n\\]\nFazemos a mudança de variável \\(j=k-2\\):\n\nquando \\(k=2\\), \\(j=0\\)\nquando \\(k\\to\\infty\\), \\(j\\to\\infty\\)\n\nEntão: \\[\nA = e^{-\\lambda}\\sum_{j=0}^{\\infty}\\frac{\\lambda^{j+2}}{j!}\n= e^{-\\lambda}\\lambda^2 \\sum_{j=0}^{\\infty}\\frac{\\lambda^{j}}{j!}.\n\\]\nA soma é novamente \\(e^\\lambda\\), logo: \\[\nA = e^{-\\lambda}\\lambda^2 e^\\lambda = \\lambda^2.\n\\]\n\n\n\n3.8.2 2.2 Conclusão para \\(E(X^2)\\)\nAgora juntamos:\n\n\\(A = \\lambda^2\\)\n\\(B = \\lambda\\)\n\nPortanto: \\[\nE(X^2) = A + B = \\lambda^2 + \\lambda.\n\\]\n\n\n\n\n3.9 3. Variância\nUsamos: \\[\n\\mathrm{Var}(X)=E(X^2)-[E(X)]^2.\n\\]\nSubstituindo:\n\n\\(E(X)=\\lambda\\)\n\\(E(X^2)=\\lambda^2 + \\lambda\\)\n\nTemos: \\[\n\\mathrm{Var}(X)\n= (\\lambda^2+\\lambda) - \\lambda^2\n= \\lambda.\n\\]\n\n\n\n3.10 4. Resultado final\nPara \\(X \\sim \\text{Poisson}(\\lambda)\\):\n\\[\nE(X)=\\lambda,\n\\qquad\n\\mathrm{Var}(X)=\\lambda.\n\\]\n\n\n\n\nExemplo: A taxa média de chamadas em um call center é \\(\\lambda=12\\) chamadas por hora. Seja \\(N\\) = número de chamadas.\n\nCalcule \\(P(N=10)\\).\n\nCalcule \\(P(N\\ge 15)\\).\n\nCalcule \\(E[N]\\) e \\(Var(N)\\).\n\nSolução:\n\n\\[\nP(N=10)=e^{-12}\\frac{12^{10}}{10!}\\approx 0{,}104\n\\]\n\\[\nP(N\\ge 15)=1-P(N\\le 14)\\approx 0{,}263\n\\]\n\\[\nE[N]=12,\\qquad Var(N)=12\n\\]"
  },
  {
    "objectID": "exercicios/lista02.html#uniforme-ab",
    "href": "exercicios/lista02.html#uniforme-ab",
    "title": "Revisão Probabilidade I",
    "section": "4.1 Uniforme \\((a,b)\\)",
    "text": "4.1 Uniforme \\((a,b)\\)\n\n\n\nItem\nExpressão\n\n\n\n\nSuporte\n\\(a\\le x\\le b\\)\n\n\n\\(f(x)\\)\n\\(1/(b-a)\\)\n\n\n\\(F(x)\\)\n\\((x-a)/(b-a)\\) para \\(a\\le x\\le b\\)\n\n\n\\(E[X]\\)\n\\((a+b)/2\\)\n\n\n\\(Var(X)\\)\n\\((b-a)^2/12\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMostrar demonstração\n\n\n\n\n\nConsidere \\(X \\sim \\text{Uniforme}(a,b)\\), com \\(a &lt; b\\).\nA função densidade é: \\[\nf(x) =\n\\begin{cases}\n\\dfrac{1}{b-a}, & a \\le x \\le b,\\\\[6pt]\n0, & \\text{caso contrário.}\n\\end{cases}\n\\]\nNosso objetivo é demonstrar que \\[\nE(X) = \\frac{a+b}{2},\n\\qquad\n\\mathrm{Var}(X) = \\frac{(b-a)^2}{12},\n\\] usando apenas as definições: \\[\nE(X)=\\int_{-\\infty}^{\\infty} x f(x)\\,dx,\n\\qquad\nE(X^2)=\\int_{-\\infty}^{\\infty} x^2 f(x)\\,dx,\n\\qquad\n\\mathrm{Var}(X)=E(X^2)-[E(X)]^2.\n\\]\n\n\n4.2 1. Cálculo de \\(E(X)\\)\nPela definição: \\[\nE(X) = \\int_{-\\infty}^{\\infty} x f(x)\\,dx.\n\\]\nComo \\(f(x)=0\\) fora do intervalo \\([a,b]\\), temos: \\[\nE(X) = \\int_a^b x \\cdot \\frac{1}{b-a}\\,dx\n     = \\frac{1}{b-a} \\int_a^b x\\,dx.\n\\]\nCalculamos a integral: \\[\n\\int_a^b x\\,dx = \\left[\\frac{x^2}{2}\\right]_a^b\n= \\frac{b^2}{2} - \\frac{a^2}{2}\n= \\frac{b^2 - a^2}{2}.\n\\]\nLogo: \\[\nE(X) = \\frac{1}{b-a} \\cdot \\frac{b^2 - a^2}{2}.\n\\]\nFatoramos \\(b^2 - a^2\\): \\[\nb^2 - a^2 = (b-a)(b+a).\n\\]\nSubstituindo: \\[\nE(X) = \\frac{1}{b-a} \\cdot \\frac{(b-a)(b+a)}{2}\n     = \\frac{b+a}{2}.\n\\]\nPortanto: \\[\nE(X) = \\frac{a+b}{2}.\n\\]\n\n\n\n4.3 2. Cálculo de \\(E(X^2)\\)\nPela definição: \\[\nE(X^2) = \\int_{-\\infty}^{\\infty} x^2 f(x)\\,dx\n       = \\int_a^b x^2 \\cdot \\frac{1}{b-a}\\,dx\n       = \\frac{1}{b-a}\\int_a^b x^2\\,dx.\n\\]\nCalculamos a integral: \\[\n\\int_a^b x^2\\,dx = \\left[\\frac{x^3}{3}\\right]_a^b\n= \\frac{b^3}{3} - \\frac{a^3}{3}\n= \\frac{b^3 - a^3}{3}.\n\\]\nEntão: \\[\nE(X^2) = \\frac{1}{b-a} \\cdot \\frac{b^3 - a^3}{3}\n       = \\frac{b^3 - a^3}{3(b-a)}.\n\\]\nAgora usamos a fatoração: \\[\nb^3 - a^3 = (b-a)(b^2 + ab + a^2).\n\\]\nSubstituindo: \\[\nE(X^2) = \\frac{(b-a)(b^2 + ab + a^2)}{3(b-a)}\n       = \\frac{b^2 + ab + a^2}{3}.\n\\]\nPortanto: \\[\nE(X^2) = \\frac{a^2 + ab + b^2}{3}.\n\\]\n\n\n\n4.4 3. Cálculo de \\(\\mathrm{Var}(X)\\)\nUsamos: \\[\n\\mathrm{Var}(X) = E(X^2) - [E(X)]^2.\n\\]\nJá obtivemos:\n\n\\(E(X) = \\dfrac{a+b}{2}\\)\n\n\\(E(X^2) = \\dfrac{a^2 + ab + b^2}{3}\\)\n\nEntão: \\[\n\\mathrm{Var}(X)\n= \\frac{a^2 + ab + b^2}{3} - \\left(\\frac{a+b}{2}\\right)^2.\n\\]\nPrimeiro, calculemos o quadrado: \\[\n\\left(\\frac{a+b}{2}\\right)^2\n= \\frac{(a+b)^2}{4}\n= \\frac{a^2 + 2ab + b^2}{4}.\n\\]\nAgora escrevemos a variância com denominador comum.\nO denominador comum entre 3 e 4 é 12:\n\nPrimeiro termo: \\[\n\\frac{a^2 + ab + b^2}{3}\n= \\frac{4(a^2 + ab + b^2)}{12}.\n\\]\nSegundo termo: \\[\n\\frac{a^2 + 2ab + b^2}{4}\n= \\frac{3(a^2 + 2ab + b^2)}{12}.\n\\]\n\nEntão: \\[\n\\mathrm{Var}(X)\n= \\frac{4(a^2 + ab + b^2)}{12} - \\frac{3(a^2 + 2ab + b^2)}{12}.\n\\]\nSubtraindo os numeradores: \\[\n4(a^2 + ab + b^2) - 3(a^2 + 2ab + b^2)\n= 4a^2 + 4ab + 4b^2 - 3a^2 - 6ab - 3b^2\n= (4a^2 - 3a^2) + (4ab - 6ab) + (4b^2 - 3b^2)\n= a^2 - 2ab + b^2.\n\\]\nLogo: \\[\n\\mathrm{Var}(X)\n= \\frac{a^2 - 2ab + b^2}{12}\n= \\frac{(b-a)^2}{12},\n\\] pois \\[\na^2 - 2ab + b^2 = (b-a)^2.\n\\]\n\n\n\n4.5 4. Resultado final\nPara \\(X \\sim \\text{Uniforme}(a,b)\\), demonstramos que: \\[\nE(X) = \\frac{a+b}{2},\n\\qquad\n\\mathrm{Var}(X) = \\frac{(b-a)^2}{12}.\n\\]\n\n\n\n\nExemplo: O tempo de resposta de um servidor web varia uniformemente entre 50 ms e 90 ms, ou seja \\(T \\sim U(50, 90)\\).\n\nCalcule \\(P(60 &lt; T &lt; 80)\\).\n\nCalcule \\(E[T]\\) e \\(Var(T)\\).\n\nInterprete o valor esperado no contexto.\n\nSolução:\n\n\\[\nP(60&lt;T&lt;80)=\\frac{80-60}{90-50}=\\frac{20}{40}=0{,}5\n\\]\n\\[\nE[T]=\\frac{50+90}{2}=70\n\\]\n\n\\[\nVar(T)=\\frac{(90-50)^2}{12}=\\frac{1600}{12}\\approx 133{,}33\n\\]\n\nO tempo médio de resposta é 70 ms."
  },
  {
    "objectID": "exercicios/lista02.html#exponencial-lambda-parametrização-por-taxa",
    "href": "exercicios/lista02.html#exponencial-lambda-parametrização-por-taxa",
    "title": "Revisão Probabilidade I",
    "section": "4.6 Exponencial \\((\\lambda)\\) (parametrização por taxa)",
    "text": "4.6 Exponencial \\((\\lambda)\\) (parametrização por taxa)\n\n\n\nItem\nExpressão\n\n\n\n\nSuporte\n\\(x\\ge 0\\)\n\n\n\\(f(x)\\)\n\\(\\lambda e^{-\\lambda x}\\)\n\n\n\\(F(x)\\)\n\\(1-e^{-\\lambda x}\\)\n\n\n\\(E[X]\\)\n\\(1/\\lambda\\)\n\n\n\\(Var(X)\\)\n\\(1/\\lambda^2\\)\n\n\nPropriedade\nSem memória: \\(P(X&gt;s+t\\mid X&gt;t)=P(X&gt;s)\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMostrar demonstração\n\n\n\n\n\nConsidere \\(X \\sim \\text{Exponencial}(\\lambda)\\), com \\(\\lambda &gt; 0\\).\nA função densidade é \\[\nf(x) =\n\\begin{cases}\n\\lambda e^{-\\lambda x}, & x \\ge 0, \\\\[4pt]\n0, & x &lt; 0.\n\\end{cases}\n\\]\nNosso objetivo é demonstrar: \\[\nE(X) = \\frac{1}{\\lambda},\n\\qquad\n\\mathrm{Var}(X) = \\frac{1}{\\lambda^2},\n\\] usando apenas as definições: \\[\nE(X) = \\int_0^\\infty x\\,\\lambda e^{-\\lambda x}\\,dx,\n\\qquad\nE(X^2) = \\int_0^\\infty x^2\\,\\lambda e^{-\\lambda x}\\,dx.\n\\]\n\n\n4.7 1. Cálculo de \\(E(X)\\)\nPela definição: \\[\nE(X) = \\int_0^\\infty x\\lambda e^{-\\lambda x}\\,dx.\n\\]\nPara resolver a integral, usamos integração por partes.\nEscolhemos: - (u = x) → (du = dx) - (dv = e^{-x}dx) → (v = -e^{-x})\nAplicando integração por partes: \\[\n\\int_0^\\infty x\\lambda e^{-\\lambda x}\\,dx\n= \\left[-x e^{-\\lambda x}\\right]_{0}^{\\infty}\n+ \\int_0^\\infty e^{-\\lambda x}\\,dx.\n\\]\nAgora avaliamos cada termo:\n\nTermo de fronteira:\n\nQuando (x): (x e^{-x} ) (exponencial domina linear).\nQuando (x=0): (0 e^{0} = 0).\n\nLogo: \\[\n\\left[-x e^{-\\lambda x}\\right]_{0}^{\\infty} = 0.\n\\]\nIntegral restante: \\[\n\\int_0^\\infty e^{-\\lambda x}\\,dx\n= \\left[-\\frac{1}{\\lambda}e^{-\\lambda x}\\right]_{0}^{\\infty}\n= \\frac{1}{\\lambda}.\n\\]\n\nPortanto: \\[\nE(X)=\\frac{1}{\\lambda}.\n\\]\n\n\n\n4.8 2. Cálculo de \\(E(X^2)\\)\nPela definição: \\[\nE(X^2)=\\int_0^\\infty x^2 \\lambda e^{-\\lambda x}\\,dx.\n\\]\nVamos aplicar integração por partes duas vezes.\n\n\n4.8.1 Primeira integração por partes\nEscolha: - (u = x^2) → (du = 2x,dx) - (dv = e^{-x}dx) → (v = -e^{-x})\nEntão: \\[\nE(X^2)\n= \\left[-x^2 e^{-\\lambda x}\\right]_0^\\infty\n+ \\int_0^\\infty 2x e^{-\\lambda x} \\, dx.\n\\]\nO termo de fronteira é zero, pelo mesmo argumento anterior.\nAssim: \\[\nE(X^2)= 2\\int_0^\\infty x e^{-\\lambda x}\\,dx.\n\\]\n\n\n\n4.8.2 Segunda integração por partes\nAgora resolvemos: \\[\n\\int_0^\\infty x e^{-\\lambda x}\\,dx.\n\\]\nEscolha: - (u = x) → (du = dx) - (dv = e^{-x}dx) → (v = -e^{-x})\nEntão: \\[\n\\int_0^\\infty x e^{-\\lambda x}\\,dx\n= \\left[-\\frac{x}{\\lambda} e^{-\\lambda x}\\right]_0^\\infty\n+ \\int_0^\\infty \\frac{1}{\\lambda} e^{-\\lambda x}\\,dx.\n\\]\nO termo de fronteira novamente é zero.\nResta: \\[\n\\int_0^\\infty x e^{-\\lambda x}\\,dx\n= \\frac{1}{\\lambda} \\int_0^\\infty e^{-\\lambda x}\\,dx\n= \\frac{1}{\\lambda}\\cdot\\frac{1}{\\lambda}\n= \\frac{1}{\\lambda^2}.\n\\]\n\n\n\n4.8.3 Conclusão do cálculo de \\(E(X^2)\\)\nVoltando ao ponto onde paramos: \\[\nE(X^2)= 2\\left(\\frac{1}{\\lambda^2}\\right)\n= \\frac{2}{\\lambda^2}.\n\\]\n\n\n\n\n4.9 3. Cálculo da variância\nUsamos: \\[\n\\mathrm{Var}(X)=E(X^2)-[E(X)]^2.\n\\]\nSubstituindo:\n\n\\(E(X^2)=\\frac{2}{\\lambda^2}\\)\n\\(E(X)=\\frac{1}{\\lambda}\\)\n\nEntão: \\[\n\\mathrm{Var}(X)\n= \\frac{2}{\\lambda^2} - \\left(\\frac{1}{\\lambda}\\right)^2\n= \\frac{2}{\\lambda^2} - \\frac{1}{\\lambda^2}\n= \\frac{1}{\\lambda^2}.\n\\]\n\n\n\n4.10 4. Resultado final\nPara \\(X \\sim \\text{Exponencial}(\\lambda)\\):\n\\[\nE(X)=\\frac{1}{\\lambda},\n\\qquad\n\\mathrm{Var}(X)=\\frac{1}{\\lambda^2}.\n\\]\n\n\n\n\nExemplo: O tempo entre chegadas ao caixa segue \\(X \\sim Exp(0{,}2)\\).\n\nCalcule \\(P(X&gt;8)\\).\n\nDetermine a mediana.\n\nInterprete a propriedade “sem memória”.\n\nSolução:\n\n\\[\nP(X&gt;8)=e^{-0{,}2\\cdot 8}=e^{-1{,}6}\\approx 0{,}2019\n\\]\n\\[\nm=\\frac{\\ln 2}{0{,}2}=5\\ln 2 \\approx 3{,}47\n\\]\nO tempo adicional não depende do tempo já passado."
  },
  {
    "objectID": "exercicios/lista02.html#normal-musigma2",
    "href": "exercicios/lista02.html#normal-musigma2",
    "title": "Revisão Probabilidade I",
    "section": "4.11 Normal \\((\\mu,\\sigma^2)\\)",
    "text": "4.11 Normal \\((\\mu,\\sigma^2)\\)\n\n\n\n\n\n\n\nItem\nExpressão\n\n\n\n\nSuporte\n\\(x\\in\\mathbb{R}\\)\n\n\n\\(f(x)\\)\n\\(\\dfrac{1}{\\sigma\\sqrt{2\\pi}}\\,\\exp\\!\\left(-\\dfrac{(x-\\mu)^2}{2\\sigma^2}\\right)\\)\n\n\n\\(F(x)\\)\n\\(\\Phi\\!\\left(\\dfrac{x-\\mu}{\\sigma}\\right)\\) (não possui forma fechada)\n\n\n\\(E[X]\\)\n\\(\\mu\\)\n\n\n\\(Var(X)\\)\n\\(\\sigma^2\\)\n\n\n\nUso de Tabelas: padronize \\(Z=(X-\\mu)/\\sigma\\) e leia \\(P(Z\\le z)\\) na tabela da Normal padrão \\(\\Phi(z)\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMostrar demonstração\n\n\n\n\n\nVamos considerar uma variável aleatória normal geral \\[\nX \\sim N(\\mu,\\sigma^2), \\quad \\sigma &gt; 0.\n\\]\nA densidade é \\[\nf_X(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}}\n\\exp\\!\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right),\\quad x\\in\\mathbb{R}.\n\\]\nNosso objetivo é demonstrar, a partir das definições, que \\[\nE(X)=\\mu,\n\\qquad\n\\mathrm{Var}(X)=\\sigma^2.\n\\]\nA estratégia será:\n\nCalcular \\(E(Z)\\) e \\(\\mathrm{Var}(Z)\\) para a normal padrão \\(Z\\sim N(0,1)\\).\nUsar a relação \\(X = \\mu + \\sigma Z\\).\n\n\n\n4.12 1. Normal padrão \\(Z \\sim N(0,1)\\)\nPara a normal padrão, a densidade é \\[\nf_Z(z)=\\frac{1}{\\sqrt{2\\pi}}e^{-z^2/2},\\quad z\\in\\mathbb{R}.\n\\]\nQueremos mostrar que: \\[\nE(Z)=0, \\qquad E(Z^2)=1 \\quad\\Rightarrow\\quad \\mathrm{Var}(Z)=1.\n\\]\n\n\n4.12.1 1.1 Cálculo de \\(E(Z)\\)\nPela definição: \\[\nE(Z)=\\int_{-\\infty}^{\\infty} z f_Z(z)\\,dz\n= \\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{\\infty} z e^{-z^2/2}\\,dz.\n\\]\nObserve que:\n\nA função \\(e^{-z^2/2}\\) é par (simétrica): \\(e^{-(-z)^2/2}=e^{-z^2/2}\\).\nA função \\(z\\) é ímpar: \\((-z) = -z\\).\nLogo, o produto \\(z e^{-z^2/2}\\) é ímpar.\n\nA integral de uma função ímpar em intervalo simétrico é zero: \\[\n\\int_{-\\infty}^{\\infty} z e^{-z^2/2}\\,dz = 0.\n\\]\nPortanto: \\[\nE(Z)=0.\n\\]\n\n\n\n4.12.2 1.2 Cálculo de \\(E(Z^2)\\)\nPela definição: \\[\nE(Z^2)=\\int_{-\\infty}^{\\infty} z^2 f_Z(z)\\,dz\n= \\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{\\infty} z^2 e^{-z^2/2}\\,dz.\n\\]\nPara calcular \\[\n\\int_{-\\infty}^{\\infty} z^2 e^{-z^2/2}\\,dz,\n\\] vamos usar um truque padrão com um parâmetro auxiliar.\nConsidere, para \\(a&gt;0\\): \\[\nI(a) = \\int_{-\\infty}^{\\infty} e^{-a z^2/2}\\,dz.\n\\]\nEste é um integral gaussiano. Sabe-se (ou demonstra-se via coordenadas polares) que \\[\nI(a) = \\sqrt{\\frac{2\\pi}{a}}.\n\\]\nAgora vamos derivar \\(I(a)\\) em relação a \\(a\\) para obter uma integral com \\(z^2\\).\n\n\n4.12.2.1 Derivando \\(I(a)\\)\nPor um lado, derivando “dentro” da integral (legítimo sob condições usuais):\n\\[\nI'(a) = \\frac{d}{da}\\int_{-\\infty}^{\\infty} e^{-a z^2/2}\\,dz\n      = \\int_{-\\infty}^{\\infty} \\frac{\\partial}{\\partial a}\n        \\big(e^{-a z^2/2}\\big)\\,dz.\n\\]\nMas \\[\n\\frac{\\partial}{\\partial a} e^{-a z^2/2}\n= -\\frac{z^2}{2} e^{-a z^2/2}.\n\\]\nLogo: \\[\nI'(a) = \\int_{-\\infty}^{\\infty} -\\frac{z^2}{2} e^{-a z^2/2}\\,dz\n      = -\\frac{1}{2}\\int_{-\\infty}^{\\infty} z^2 e^{-a z^2/2}\\,dz.\n\\]\nPor outro lado, derivando a expressão fechada \\[\nI(a) = \\sqrt{\\frac{2\\pi}{a}}\n= (2\\pi)^{1/2} a^{-1/2},\n\\]\ntemos \\[\nI'(a) = (2\\pi)^{1/2}\\left(-\\frac{1}{2}\\right)a^{-3/2}\n      = -\\frac{\\sqrt{2\\pi}}{2}\\,a^{-3/2}.\n\\]\nIgualando as duas expressões para \\(I'(a)\\): \\[\n-\\frac{1}{2}\\int_{-\\infty}^{\\infty} z^2 e^{-a z^2/2}\\,dz\n= -\\frac{\\sqrt{2\\pi}}{2}\\,a^{-3/2}.\n\\]\nMultiplicando por \\(-2\\) ambos os lados: \\[\n\\int_{-\\infty}^{\\infty} z^2 e^{-a z^2/2}\\,dz\n= \\sqrt{2\\pi}\\,a^{-3/2}.\n\\]\n\n\n\n4.12.2.2 Aplicando o resultado em \\(a=1\\)\nQueremos o caso com \\(a=1\\), isto é, \\[\n\\int_{-\\infty}^{\\infty} z^2 e^{-z^2/2}\\,dz.\n\\]\nPela fórmula: \\[\n\\int_{-\\infty}^{\\infty} z^2 e^{-z^2/2}\\,dz\n= \\sqrt{2\\pi}\\cdot 1^{-3/2}\n= \\sqrt{2\\pi}.\n\\]\nAgora voltamos para \\(E(Z^2)\\): \\[\nE(Z^2)\n= \\frac{1}{\\sqrt{2\\pi}}\n\\int_{-\\infty}^{\\infty} z^2 e^{-z^2/2}\\,dz\n= \\frac{1}{\\sqrt{2\\pi}}\\cdot \\sqrt{2\\pi} = 1.\n\\]\nLogo: \\[\nE(Z^2)=1.\n\\]\n\n\n\n\n4.12.3 1.3 Variância da normal padrão\nA variância é \\[\n\\mathrm{Var}(Z)=E(Z^2)-[E(Z)]^2=1-0^2=1.\n\\]\nConcluímos: \\[\nE(Z)=0,\\qquad \\mathrm{Var}(Z)=1\n\\quad\\text{para } Z\\sim N(0,1).\n\\]\n\n\n\n\n4.13 2. Normal geral \\(X \\sim N(\\mu,\\sigma^2)\\)\nUma variável normal geral pode ser escrita como \\[\nX = \\mu + \\sigma Z,\n\\] onde \\(Z \\sim N(0,1)\\).\nVamos usar essa relação para encontrar \\(E(X)\\) e \\(\\mathrm{Var}(X)\\).\n\n\n4.13.1 2.1 Cálculo de \\(E(X)\\)\nUsando a linearidade da esperança: \\[\nE(X) = E(\\mu + \\sigma Z)\n     = E(\\mu) + E(\\sigma Z)\n     = \\mu + \\sigma E(Z).\n\\]\nComo já mostramos que \\(E(Z)=0\\), obtemos: \\[\nE(X) = \\mu + \\sigma \\cdot 0 = \\mu.\n\\]\n\n\n\n4.13.2 2.2 Cálculo de \\(\\mathrm{Var}(X)\\)\nUsamos a propriedade de variância para transformações lineares: \\[\n\\mathrm{Var}(a + bZ) = b^2\\,\\mathrm{Var}(Z).\n\\]\nAqui, \\(a=\\mu\\) e \\(b=\\sigma\\), então: \\[\n\\mathrm{Var}(X) = \\mathrm{Var}(\\mu + \\sigma Z)\n                = \\sigma^2\\,\\mathrm{Var}(Z).\n\\]\nSabemos que \\(\\mathrm{Var}(Z)=1\\), logo: \\[\n\\mathrm{Var}(X) = \\sigma^2 \\cdot 1 = \\sigma^2.\n\\]\n\n\n\n\n4.14 3. Resultado final\nPara uma variável aleatória normal geral \\(X \\sim N(\\mu,\\sigma^2)\\), demonstramos que:\n\\[\nE(X) = \\mu,\n\\qquad\n\\mathrm{Var}(X) = \\sigma^2.\n\\]\n\n\n\n\nExemplo: Pesos de pacotes seguem \\(W \\sim N(25, 1{,}5^2)\\).\n\nCalcule \\(P(24 &lt; W &lt; 27)\\).\n\nDetermine o percentil 95%.\n\nInterprete o percentil no controle de qualidade.\n\nSolução:\n\n\\[\nP(24&lt;W&lt;27)=P(-0{,}67&lt;Z&lt;1{,}33)=\\Phi(1{,}33)-\\Phi(-0{,}67)\n\\]\n\n\\[\n\\approx 0{,}9082 - 0{,}2514 = 0{,}6568\n\\]\n\n\\[\nx_{0{,}95}=25 + 1{,}645\\cdot 1{,}5 = 27{,}4675\n\\]\n95% dos pacotes pesam até 27,47 kg."
  },
  {
    "objectID": "exercicios/lista02.html#hipergeométrica-approx-binomial",
    "href": "exercicios/lista02.html#hipergeométrica-approx-binomial",
    "title": "Revisão Probabilidade I",
    "section": "5.1 Hipergeométrica \\(\\approx\\) Binomial",
    "text": "5.1 Hipergeométrica \\(\\approx\\) Binomial\nCondição: população grande vs. amostra pequena (fração amostral \\(n/N\\) pequena).\nUse \\(X\\sim Bin(n, p=K/N)\\) como aproximação."
  },
  {
    "objectID": "exercicios/lista02.html#binomial-approx-poisson",
    "href": "exercicios/lista02.html#binomial-approx-poisson",
    "title": "Revisão Probabilidade I",
    "section": "5.2 Binomial \\(\\approx\\) Poisson",
    "text": "5.2 Binomial \\(\\approx\\) Poisson\nCondição: \\(n\\) grande, \\(p\\) pequeno, \\(\\lambda=np\\) moderado.\nAproximação: \\(P_{Bin}(X=k)\\approx e^{-\\lambda}\\lambda^k/k!\\)."
  },
  {
    "objectID": "exercicios/lista02.html#binomial-approx-normal",
    "href": "exercicios/lista02.html#binomial-approx-normal",
    "title": "Revisão Probabilidade I",
    "section": "5.3 Binomial \\(\\approx\\) Normal",
    "text": "5.3 Binomial \\(\\approx\\) Normal\nCondição: \\(np(1-p)\\gtrsim 10\\).\nAproximação: \\(X\\approx N(\\mu=np,\\sigma^2=np(1-p))\\) com correção de continuidade."
  },
  {
    "objectID": "exercicios/lista02.html#poisson-approx-normal",
    "href": "exercicios/lista02.html#poisson-approx-normal",
    "title": "Revisão Probabilidade I",
    "section": "5.4 Poisson \\(\\approx\\) Normal",
    "text": "5.4 Poisson \\(\\approx\\) Normal\nCondição: \\(\\lambda\\gtrsim 10\\).\nAproximação: \\(X\\approx N(\\mu=\\lambda,\\sigma^2=\\lambda)\\) com correção de continuidade."
  },
  {
    "objectID": "exercicios/lista02.html#bernoulli-p-1",
    "href": "exercicios/lista02.html#bernoulli-p-1",
    "title": "Revisão Teórica + Exercícios — Probabilidade",
    "section": "9.1 Bernoulli \\((p)\\)",
    "text": "9.1 Bernoulli \\((p)\\)\nInterpretação: 1 sucesso/fracasso em único ensaio.\n\n\n\n\n\n\n\nItem\nExpressão\n\n\n\n\nSuporte\n\\(x\\in\\{0,1\\}\\)\n\n\nParâmetro\n\\(0&lt;p&lt;1\\)\n\n\nFunção de probabilidade \\(p(x)\\)\n\\(p^x(1-p)^{1-x}\\)\n\n\nDistribuição \\(F(x)\\)\n\\(0\\) se \\(x&lt;0\\); \\(1-p\\) se \\(0\\le x&lt;1\\); \\(1\\) se \\(x\\ge1\\)\n\n\nEsperança \\(E[X]\\)\n\\(p\\)\n\n\nVariância \\(Var(X)\\)\n\\(p(1-p)\\)\n\n\n\nExercícios 1. (Teoria) Mostre que \\(E[X^k]=p\\) para todo \\(k\\ge1\\).\n2. (Prática) Uma campanha tem taxa de clique \\(p=0{,}08\\). Em 1 impressão, qual a probabilidade de clique? E de não clique?\n3. (Prática) Simule 100 ensaios Bernoulli com \\(p=0{,}3\\) (faça no software de sua escolha) e estime \\(p\\) pela média amostral."
  },
  {
    "objectID": "exercicios/lista02.html#binomial-np-1",
    "href": "exercicios/lista02.html#binomial-np-1",
    "title": "Revisão Teórica + Exercícios — Probabilidade",
    "section": "9.2 Binomial \\((n,p)\\)",
    "text": "9.2 Binomial \\((n,p)\\)\nInterpretação: número de sucessos em \\(n\\) ensaios independentes, prob. \\(p\\).\n\n\n\nItem\nExpressão\n\n\n\n\nSuporte\n\\(k=0,1,\\dots,n\\)\n\n\nParâmetros\n\\(n\\in\\mathbb{N}\\), \\(0&lt;p&lt;1\\)\n\n\n\\(p(k)\\)\n\\(\\binom{n}{k}p^k(1-p)^{n-k}\\)\n\n\n\\(F(k)\\)\n\\(\\sum_{j=0}^{k}\\binom{n}{j}p^j(1-p)^{n-j}\\)\n\n\n\\(E[X]\\)\n\\(np\\)\n\n\n\\(Var(X)\\)\n\\(np(1-p)\\)\n\n\n\nExercícios 1. (Teoria) Use a soma de \\(n\\) Bernoullis i.i.d. para deduzir \\(E[X]\\) e \\(Var(X)\\).\n2. (Prática) \\(n=12\\), \\(p=0{,}25\\): calcule \\(P(X\\ge 4)\\) e \\(P(2\\le X\\le5)\\).\n3. (Prática) Em um lote, a taxa de defeitos é \\(2\\%\\). Em 200 itens, qual a probabilidade de no máximo 3 defeituosos?"
  },
  {
    "objectID": "exercicios/lista02.html#geométrica-p-1",
    "href": "exercicios/lista02.html#geométrica-p-1",
    "title": "Revisão Teórica + Exercícios — Probabilidade",
    "section": "9.3 Geométrica \\((p)\\)",
    "text": "9.3 Geométrica \\((p)\\)\nConvenção usada: \\(X\\) = número de ensaios até o 1º sucesso (apoio \\(1,2,\\dots\\)).\n\n\n\nItem\nExpressão\n\n\n\n\n\\(p(k)\\)\n\\(p(1-p)^{k-1}\\)\n\n\n\\(F(k)\\)\n\\(1-(1-p)^k\\)\n\n\n\\(E[X]\\)\n\\(1/p\\)\n\n\n\\(Var(X)\\)\n\\((1-p)/p^2\\)\n\n\nPropriedade\nSem memória: \\(P(X&gt;s+t\\mid X&gt;t)=P(X&gt;s)\\)\n\n\n\nExercícios 1. (Teoria) Prove a falta de memória usando séries geométricas.\n2. (Prática) Para \\(p=0{,}2\\), calcule \\(P(X&gt;5)\\) e a mediana.\n3. (Prática) Interprete o que significa \\(E[X]=1/p\\) em campanhas de tentativa e erro."
  },
  {
    "objectID": "exercicios/lista02.html#pascal-negativa-rp-1",
    "href": "exercicios/lista02.html#pascal-negativa-rp-1",
    "title": "Revisão Teórica + Exercícios — Probabilidade",
    "section": "9.4 Pascal / Negativa \\((r,p)\\)",
    "text": "9.4 Pascal / Negativa \\((r,p)\\)\nConvenção usada: \\(Y\\) = número de falhas antes do \\(r\\)-ésimo sucesso (apoio \\(0,1,\\dots\\)).\n\n\n\nItem\nExpressão\n\n\n\n\n\\(p(k)\\)\n\\(\\binom{k+r-1}{k}(1-p)^k p^r\\)\n\n\n\\(F(k)\\)\n\\(\\sum_{j=0}^{k}\\binom{j+r-1}{j}(1-p)^j p^r\\)\n\n\n\\(E[Y]\\)\n\\(r(1-p)/p\\)\n\n\n\\(Var(Y)\\)\n\\(r(1-p)/p^2\\)\n\n\n\nExercícios 1. (Teoria) Deduzir \\(E[Y]\\) como soma de \\(r\\) geométricas independentes.\n2. (Prática) \\(r=3\\), \\(p=0{,}4\\): calcule \\(P(Y\\le 2)\\) e \\(E[Y]\\).\n3. (Prática) Explique quando preferir a convenção “número de ensaios até \\(r\\) sucessos”."
  },
  {
    "objectID": "exercicios/lista02.html#hipergeométrica-nkn-1",
    "href": "exercicios/lista02.html#hipergeométrica-nkn-1",
    "title": "Revisão Teórica + Exercícios — Probabilidade",
    "section": "9.5 Hipergeométrica \\((N,K,n)\\)",
    "text": "9.5 Hipergeométrica \\((N,K,n)\\)\nAmostragem sem reposição. \\(N\\) total, \\(K\\) sucessos na população, amostra \\(n\\).\n\n\n\n\n\n\n\nItem\nExpressão\n\n\n\n\nSuporte\n\\(k=\\max(0,n-(N-K)),\\dots,\\min(n,K)\\)\n\n\n\\(p(k)\\)\n\\(\\dfrac{\\binom{K}{k}\\binom{N-K}{n-k}}{\\binom{N}{n}}\\)\n\n\n\\(E[X]\\)\n\\(n\\,\\dfrac{K}{N}\\)\n\n\n\\(Var(X)\\)\n\\(n\\,\\dfrac{K}{N}\\!\\left(1-\\dfrac{K}{N}\\right)\\!\\dfrac{N-n}{N-1}\\)\n\n\n\nExercícios 1. (Teoria) Mostre que \\(Var(X)\\) tende a \\(np(1-p)\\) quando \\(N\\to\\infty\\) com \\(p=K/N\\) fixo.\n2. (Prática) \\(N=500\\), \\(K=60\\), \\(n=20\\): calcule \\(P(X=3)\\) e \\(E[X]\\).\n3. (Prática) Compare com a Binomial \\((n, p=K/N)\\)."
  },
  {
    "objectID": "exercicios/lista02.html#poisson-lambda-1",
    "href": "exercicios/lista02.html#poisson-lambda-1",
    "title": "Revisão Teórica + Exercícios — Probabilidade",
    "section": "9.6 Poisson \\((\\lambda)\\)",
    "text": "9.6 Poisson \\((\\lambda)\\)\nContagem de eventos raros em intervalo fixo.\n\n\n\nItem\nExpressão\n\n\n\n\nSuporte\n\\(k=0,1,2,\\dots\\)\n\n\n\\(p(k)\\)\n\\(e^{-\\lambda}\\lambda^k/k!\\)\n\n\n\\(F(k)\\)\n\\(\\sum_{j=0}^{k}e^{-\\lambda}\\lambda^j/j!\\)\n\n\n\\(E[X]\\)\n\\(\\lambda\\)\n\n\n\\(Var(X)\\)\n\\(\\lambda\\)\n\n\n\nExercícios 1. (Teoria) Mostre que a soma de Poissons independentes é Poisson com parâmetro somado.\n2. (Prática) \\(\\lambda=4\\)/h: probabilidade de ao menos 1 evento em 15 min; probabilidade de \\(k=3\\) em 1 h.\n3. (Prática) Estime \\(\\lambda\\) por máxima verossimilhança a partir de uma amostra observada (descreva o procedimento)."
  },
  {
    "objectID": "exercicios/lista02.html#uniforme-ab-1",
    "href": "exercicios/lista02.html#uniforme-ab-1",
    "title": "Revisão Teórica + Exercícios — Probabilidade",
    "section": "10.1 Uniforme \\((a,b)\\)",
    "text": "10.1 Uniforme \\((a,b)\\)\n\n\n\nItem\nExpressão\n\n\n\n\nSuporte\n\\(a\\le x\\le b\\)\n\n\n\\(f(x)\\)\n\\(1/(b-a)\\)\n\n\n\\(F(x)\\)\n\\((x-a)/(b-a)\\) para \\(a\\le x\\le b\\)\n\n\n\\(E[X]\\)\n\\((a+b)/2\\)\n\n\n\\(Var(X)\\)\n\\((b-a)^2/12\\)\n\n\n\nExercícios 1. (Teoria) Mostre que \\(Var(X)=E[X^2]-E[X]^2\\) produz \\((b-a)^2/12\\).\n2. (Prática) Gere um número aleatório uniforme em \\([2,8]\\) a partir de \\(U(0,1)\\) via transformação.\n3. (Prática) Para \\(a=10\\), \\(b=20\\), calcule \\(P(12&lt;X&lt;17)\\)."
  },
  {
    "objectID": "exercicios/lista02.html#exponencial-lambda-parametrização-por-taxa-1",
    "href": "exercicios/lista02.html#exponencial-lambda-parametrização-por-taxa-1",
    "title": "Revisão Teórica + Exercícios — Probabilidade",
    "section": "10.2 Exponencial \\((\\lambda)\\) (parametrização por taxa)",
    "text": "10.2 Exponencial \\((\\lambda)\\) (parametrização por taxa)\n\n\n\nItem\nExpressão\n\n\n\n\nSuporte\n\\(x\\ge 0\\)\n\n\n\\(f(x)\\)\n\\(\\lambda e^{-\\lambda x}\\)\n\n\n\\(F(x)\\)\n\\(1-e^{-\\lambda x}\\)\n\n\n\\(E[X]\\)\n\\(1/\\lambda\\)\n\n\n\\(Var(X)\\)\n\\(1/\\lambda^2\\)\n\n\nPropriedade\nSem memória: \\(P(X&gt;s+t\\mid X&gt;t)=P(X&gt;s)\\)\n\n\n\nExercícios 1. (Teoria) Deduzir a falta de memória a partir de \\(F(x)\\).\n2. (Prática) Com média 8 min (\\(\\lambda=1/8\\)), calcule \\(P(X&gt;12)\\) e o tempo mediano.\n3. (Prática) Um equipamento já operou 30 min; probabilidade de funcionar mais 10 min?"
  },
  {
    "objectID": "exercicios/lista02.html#normal-musigma2-1",
    "href": "exercicios/lista02.html#normal-musigma2-1",
    "title": "Revisão Teórica + Exercícios — Probabilidade",
    "section": "10.3 Normal \\((\\mu,\\sigma^2)\\)",
    "text": "10.3 Normal \\((\\mu,\\sigma^2)\\)\n\n\n\n\n\n\n\nItem\nExpressão\n\n\n\n\nSuporte\n\\(x\\in\\mathbb{R}\\)\n\n\n\\(f(x)\\)\n\\(\\dfrac{1}{\\sigma\\sqrt{2\\pi}}\\,\\exp\\!\\left(-\\dfrac{(x-\\mu)^2}{2\\sigma^2}\\right)\\)\n\n\n\\(F(x)\\)\n\\(\\Phi\\!\\left(\\dfrac{x-\\mu}{\\sigma}\\right)\\) (não possui forma fechada)\n\n\n\\(E[X]\\)\n\\(\\mu\\)\n\n\n\\(Var(X)\\)\n\\(\\sigma^2\\)\n\n\n\nUso de Tabelas: padronize \\(Z=(X-\\mu)/\\sigma\\) e leia \\(P(Z\\le z)\\) na tabela da Normal padrão \\(\\Phi(z)\\).\nExercícios 1. (Teoria) Mostre que \\(aX+b\\sim N(a\\mu+b, a^2\\sigma^2)\\).\n2. (Prática) \\(\\mu=70\\), \\(\\sigma=8\\): calcule \\(P(60&lt;X&lt;82)\\) e o percentil 97,5%.\n3. (Prática) Em produção, qual limite superior tal que \\(5\\%\\) das peças o excedam?"
  },
  {
    "objectID": "exercicios/lista02.html#hipergeométrica-approx-binomial-1",
    "href": "exercicios/lista02.html#hipergeométrica-approx-binomial-1",
    "title": "Revisão Teórica + Exercícios — Probabilidade",
    "section": "11.1 Hipergeométrica \\(\\approx\\) Binomial",
    "text": "11.1 Hipergeométrica \\(\\approx\\) Binomial\nCondição: população grande vs. amostra pequena (fração amostral \\(n/N\\) pequena).\nUse \\(X\\sim Bin(n, p=K/N)\\) como aproximação.\nExercício. Para \\(N=5000\\), \\(K=400\\), \\(n=20\\), compare \\(P(X=3)\\) exato vs. aproximado."
  },
  {
    "objectID": "exercicios/lista02.html#binomial-approx-poisson-1",
    "href": "exercicios/lista02.html#binomial-approx-poisson-1",
    "title": "Revisão Teórica + Exercícios — Probabilidade",
    "section": "11.2 Binomial \\(\\approx\\) Poisson",
    "text": "11.2 Binomial \\(\\approx\\) Poisson\nCondição: \\(n\\) grande, \\(p\\) pequeno, \\(\\lambda=np\\) moderado.\nAproximação: \\(P_{Bin}(X=k)\\approx e^{-\\lambda}\\lambda^k/k!\\).\nExercício. \\(n=200\\), \\(p=0{,}02\\): estime \\(P(X=0)\\) via Poisson e compare com valor exato."
  },
  {
    "objectID": "exercicios/lista02.html#binomial-approx-normal-1",
    "href": "exercicios/lista02.html#binomial-approx-normal-1",
    "title": "Revisão Teórica + Exercícios — Probabilidade",
    "section": "11.3 Binomial \\(\\approx\\) Normal",
    "text": "11.3 Binomial \\(\\approx\\) Normal\nCondição: \\(np(1-p)\\gtrsim 10\\).\nAproximação: \\(X\\approx N(\\mu=np,\\sigma^2=np(1-p))\\) com correção de continuidade.\nExercício. \\(n=150\\), \\(p=0{,}3\\): estime \\(P(40\\le X\\le 55)\\) com correção e compare."
  },
  {
    "objectID": "exercicios/lista02.html#poisson-approx-normal-1",
    "href": "exercicios/lista02.html#poisson-approx-normal-1",
    "title": "Revisão Teórica + Exercícios — Probabilidade",
    "section": "11.4 Poisson \\(\\approx\\) Normal",
    "text": "11.4 Poisson \\(\\approx\\) Normal\nCondição: \\(\\lambda\\gtrsim 10\\).\nAproximação: \\(X\\approx N(\\mu=\\lambda,\\sigma^2=\\lambda)\\) com correção de continuidade.\nExercício. \\(\\lambda=36\\): estime \\(P(30\\le X\\le 45)\\) e compare com Poisson exato (descreva o passo a passo)."
  },
  {
    "objectID": "exercicios/lista02.html#versão-mind-map-top-down-para-usar-em-slides-revisão-visual",
    "href": "exercicios/lista02.html#versão-mind-map-top-down-para-usar-em-slides-revisão-visual",
    "title": "Revisão Teórica + Exercícios — Probabilidade",
    "section": "6.1 2. Versão “mind map” (top-down) para usar em slides / revisão visual",
    "text": "6.1 2. Versão “mind map” (top-down) para usar em slides / revisão visual\nEsta versão organiza as distribuições de forma mais “hierárquica”, como um mapa mental.\nVocê pode colocar como outra seção do .qmd:\n## Mapa mental das distribuições\n\n```{mermaid}\nflowchart TD\n\n  VA[Variáveis Aleatórias]\n\n  subgraph Discretas\n    Bernoulli[\"Bernoulli(p)\"]\n    Binomial[\"Binomial(n,p)\"]\n    Geom[\"Geométrica(p)\"]\n    NegBin[\"Binomial Negativa(r,p)\"]\n    Hiper[\"Hipergeométrica(N,K,n)\"]\n    Poisson[\"Poisson(λ)\"]\n  end\n\n  subgraph Contínuas\n    Uniform[\"Uniforme(a,b)\"]\n    Exp[\"Exponencial(λ)\"]\n    Gamma[\"Gamma(α,λ)\"]\n    Chi2[\"Qui-quadrado(ν)\"]\n    Beta[\"Beta(α,β)\"]\n    Normal[\"Normal(μ,σ²)\"]\n    Weibull[\"Weibull(α,λ)\"]\n  end\n\n  VA --&gt; Discretas\n  VA --&gt; Contínuas\n\n  %% Construções discretas\n  Bernoulli --&gt;|\"soma de n ensaios\"| Binomial\n  Bernoulli --&gt;|\"ensaios até 1º sucesso\"| Geom\n  Geom --&gt;|\"soma de r geométricas\"| NegBin\n\n  Hiper --&gt;|\"N grande, n pequeno\"| Binomial\n  Binomial --&gt;|\"n grande, p pequeno (λ=np)\"| Poisson\n  Binomial --&gt;|\"np(1-p) grande (CLT)\"| Normal\n\n  Poisson --&gt;|\"λ grande\"| Normal\n\n  %% Processo de Poisson\n  Poisson --&gt;|\"tempos entre chegadas\"| Exp\n\n  %% Ligações contínuas\n  Exp --&gt;|\"soma de k expo\"| Gamma\n  Gamma --&gt;|\"α=1\"| Exp\n  Gamma --&gt;|\"α=ν/2, λ=1/2\"| Chi2\n  Gamma --&gt;|\"razão de duas Gammas\"| Beta\n\n  Uniform --&gt;|\"transformação inversa\"| Exp\n  Uniform --&gt;|\"soma de muitos (CLT)\"| Normal\n\n  Exp --&gt;|\"transformação de potência\"| Weibull\n\n  Normal -.-&gt;|\"soma de muitas v.a. iid\"| Normal\n```"
  },
  {
    "objectID": "exercicios/lista02.html#distribuições-contínuas-1",
    "href": "exercicios/lista02.html#distribuições-contínuas-1",
    "title": "Revisão Probabilidade I",
    "section": "6.1 Distribuições Contínuas",
    "text": "6.1 Distribuições Contínuas\n\n\n6.1.1 7. Uniforme Contínua \\(\\mathrm{U}(a,b)\\)\n\n\n\n\n\n\nMostrar demonstração\n\n\n\n\n\n\n6.1.1.1 Esperança\n\\[\nE(X)=\\frac{1}{b-a}\\int_a^b x\\,dx\n= \\frac{a+b}{2}.\n\\]\n\n\n6.1.1.2 Segundo momento\n\\[\nE(X^2)=\\frac{b^2+ab+a^2}{3}.\n\\]\n\n\n6.1.1.3 Variância\n\\[\n\\mathrm{Var}(X)=\\frac{(b-a)^2}{12}.\n\\]\n\n\n\n\n\n\n\n6.1.2 8. Exponencial (\\(\\lambda\\))\n\n\n\n\n\n\nMostrar demonstração\n\n\n\n\n\n\n6.1.2.1 Esperança\nUse integração por partes:\n\\[\nE(X)=\\frac{1}{\\lambda}.\n\\]\n\n\n6.1.2.2 Segundo momento\n\\[\nE(X^2)=\\frac{2}{\\lambda^2}.\n\\]\n\n\n6.1.2.3 Variância\n\\[\n\\mathrm{Var}(X)=\\frac{1}{\\lambda^2}.\n\\]\n\n\n\n\n\n\n\n6.1.3 9. Normal (N(,^2))\n\n\n\n\n\n\nMostrar demonstração\n\n\n\n\n\nPrimeiro para \\(Z\\sim N(0,1)\\):\n\nsimetria → \\(E(Z)=0\\)\ncálculo clássico → \\(E(Z^2)=1\\)\n\nAgora, se \\(X=\\mu+\\sigma Z\\):\n\\[\nE(X)=\\mu,\\qquad \\mathrm{Var}(X)=\\sigma^2.\n\\]"
  },
  {
    "objectID": "exercicios/lista02.html#pascal-binomial-negativa-rp",
    "href": "exercicios/lista02.html#pascal-binomial-negativa-rp",
    "title": "Revisão Probabilidade I",
    "section": "3.4 Pascal / Binomial Negativa \\((r,p)\\)",
    "text": "3.4 Pascal / Binomial Negativa \\((r,p)\\)\nConvenção usada: \\(Y\\) = número de falhas antes do \\(r\\)-ésimo sucesso (apoio \\(0,1,\\dots\\)).\n\n\n\nItem\nExpressão\n\n\n\n\n\\(p(k)\\)\n\\(\\binom{k+r-1}{k}(1-p)^k p^r\\)\n\n\n\\(F(k)\\)\n\\(\\sum_{j=0}^{k}\\binom{j+r-1}{j}(1-p)^j p^r\\)\n\n\n\\(E[Y]\\)\n\\(r(1-p)/p\\)\n\n\n\\(Var(Y)\\)\n\\(r(1-p)/p^2\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMostrar demonstração\n\n\n\n\n\nVamos usar a seguinte definição/convenção:\n\nEm cada tentativa, ocorre sucesso com probabilidade \\(p\\) e falha com probabilidade \\(1-p\\), independentemente.\n\\(X\\) é o número de falhas até o \\(r\\)-ésimo sucesso.\nDizemos então que \\(X \\sim \\text{Binomial Negativa}(r,p)\\), com \\(r \\in \\mathbb{N}\\).\n\nA função de probabilidade é \\[\nP(X = k)\n= \\binom{k + r - 1}{k} (1-p)^k p^r,\n\\quad k = 0,1,2,\\dots\n\\]\nNosso objetivo é demonstrar que \\[\nE(X) = \\frac{r(1-p)}{p}\n\\qquad \\text{e} \\qquad\n\\mathrm{Var}(X) = \\frac{r(1-p)}{p^2}.\n\\]\nEm vez de somar diretamente a série da f.p., vamos usar uma interpretação construída a partir da Geométrica, que já sabemos tratar.\n\n\n3.4.1 1. Ligação com a distribuição geométrica\nConsidere o processo de Bernoulli (tentativas independentes com probabilidade de sucesso \\(p\\)).\nEntre sucessos consecutivos, o número de falhas que ocorre é sempre do mesmo tipo:\n\nAntes do 1º sucesso, temos um certo número de falhas \\(Y_1\\).\nEntre o 1º e o 2º sucesso, temos um número de falhas \\(Y_2\\).\n…\nEntre o (r-1)º e o r-ésimo sucesso, temos um número de falhas \\(Y_r\\).\n\nEssas quantidades \\(Y_1, Y_2, \\dots, Y_r\\) são independentes e têm mesma distribuição.\nAlém disso, o número total de falhas até o \\(r\\)-ésimo sucesso é \\[\nX = Y_1 + Y_2 + \\cdots + Y_r.\n\\]\nVamos então:\n\nDeterminar a distribuição de cada \\(Y_i\\).\nCalcular \\(E(Y_i)\\) e \\(\\mathrm{Var}(Y_i)\\).\nSomar para obter \\(E(X)\\) e \\(\\mathrm{Var}(X)\\).\n\n\n\n\n3.4.2 2. Distribuição de \\(Y_i\\) (número de falhas entre dois sucessos)\nEntre dois sucessos consecutivos, o experimento funciona assim:\n\nObservamos uma sequência de falhas, todas com probabilidade \\((1-p)\\),\n\nseguida de um sucesso, com probabilidade \\(p\\).\n\nSe \\(Y_i = k\\), isso significa: falha, falha, …, falha (\\(k\\) vezes), depois um sucesso.\nLogo \\[\nP(Y_i = k) = (1-p)^k p, \\quad k = 0,1,2,\\dots\n\\]\nEssa é uma Geométrica com suporte em \\(\\{0,1,2,\\dots\\}\\), às vezes chamada de geométrica “deslocada” ou “número de falhas antes do sucesso”.\nSabemos (ou podemos derivar a partir da geométrica padrão em \\(\\{1,2,\\dots\\}\\)) que:\n\nSe \\(G\\) tem \\(P(G=k)=p(1-p)^{k-1}\\), \\(k=1,2,\\dots\\), então \\(E(G)=\\frac{1}{p}\\) e \\(\\mathrm{Var}(G)=\\frac{1-p}{p^2}\\).\nSe definimos \\(Y = G - 1\\), então \\(Y\\) tem suporte \\(\\{0,1,2,\\dots\\}\\), e \\[\nE(Y) = E(G-1) = E(G) - 1 = \\frac{1}{p} - 1 = \\frac{1-p}{p},\n\\] \\[\n\\mathrm{Var}(Y) = \\mathrm{Var}(G-1) = \\mathrm{Var}(G) = \\frac{1-p}{p^2}.\n\\]\n\nPortanto, cada \\(Y_i\\) tem:\n\\[\nE(Y_i)=\\frac{1-p}{p}, \\qquad\n\\mathrm{Var}(Y_i)=\\frac{1-p}{p^2}.\n\\]\nE os \\(Y_i\\) são i.i.d. (independentes e identicamente distribuídos).\n\n\n\n3.4.3 3. Expressão de \\(X\\) como soma de geométricas\nRelembrando: \\[\nX = Y_1 + Y_2 + \\cdots + Y_r,\n\\] com \\(Y_i\\) independentes e com a mesma distribuição.\nVamos usar:\n\nLinearidade da esperança: \\[\nE(X) = E(Y_1) + \\cdots + E(Y_r),\n\\]\nVariância da soma de independentes: \\[\n\\mathrm{Var}(X)\n= \\mathrm{Var}(Y_1) + \\cdots + \\mathrm{Var}(Y_r),\n\\] pois não há termos de covariância (independência \\(\\Rightarrow\\) covariância zero).\n\n\n\n\n3.4.4 4. Cálculo de \\(E(X)\\)\nPela linearidade da esperança: \\[\nE(X)\n= E(Y_1 + \\cdots + Y_r)\n= E(Y_1) + \\cdots + E(Y_r).\n\\]\nComo todos têm a mesma esperança: \\[\nE(X)\n= r \\cdot E(Y_1).\n\\]\nUsando o valor encontrado para a geométrica: \\[\nE(Y_1) = \\frac{1-p}{p},\n\\] então: \\[\nE(X)\n= r \\cdot \\frac{1-p}{p}\n= \\frac{r(1-p)}{p}.\n\\]\nEste é o valor esperado da Binomial Negativa (número de falhas até o \\(r\\)-ésimo sucesso).\n\n\n\n3.4.5 5. Cálculo de \\(\\mathrm{Var}(X)\\)\nDa variância da soma de variáveis independentes: \\[\n\\mathrm{Var}(X)\n= \\mathrm{Var}(Y_1 + \\cdots + Y_r)\n= \\mathrm{Var}(Y_1) + \\cdots + \\mathrm{Var}(Y_r),\n\\] pois \\(\\mathrm{Cov}(Y_i, Y_j)=0\\) para \\(i\\ne j\\).\nComo todas têm a mesma variância: \\[\n\\mathrm{Var}(X)\n= r \\cdot \\mathrm{Var}(Y_1).\n\\]\nUsando o valor da geométrica: \\[\n\\mathrm{Var}(Y_1) = \\frac{1-p}{p^2},\n\\] obtemos: \\[\n\\mathrm{Var}(X)\n= r \\cdot \\frac{1-p}{p^2}\n= \\frac{r(1-p)}{p^2}.\n\\]\n\n\n\n3.4.6 6. Resultado final\nPortanto, para \\(X \\sim \\text{Binomial Negativa}(r,p)\\) (número de falhas até o \\(r\\)-ésimo sucesso), temos:\n\\[\nE(X) = \\frac{r(1-p)}{p},\n\\qquad\n\\mathrm{Var}(X) = \\frac{r(1-p)}{p^2}.\n\\]\n\n\n\n\nExemplo: Um pesquisador precisa de 4 pessoas que aceitem responder um questionário. Cada tentativa tem probabilidade \\(p=0{,}25\\) de sucesso. Seja \\(Y\\) = número de recusas até o 4º sucesso.\n\nCalcule \\(P(Y=6)\\).\n\nCalcule \\(E[Y]\\).\n\nSolução:\n\n\\[\nP(Y=6)=\\binom{9}{6}(0{,}75)^6 (0{,}25)^4 \\approx 0{,}050\n\\]\n\\[\nE[Y]=\\frac{4(1-0{,}25)}{0{,}25}=12\n\\]"
  },
  {
    "objectID": "exercicios/lista02.html#cálculo-de-ex-3",
    "href": "exercicios/lista02.html#cálculo-de-ex-3",
    "title": "Revisão Probabilidade I",
    "section": "3.7 1. Cálculo de \\(E(X)\\)",
    "text": "3.7 1. Cálculo de \\(E(X)\\)\nPela definição de esperança: \\[\nE(X)=\\sum_{k=0}^{\\infty} k\\,P(X=k)\n    =\\sum_{k=0}^{\\infty} k\\,e^{-\\lambda}\\frac{\\lambda^k}{k!}.\n\\]\nO termo \\(k=0\\) é nulo, então: \\[\nE(X)=e^{-\\lambda}\\sum_{k=1}^{\\infty} k\\frac{\\lambda^k}{k!}.\n\\]\nUsamos a identidade: \\[\nk\\frac{\\lambda^k}{k!}=\\frac{\\lambda^k}{(k-1)!}.\n\\]\nLogo: \\[\nE(X)=e^{-\\lambda}\\sum_{k=1}^{\\infty} \\frac{\\lambda^k}{(k-1)!}.\n\\]\nAgora fazemos a mudança de variável \\(j=k-1\\):\n\nquando \\(k=1\\), \\(j=0\\)\n\nquando \\(k\\to\\infty\\), \\(j\\to\\infty\\)\n\nEntão: \\[\nE(X)=e^{-\\lambda}\\sum_{j=0}^{\\infty} \\frac{\\lambda^{j+1}}{j!}\n= e^{-\\lambda}\\lambda \\sum_{j=0}^{\\infty} \\frac{\\lambda^{j}}{j!}.\n\\]\nMas: \\[\n\\sum_{j=0}^{\\infty} \\frac{\\lambda^{j}}{j!} = e^{\\lambda}.\n\\]\nPortanto: \\[\nE(X)=e^{-\\lambda}\\lambda e^{\\lambda} = \\lambda.\n\\]\nAssim demonstramos: \\[\nE(X)=\\lambda.\n\\]"
  },
  {
    "objectID": "exercicios/lista02.html#cálculo-de-ex2-2",
    "href": "exercicios/lista02.html#cálculo-de-ex2-2",
    "title": "Revisão Probabilidade I",
    "section": "3.8 2. Cálculo de \\(E(X^2)\\)",
    "text": "3.8 2. Cálculo de \\(E(X^2)\\)\nAgora usamos a definição: \\[\nE(X^2)=\\sum_{k=0}^{\\infty} k^2 P(X=k)\n      =e^{-\\lambda}\\sum_{k=0}^{\\infty} k^2 \\frac{\\lambda^k}{k!}.\n\\]\nTruque clássico: escrever \\[\nk^2 = k(k-1) + k.\n\\]\nEntão: \\[\nE(X^2)\n= e^{-\\lambda}\\sum_{k=0}^{\\infty} k(k-1)\\frac{\\lambda^k}{k!}\n+ e^{-\\lambda}\\sum_{k=0}^{\\infty} k\\frac{\\lambda^k}{k!}.\n\\]\nChamemos:\n\nPrimeiro somatório:\n\\[A = e^{-\\lambda}\\sum_{k=0}^{\\infty} k(k-1)\\frac{\\lambda^k}{k!}\\]\nSegundo somatório:\n\\[B = e^{-\\lambda}\\sum_{k=0}^{\\infty} k\\frac{\\lambda^k}{k!}\\]\n\nMas já vimos antes que \\(B = E(X) = \\lambda\\).\nVamos calcular \\(A\\).\n\n\n3.8.1 2.1 Cálculo do termo \\(A\\)\nNote que: \\[\nk(k-1)\\frac{\\lambda^k}{k!}\n= \\frac{\\lambda^k}{(k-2)!}.\n\\]\nPortanto: \\[\nA = e^{-\\lambda}\\sum_{k=2}^{\\infty}\\frac{\\lambda^k}{(k-2)!}.\n\\]\nFazemos a mudança de variável \\(j=k-2\\):\n\nquando \\(k=2\\), \\(j=0\\)\nquando \\(k\\to\\infty\\), \\(j\\to\\infty\\)\n\nEntão: \\[\nA = e^{-\\lambda}\\sum_{j=0}^{\\infty}\\frac{\\lambda^{j+2}}{j!}\n= e^{-\\lambda}\\lambda^2 \\sum_{j=0}^{\\infty}\\frac{\\lambda^{j}}{j!}.\n\\]\nA soma é novamente \\(e^\\lambda\\), logo: \\[\nA = e^{-\\lambda}\\lambda^2 e^\\lambda = \\lambda^2.\n\\]\n\n\n\n3.8.2 2.2 Conclusão para \\(E(X^2)\\)\nAgora juntamos:\n\n\\(A = \\lambda^2\\)\n\\(B = \\lambda\\)\n\nPortanto: \\[\nE(X^2) = A + B = \\lambda^2 + \\lambda.\n\\]"
  },
  {
    "objectID": "exercicios/lista02.html#variância",
    "href": "exercicios/lista02.html#variância",
    "title": "Revisão Probabilidade I",
    "section": "3.9 3. Variância",
    "text": "3.9 3. Variância\nUsamos: \\[\n\\mathrm{Var}(X)=E(X^2)-[E(X)]^2.\n\\]\nSubstituindo:\n\n\\(E(X)=\\lambda\\)\n\\(E(X^2)=\\lambda^2 + \\lambda\\)\n\nTemos: \\[\n\\mathrm{Var}(X)\n= (\\lambda^2+\\lambda) - \\lambda^2\n= \\lambda.\n\\]"
  },
  {
    "objectID": "exercicios/lista02.html#resultado-final-4",
    "href": "exercicios/lista02.html#resultado-final-4",
    "title": "Revisão Probabilidade I",
    "section": "3.10 4. Resultado final",
    "text": "3.10 4. Resultado final\nPara \\(X \\sim \\text{Poisson}(\\lambda)\\):\n\\[\nE(X)=\\lambda,\n\\qquad\n\\mathrm{Var}(X)=\\lambda.\n\\]"
  },
  {
    "objectID": "exercicios/lista02.html#cálculo-de-ex-4",
    "href": "exercicios/lista02.html#cálculo-de-ex-4",
    "title": "Revisão Probabilidade I",
    "section": "4.2 1. Cálculo de \\(E(X)\\)",
    "text": "4.2 1. Cálculo de \\(E(X)\\)\nPela definição: \\[\nE(X) = \\int_{-\\infty}^{\\infty} x f(x)\\,dx.\n\\]\nComo \\(f(x)=0\\) fora do intervalo \\([a,b]\\), temos: \\[\nE(X) = \\int_a^b x \\cdot \\frac{1}{b-a}\\,dx\n     = \\frac{1}{b-a} \\int_a^b x\\,dx.\n\\]\nCalculamos a integral: \\[\n\\int_a^b x\\,dx = \\left[\\frac{x^2}{2}\\right]_a^b\n= \\frac{b^2}{2} - \\frac{a^2}{2}\n= \\frac{b^2 - a^2}{2}.\n\\]\nLogo: \\[\nE(X) = \\frac{1}{b-a} \\cdot \\frac{b^2 - a^2}{2}.\n\\]\nFatoramos \\(b^2 - a^2\\): \\[\nb^2 - a^2 = (b-a)(b+a).\n\\]\nSubstituindo: \\[\nE(X) = \\frac{1}{b-a} \\cdot \\frac{(b-a)(b+a)}{2}\n     = \\frac{b+a}{2}.\n\\]\nPortanto: \\[\nE(X) = \\frac{a+b}{2}.\n\\]"
  },
  {
    "objectID": "exercicios/lista02.html#cálculo-de-ex2-3",
    "href": "exercicios/lista02.html#cálculo-de-ex2-3",
    "title": "Revisão Probabilidade I",
    "section": "4.3 2. Cálculo de \\(E(X^2)\\)",
    "text": "4.3 2. Cálculo de \\(E(X^2)\\)\nPela definição: \\[\nE(X^2) = \\int_{-\\infty}^{\\infty} x^2 f(x)\\,dx\n       = \\int_a^b x^2 \\cdot \\frac{1}{b-a}\\,dx\n       = \\frac{1}{b-a}\\int_a^b x^2\\,dx.\n\\]\nCalculamos a integral: \\[\n\\int_a^b x^2\\,dx = \\left[\\frac{x^3}{3}\\right]_a^b\n= \\frac{b^3}{3} - \\frac{a^3}{3}\n= \\frac{b^3 - a^3}{3}.\n\\]\nEntão: \\[\nE(X^2) = \\frac{1}{b-a} \\cdot \\frac{b^3 - a^3}{3}\n       = \\frac{b^3 - a^3}{3(b-a)}.\n\\]\nAgora usamos a fatoração: \\[\nb^3 - a^3 = (b-a)(b^2 + ab + a^2).\n\\]\nSubstituindo: \\[\nE(X^2) = \\frac{(b-a)(b^2 + ab + a^2)}{3(b-a)}\n       = \\frac{b^2 + ab + a^2}{3}.\n\\]\nPortanto: \\[\nE(X^2) = \\frac{a^2 + ab + b^2}{3}.\n\\]"
  },
  {
    "objectID": "exercicios/lista02.html#cálculo-de-mathrmvarx-3",
    "href": "exercicios/lista02.html#cálculo-de-mathrmvarx-3",
    "title": "Revisão Probabilidade I",
    "section": "4.4 3. Cálculo de \\(\\mathrm{Var}(X)\\)",
    "text": "4.4 3. Cálculo de \\(\\mathrm{Var}(X)\\)\nUsamos: \\[\n\\mathrm{Var}(X) = E(X^2) - [E(X)]^2.\n\\]\nJá obtivemos:\n\n\\(E(X) = \\dfrac{a+b}{2}\\)\n\n\\(E(X^2) = \\dfrac{a^2 + ab + b^2}{3}\\)\n\nEntão: \\[\n\\mathrm{Var}(X)\n= \\frac{a^2 + ab + b^2}{3} - \\left(\\frac{a+b}{2}\\right)^2.\n\\]\nPrimeiro, calculemos o quadrado: \\[\n\\left(\\frac{a+b}{2}\\right)^2\n= \\frac{(a+b)^2}{4}\n= \\frac{a^2 + 2ab + b^2}{4}.\n\\]\nAgora escrevemos a variância com denominador comum.\nO denominador comum entre 3 e 4 é 12:\n\nPrimeiro termo: \\[\n\\frac{a^2 + ab + b^2}{3}\n= \\frac{4(a^2 + ab + b^2)}{12}.\n\\]\nSegundo termo: \\[\n\\frac{a^2 + 2ab + b^2}{4}\n= \\frac{3(a^2 + 2ab + b^2)}{12}.\n\\]\n\nEntão: \\[\n\\mathrm{Var}(X)\n= \\frac{4(a^2 + ab + b^2)}{12} - \\frac{3(a^2 + 2ab + b^2)}{12}.\n\\]\nSubtraindo os numeradores: \\[\n4(a^2 + ab + b^2) - 3(a^2 + 2ab + b^2)\n= 4a^2 + 4ab + 4b^2 - 3a^2 - 6ab - 3b^2\n= (4a^2 - 3a^2) + (4ab - 6ab) + (4b^2 - 3b^2)\n= a^2 - 2ab + b^2.\n\\]\nLogo: \\[\n\\mathrm{Var}(X)\n= \\frac{a^2 - 2ab + b^2}{12}\n= \\frac{(b-a)^2}{12},\n\\] pois \\[\na^2 - 2ab + b^2 = (b-a)^2.\n\\]"
  },
  {
    "objectID": "exercicios/lista02.html#resultado-final-5",
    "href": "exercicios/lista02.html#resultado-final-5",
    "title": "Revisão Probabilidade I",
    "section": "4.5 4. Resultado final",
    "text": "4.5 4. Resultado final\nPara \\(X \\sim \\text{Uniforme}(a,b)\\), demonstramos que: \\[\nE(X) = \\frac{a+b}{2},\n\\qquad\n\\mathrm{Var}(X) = \\frac{(b-a)^2}{12}.\n\\]"
  },
  {
    "objectID": "exercicios/lista02.html#cálculo-de-ex-5",
    "href": "exercicios/lista02.html#cálculo-de-ex-5",
    "title": "Revisão Probabilidade I",
    "section": "4.7 1. Cálculo de \\(E(X)\\)",
    "text": "4.7 1. Cálculo de \\(E(X)\\)\nPela definição: \\[\nE(X) = \\int_0^\\infty x\\lambda e^{-\\lambda x}\\,dx.\n\\]\nPara resolver a integral, usamos integração por partes.\nEscolhemos: - (u = x) → (du = dx) - (dv = e^{-x}dx) → (v = -e^{-x})\nAplicando integração por partes: \\[\n\\int_0^\\infty x\\lambda e^{-\\lambda x}\\,dx\n= \\left[-x e^{-\\lambda x}\\right]_{0}^{\\infty}\n+ \\int_0^\\infty e^{-\\lambda x}\\,dx.\n\\]\nAgora avaliamos cada termo:\n\nTermo de fronteira:\n\nQuando (x): (x e^{-x} ) (exponencial domina linear).\nQuando (x=0): (0 e^{0} = 0).\n\nLogo: \\[\n\\left[-x e^{-\\lambda x}\\right]_{0}^{\\infty} = 0.\n\\]\nIntegral restante: \\[\n\\int_0^\\infty e^{-\\lambda x}\\,dx\n= \\left[-\\frac{1}{\\lambda}e^{-\\lambda x}\\right]_{0}^{\\infty}\n= \\frac{1}{\\lambda}.\n\\]\n\nPortanto: \\[\nE(X)=\\frac{1}{\\lambda}.\n\\]"
  },
  {
    "objectID": "exercicios/lista02.html#cálculo-de-ex2-4",
    "href": "exercicios/lista02.html#cálculo-de-ex2-4",
    "title": "Revisão Probabilidade I",
    "section": "4.8 2. Cálculo de \\(E(X^2)\\)",
    "text": "4.8 2. Cálculo de \\(E(X^2)\\)\nPela definição: \\[\nE(X^2)=\\int_0^\\infty x^2 \\lambda e^{-\\lambda x}\\,dx.\n\\]\nVamos aplicar integração por partes duas vezes.\n\n\n4.8.1 Primeira integração por partes\nEscolha: - (u = x^2) → (du = 2x,dx) - (dv = e^{-x}dx) → (v = -e^{-x})\nEntão: \\[\nE(X^2)\n= \\left[-x^2 e^{-\\lambda x}\\right]_0^\\infty\n+ \\int_0^\\infty 2x e^{-\\lambda x} \\, dx.\n\\]\nO termo de fronteira é zero, pelo mesmo argumento anterior.\nAssim: \\[\nE(X^2)= 2\\int_0^\\infty x e^{-\\lambda x}\\,dx.\n\\]\n\n\n\n4.8.2 Segunda integração por partes\nAgora resolvemos: \\[\n\\int_0^\\infty x e^{-\\lambda x}\\,dx.\n\\]\nEscolha: - (u = x) → (du = dx) - (dv = e^{-x}dx) → (v = -e^{-x})\nEntão: \\[\n\\int_0^\\infty x e^{-\\lambda x}\\,dx\n= \\left[-\\frac{x}{\\lambda} e^{-\\lambda x}\\right]_0^\\infty\n+ \\int_0^\\infty \\frac{1}{\\lambda} e^{-\\lambda x}\\,dx.\n\\]\nO termo de fronteira novamente é zero.\nResta: \\[\n\\int_0^\\infty x e^{-\\lambda x}\\,dx\n= \\frac{1}{\\lambda} \\int_0^\\infty e^{-\\lambda x}\\,dx\n= \\frac{1}{\\lambda}\\cdot\\frac{1}{\\lambda}\n= \\frac{1}{\\lambda^2}.\n\\]\n\n\n\n4.8.3 Conclusão do cálculo de \\(E(X^2)\\)\nVoltando ao ponto onde paramos: \\[\nE(X^2)= 2\\left(\\frac{1}{\\lambda^2}\\right)\n= \\frac{2}{\\lambda^2}.\n\\]"
  },
  {
    "objectID": "exercicios/lista02.html#cálculo-da-variância",
    "href": "exercicios/lista02.html#cálculo-da-variância",
    "title": "Revisão Probabilidade I",
    "section": "4.9 3. Cálculo da variância",
    "text": "4.9 3. Cálculo da variância\nUsamos: \\[\n\\mathrm{Var}(X)=E(X^2)-[E(X)]^2.\n\\]\nSubstituindo:\n\n\\(E(X^2)=\\frac{2}{\\lambda^2}\\)\n\\(E(X)=\\frac{1}{\\lambda}\\)\n\nEntão: \\[\n\\mathrm{Var}(X)\n= \\frac{2}{\\lambda^2} - \\left(\\frac{1}{\\lambda}\\right)^2\n= \\frac{2}{\\lambda^2} - \\frac{1}{\\lambda^2}\n= \\frac{1}{\\lambda^2}.\n\\]"
  },
  {
    "objectID": "exercicios/lista02.html#resultado-final-6",
    "href": "exercicios/lista02.html#resultado-final-6",
    "title": "Revisão Probabilidade I",
    "section": "4.10 4. Resultado final",
    "text": "4.10 4. Resultado final\nPara \\(X \\sim \\text{Exponencial}(\\lambda)\\):\n\\[\nE(X)=\\frac{1}{\\lambda},\n\\qquad\n\\mathrm{Var}(X)=\\frac{1}{\\lambda^2}.\n\\]"
  },
  {
    "objectID": "exercicios/lista02.html#normal-padrão-z-sim-n01",
    "href": "exercicios/lista02.html#normal-padrão-z-sim-n01",
    "title": "Revisão Probabilidade I",
    "section": "4.12 1. Normal padrão \\(Z \\sim N(0,1)\\)",
    "text": "4.12 1. Normal padrão \\(Z \\sim N(0,1)\\)\nPara a normal padrão, a densidade é \\[\nf_Z(z)=\\frac{1}{\\sqrt{2\\pi}}e^{-z^2/2},\\quad z\\in\\mathbb{R}.\n\\]\nQueremos mostrar que: \\[\nE(Z)=0, \\qquad E(Z^2)=1 \\quad\\Rightarrow\\quad \\mathrm{Var}(Z)=1.\n\\]\n\n\n4.12.1 1.1 Cálculo de \\(E(Z)\\)\nPela definição: \\[\nE(Z)=\\int_{-\\infty}^{\\infty} z f_Z(z)\\,dz\n= \\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{\\infty} z e^{-z^2/2}\\,dz.\n\\]\nObserve que:\n\nA função \\(e^{-z^2/2}\\) é par (simétrica): \\(e^{-(-z)^2/2}=e^{-z^2/2}\\).\nA função \\(z\\) é ímpar: \\((-z) = -z\\).\nLogo, o produto \\(z e^{-z^2/2}\\) é ímpar.\n\nA integral de uma função ímpar em intervalo simétrico é zero: \\[\n\\int_{-\\infty}^{\\infty} z e^{-z^2/2}\\,dz = 0.\n\\]\nPortanto: \\[\nE(Z)=0.\n\\]\n\n\n\n4.12.2 1.2 Cálculo de \\(E(Z^2)\\)\nPela definição: \\[\nE(Z^2)=\\int_{-\\infty}^{\\infty} z^2 f_Z(z)\\,dz\n= \\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{\\infty} z^2 e^{-z^2/2}\\,dz.\n\\]\nPara calcular \\[\n\\int_{-\\infty}^{\\infty} z^2 e^{-z^2/2}\\,dz,\n\\] vamos usar um truque padrão com um parâmetro auxiliar.\nConsidere, para \\(a&gt;0\\): \\[\nI(a) = \\int_{-\\infty}^{\\infty} e^{-a z^2/2}\\,dz.\n\\]\nEste é um integral gaussiano. Sabe-se (ou demonstra-se via coordenadas polares) que \\[\nI(a) = \\sqrt{\\frac{2\\pi}{a}}.\n\\]\nAgora vamos derivar \\(I(a)\\) em relação a \\(a\\) para obter uma integral com \\(z^2\\).\n\n\n4.12.2.1 Derivando \\(I(a)\\)\nPor um lado, derivando “dentro” da integral (legítimo sob condições usuais):\n\\[\nI'(a) = \\frac{d}{da}\\int_{-\\infty}^{\\infty} e^{-a z^2/2}\\,dz\n      = \\int_{-\\infty}^{\\infty} \\frac{\\partial}{\\partial a}\n        \\big(e^{-a z^2/2}\\big)\\,dz.\n\\]\nMas \\[\n\\frac{\\partial}{\\partial a} e^{-a z^2/2}\n= -\\frac{z^2}{2} e^{-a z^2/2}.\n\\]\nLogo: \\[\nI'(a) = \\int_{-\\infty}^{\\infty} -\\frac{z^2}{2} e^{-a z^2/2}\\,dz\n      = -\\frac{1}{2}\\int_{-\\infty}^{\\infty} z^2 e^{-a z^2/2}\\,dz.\n\\]\nPor outro lado, derivando a expressão fechada \\[\nI(a) = \\sqrt{\\frac{2\\pi}{a}}\n= (2\\pi)^{1/2} a^{-1/2},\n\\]\ntemos \\[\nI'(a) = (2\\pi)^{1/2}\\left(-\\frac{1}{2}\\right)a^{-3/2}\n      = -\\frac{\\sqrt{2\\pi}}{2}\\,a^{-3/2}.\n\\]\nIgualando as duas expressões para \\(I'(a)\\): \\[\n-\\frac{1}{2}\\int_{-\\infty}^{\\infty} z^2 e^{-a z^2/2}\\,dz\n= -\\frac{\\sqrt{2\\pi}}{2}\\,a^{-3/2}.\n\\]\nMultiplicando por \\(-2\\) ambos os lados: \\[\n\\int_{-\\infty}^{\\infty} z^2 e^{-a z^2/2}\\,dz\n= \\sqrt{2\\pi}\\,a^{-3/2}.\n\\]\n\n\n\n4.12.2.2 Aplicando o resultado em \\(a=1\\)\nQueremos o caso com \\(a=1\\), isto é, \\[\n\\int_{-\\infty}^{\\infty} z^2 e^{-z^2/2}\\,dz.\n\\]\nPela fórmula: \\[\n\\int_{-\\infty}^{\\infty} z^2 e^{-z^2/2}\\,dz\n= \\sqrt{2\\pi}\\cdot 1^{-3/2}\n= \\sqrt{2\\pi}.\n\\]\nAgora voltamos para \\(E(Z^2)\\): \\[\nE(Z^2)\n= \\frac{1}{\\sqrt{2\\pi}}\n\\int_{-\\infty}^{\\infty} z^2 e^{-z^2/2}\\,dz\n= \\frac{1}{\\sqrt{2\\pi}}\\cdot \\sqrt{2\\pi} = 1.\n\\]\nLogo: \\[\nE(Z^2)=1.\n\\]\n\n\n\n\n4.12.3 1.3 Variância da normal padrão\nA variância é \\[\n\\mathrm{Var}(Z)=E(Z^2)-[E(Z)]^2=1-0^2=1.\n\\]\nConcluímos: \\[\nE(Z)=0,\\qquad \\mathrm{Var}(Z)=1\n\\quad\\text{para } Z\\sim N(0,1).\n\\]"
  },
  {
    "objectID": "exercicios/lista02.html#normal-geral-x-sim-nmusigma2",
    "href": "exercicios/lista02.html#normal-geral-x-sim-nmusigma2",
    "title": "Revisão Probabilidade I",
    "section": "4.13 2. Normal geral \\(X \\sim N(\\mu,\\sigma^2)\\)",
    "text": "4.13 2. Normal geral \\(X \\sim N(\\mu,\\sigma^2)\\)\nUma variável normal geral pode ser escrita como \\[\nX = \\mu + \\sigma Z,\n\\] onde \\(Z \\sim N(0,1)\\).\nVamos usar essa relação para encontrar \\(E(X)\\) e \\(\\mathrm{Var}(X)\\).\n\n\n4.13.1 2.1 Cálculo de \\(E(X)\\)\nUsando a linearidade da esperança: \\[\nE(X) = E(\\mu + \\sigma Z)\n     = E(\\mu) + E(\\sigma Z)\n     = \\mu + \\sigma E(Z).\n\\]\nComo já mostramos que \\(E(Z)=0\\), obtemos: \\[\nE(X) = \\mu + \\sigma \\cdot 0 = \\mu.\n\\]\n\n\n\n4.13.2 2.2 Cálculo de \\(\\mathrm{Var}(X)\\)\nUsamos a propriedade de variância para transformações lineares: \\[\n\\mathrm{Var}(a + bZ) = b^2\\,\\mathrm{Var}(Z).\n\\]\nAqui, \\(a=\\mu\\) e \\(b=\\sigma\\), então: \\[\n\\mathrm{Var}(X) = \\mathrm{Var}(\\mu + \\sigma Z)\n                = \\sigma^2\\,\\mathrm{Var}(Z).\n\\]\nSabemos que \\(\\mathrm{Var}(Z)=1\\), logo: \\[\n\\mathrm{Var}(X) = \\sigma^2 \\cdot 1 = \\sigma^2.\n\\]"
  },
  {
    "objectID": "exercicios/lista02.html#resultado-final-7",
    "href": "exercicios/lista02.html#resultado-final-7",
    "title": "Revisão Probabilidade I",
    "section": "4.14 3. Resultado final",
    "text": "4.14 3. Resultado final\nPara uma variável aleatória normal geral \\(X \\sim N(\\mu,\\sigma^2)\\), demonstramos que:\n\\[\nE(X) = \\mu,\n\\qquad\n\\mathrm{Var}(X) = \\sigma^2.\n\\]"
  },
  {
    "objectID": "revisao.html",
    "href": "revisao.html",
    "title": "Revisão Probabilidade I",
    "section": "",
    "text": "Definição. Uma variável aleatória é uma função que associa a cada resultado do experimento um número real.\n\nDiscretas: suporte finito/enumerável (ex.: 0,1,2,…). Caracterizam-se por função de probabilidade \\(P(X=x_i) =p(x_i) =p_i\\). Uma função de probabilidade satisfaz \\(0 \\le p_i \\le 1\\) e \\(\\sum_{i=1} p_i = 1\\)\nContínuas: suporte intervalar. Caracterizam-se por densidade \\(f(x)\\) tal que \\(P(a&lt;X&lt; b)=\\int_a^b f(x)\\,dx\\). Uma função densidade satisfaz \\(f(x) \\ge 0\\) e \\(\\int_{-\\infty}^{\\infty} f(x)\\, dx = 1\\).\n\nA função de distribuição é \\(F(x)=P(X\\le x)\\). Em ambos os casos, \\(F(x)\\) é não-decrescente e \\(\\lim_{x\\to-\\infty}F(x)=0\\), \\(\\lim_{x\\to\\infty}F(x)=1\\).",
    "crumbs": [
      "Revisão Probabilidade I"
    ]
  },
  {
    "objectID": "revisao.html#bernoulli-p",
    "href": "revisao.html#bernoulli-p",
    "title": "Revisão Probabilidade I",
    "section": "3.1 Bernoulli \\((p)\\)",
    "text": "3.1 Bernoulli \\((p)\\)\nInterpretação: 1 sucesso/fracasso em único ensaio.\n\n\n\n\n\n\n\nItem\nExpressão\n\n\n\n\nSuporte\n\\(x\\in\\{0,1\\}\\)\n\n\nParâmetro\n\\(0&lt;p&lt;1\\)\n\n\nFunção de probabilidade \\(p(x)\\)\n\\(p^x(1-p)^{1-x}\\)\n\n\nDistribuição \\(F(x)\\)\n\\(0\\) se \\(x&lt;0\\); \\(1-p\\) se \\(0\\le x&lt;1\\); \\(1\\) se \\(x\\ge1\\)\n\n\nEsperança \\(E[X]\\)\n\\(p\\)\n\n\nVariância \\(Var(X)\\)\n\\(p(1-p)\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMostrar demonstração\n\n\n\n\n\nA variável Bernoulli assume valores \\(0\\) e \\(1\\), com \\(P(X=1)=p\\) e \\(P(X=0)=1-p\\).\nEsperança\n\\[\nE(X)=0(1-p)+1\\cdot p = p.\n\\]\nSegundo momento\n\\[\nE(X^2)=0^2(1-p)+1^2\\cdot p = p.\n\\]\nVariância\n\\[\n\\mathrm{Var}(X)=E(X^2)-[E(X)]^2\n= p - p^2 = p(1-p).\n\\]\n\n\n\nExemplo: Um alarme dispara corretamente com probabilidade \\(p = 0{,}92\\). Seja \\(X\\) = 1 se o alarme dispara corretamente, 0 caso contrário.\n\nCalcule \\(P(X=1)\\) e \\(P(X=0)\\).\n\nCalcule \\(E[X]\\) e interprete.\n\nSolução:\n\n\\(P(X=1)=0{,}92\\)\n\n\\(P(X=0)=0{,}08\\)\n\n\\[\nE[X]=p=0{,}92\n\\]\nInterpretação: o alarme funciona corretamente em 92% dos acionamentos.",
    "crumbs": [
      "Revisão Probabilidade I"
    ]
  },
  {
    "objectID": "revisao.html#binomial-np",
    "href": "revisao.html#binomial-np",
    "title": "Revisão Probabilidade I",
    "section": "3.2 Binomial \\((n,p)\\)",
    "text": "3.2 Binomial \\((n,p)\\)\nInterpretação: número de sucessos em \\(n\\) ensaios independentes, prob. \\(p\\).\n\n\n\nItem\nExpressão\n\n\n\n\nSuporte\n\\(k=0,1,\\dots,n\\)\n\n\nParâmetros\n\\(n\\in\\mathbb{N}\\), \\(0&lt;p&lt;1\\)\n\n\n\\(p(k)\\)\n\\(\\binom{n}{k}p^k(1-p)^{n-k}\\)\n\n\n\\(F(k)\\)\n\\(\\sum_{j=0}^{k}\\binom{n}{j}p^j(1-p)^{n-j}\\)\n\n\n\\(E[X]\\)\n\\(np\\)\n\n\n\\(Var(X)\\)\n\\(np(1-p)\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMostrar demonstração\n\n\n\n\n\nConsidere \\(X \\sim \\text{Binomial}(n,p)\\), isto é, \\[\nP(X = k) = \\binom{n}{k} p^k (1-p)^{n-k}, \\quad k = 0,1,\\dots,n.\n\\]\nVamos demonstrar:\n\n\\(E(X) = np\\)\n\\(\\mathrm{Var}(X) = np(1-p)\\)\n\nusando\n\\[\nE(X) = \\sum_{k=0}^n k\\,P(X=k),\n\\qquad\n\\mathrm{Var}(X) = E(X^2) - [E(X)]^2.\n\\]\n\nCálculo de E(X)\nPela definição: \\[\nE(X) = \\sum_{k=0}^n k\\,P(X=k)\n     = \\sum_{k=0}^n k \\binom{n}{k} p^k (1-p)^{n-k}.\n\\]\nNote que o termo com \\(k=0\\) é zero (pois tem um fator \\(k\\)), então podemos começar em \\(k=1\\): \\[\nE(X) = \\sum_{k=1}^n k \\binom{n}{k} p^k (1-p)^{n-k}.\n\\]\nUsamos agora a identidade combinatória \\[\nk \\binom{n}{k} = n \\binom{n-1}{k-1}.\n\\]\nSubstituindo: \\[\nE(X)= \\sum_{k=1}^n n \\binom{n-1}{k-1} p^k (1-p)^{n-k}.\n\\]\nColocamos \\(n\\) em evidência: \\[\nE(X)= n \\sum_{k=1}^n \\binom{n-1}{k-1} p^k (1-p)^{n-k}.\n\\]\nAgora fazemos a mudança de índice \\(j = k-1\\):\n\nquando \\(k = 1\\), \\(j = 0\\);\n\nquando \\(k = n\\), \\(j = n-1\\);\n\n\\(k = j + 1\\).\n\nEntão: \\[\nE(X)= n \\sum_{j=0}^{n-1} \\binom{n-1}{j} p^{j+1} (1-p)^{n-(j+1)}.\n\\]\nReorganizando as potências: \\[\np^{j+1} = p \\cdot p^j,\\qquad n-(j+1) = (n-1)-j,\n\\] obtemos: \\[\nE(X)= n p \\sum_{j=0}^{n-1} \\binom{n-1}{j} p^{j} (1-p)^{(n-1)-j}.\n\\]\nRepare que a soma \\[\n\\sum_{j=0}^{n-1} \\binom{n-1}{j} p^{j} (1-p)^{(n-1)-j}\n\\] é exatamente o desenvolvimento binomial de \\[\n(p + (1-p))^{n-1} = 1^{n-1} = 1.\n\\]\nPortanto: \\[\nE(X) = n p \\cdot 1 = np.\n\\]\n\nCálculo de \\(E(X^2)\\)\nPela definição: \\[\nE(X^2) = \\sum_{k=0}^n k^2\\,P(X=k)\n       = \\sum_{k=0}^n k^2 \\binom{n}{k} p^k (1-p)^{n-k}.\n\\]\nTruque clássico: escrever \\[\nk^2 = k(k-1) + k.\n\\]\nEntão: \\[\n\\begin{eqnarray}\nE(X^2)\n&=& \\sum_{k=0}^n [k(k-1) + k] \\binom{n}{k} p^k (1-p)^{n-k} \\\\\n&=& \\sum_{k=0}^n k(k-1)\\binom{n}{k} p^k (1-p)^{n-k}\n  + \\sum_{k=0}^n k\\binom{n}{k} p^k (1-p)^{n-k}.\n\\end{eqnarray}\n\\]\nChamemos:\n\n\\(A = \\sum_{k=0}^n k(k-1)\\binom{n}{k} p^k (1-p)^{n-k}\\)\n\n\\(B = \\sum_{k=0}^n k\\binom{n}{k} p^k (1-p)^{n-k}\\)\n\nLogo, \\(E(X^2) = A + B\\).\nMas repare que \\(B = E(X)\\), que já calculamos: \\[\nB = E(X) = np.\n\\]\nFalta calcular \\(A\\).\n\nCálculo de \\(A\\)\nUsamos agora a identidade: \\[\nk(k-1)\\binom{n}{k} = n(n-1)\\binom{n-2}{k-2}.\n\\]\nEntão: \\[\nA = \\sum_{k=0}^n k(k-1)\\binom{n}{k} p^k (1-p)^{n-k}\n  = \\sum_{k=2}^n n(n-1)\\binom{n-2}{k-2} p^k (1-p)^{n-k}.\n\\]\nPodemos tirar \\(n(n-1)\\) em evidência: \\[\nA = n(n-1)\\sum_{k=2}^n \\binom{n-2}{k-2} p^k (1-p)^{n-k}.\n\\]\nAgora faça a mudança de índice \\(j = k-2\\):\n\nquando \\(k = 2\\), \\(j = 0\\);\n\nquando \\(k = n\\), \\(j = n-2\\);\n\n\\(k = j + 2\\).\n\nSubstituindo: \\[\nA = n(n-1)\\sum_{j=0}^{n-2} \\binom{n-2}{j} p^{j+2} (1-p)^{n-(j+2)}.\n\\]\nReorganizando as potências: \\[\np^{j+2} = p^2 p^j,\n\\qquad\nn-(j+2) = (n-2)-j,\n\\] temos: \\[\nA = n(n-1)p^2 \\sum_{j=0}^{n-2} \\binom{n-2}{j} p^{j} (1-p)^{(n-2)-j}.\n\\]\nA soma é novamente um binômio: \\[\n\\sum_{j=0}^{n-2} \\binom{n-2}{j} p^{j} (1-p)^{(n-2)-j}\n= (p + (1-p))^{n-2} = 1^{n-2} = 1.\n\\]\nPortanto: \\[\nA = n(n-1)p^2.\n\\]\n\nConclusão para \\(E(X^2)\\)\nLembrando que:\n\n\\(A = n(n-1)p^2\\)\n\n\\(B = np\\)\n\ntemos: \\[\nE(X^2) = A + B = n(n-1)p^2 + np.\n\\]\n\nCálculo de \\(\\mathrm{Var}(X)\\)\nAgora usamos: \\[\n\\mathrm{Var}(X) = E(X^2) - [E(X)]^2.\n\\]\nJá temos:\n\n\\(E(X^2) = n(n-1)p^2 + np\\)\n\\(E(X) = np\\)\n\nLogo: \\[\n\\mathrm{Var}(X)\n= \\big(n(n-1)p^2 + np\\big) - (np)^2.\n\\]\nAgora expandimos: \\[\n(np)^2 = n^2 p^2,\n\\] então: \\[\n\\mathrm{Var}(X)\n= n(n-1)p^2 + np - n^2 p^2.\n\\]\nNote que \\[\nn(n-1)p^2 = (n^2 - n)p^2,\n\\] então: \\[\n\\mathrm{Var}(X)\n= (n^2 - n)p^2 + np - n^2 p^2\n= -n p^2 + np\n= n p (1-p).\n\\]\n\nLogo,\nPara \\(X \\sim \\text{Binomial}(n,p)\\), \\[\nE(X) = np,\n\\qquad\n\\mathrm{Var}(X) = np(1-p).\n\\]\n\n\n\nExemplo: A probabilidade de um cliente comprar um produto é \\(p=0{,}3\\). Em um dia, 20 clientes entram na loja. Seja \\(X\\) = número de compras.\n\nCalcule \\(P(X=8)\\).\n\nCalcule \\(E[X]\\).\n\nSolução:\n\n\\[\nP(X=8)=\\binom{20}{8}(0{,}3)^8(0{,}7)^{12}\\approx 0{,}053\n\\]\n\\[\nE[X]=np=20\\cdot 0{,}3 = 6\n\\]",
    "crumbs": [
      "Revisão Probabilidade I"
    ]
  },
  {
    "objectID": "revisao.html#geométrica-p",
    "href": "revisao.html#geométrica-p",
    "title": "Revisão Probabilidade I",
    "section": "3.3 Geométrica \\((p)\\)",
    "text": "3.3 Geométrica \\((p)\\)\nConvenção usada: \\(X\\) = número de ensaios até o 1º sucesso (apoio \\(1,2,\\dots\\)).\n\n\n\nItem\nExpressão\n\n\n\n\n\\(p(k)\\)\n\\(p(1-p)^{k-1}\\)\n\n\n\\(F(k)\\)\n\\(1-(1-p)^k\\)\n\n\n\\(E[X]\\)\n\\(1/p\\)\n\n\n\\(Var(X)\\)\n\\((1-p)/p^2\\)\n\n\nPropriedade\nSem memória: \\(P(X&gt;s+t\\mid X&gt;t)=P(X&gt;s)\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMostrar demonstração\n\n\n\n\n\nConsidere \\(X \\sim \\text{Geom}(p)\\) com suporte \\(k=1,2,\\dots\\), isto é, \\[\nP(X = k) = p(1-p)^{k-1}, \\quad k = 1,2,\\dots,\\quad 0&lt;p&lt;1.\n\\]\nVamos demonstrar que \\[\nE(X) = \\frac{1}{p}\n\\qquad \\text{e} \\qquad\n\\mathrm{Var}(X) = \\frac{1-p}{p^2},\n\\] usando as definições \\[\nE(X) = \\sum_{k=1}^{\\infty} k\\,P(X=k),\n\\qquad\n\\mathrm{Var}(X) = E(X^2) - [E(X)]^2.\n\\]\n\nCálculo de \\(E(X)\\)\nPela definição de esperança: \\[\nE(X) = \\sum_{k=1}^{\\infty} k\\,P(X=k)\n     = \\sum_{k=1}^{\\infty} k\\,p(1-p)^{k-1}.\n\\]\nColocamos o fator \\(p\\) em evidência: \\[\nE(X) = p \\sum_{k=1}^{\\infty} k (1-p)^{k-1}.\n\\]\nPara simplificar, definimos \\[\nr = 1-p.\n\\]\nNote que \\(0&lt;r&lt;1\\). Então a soma fica \\[\nE(X) = p \\sum_{k=1}^{\\infty} k r^{k-1}.\n\\]\nAgora usamos uma identidade clássica de séries: - Sabemos que, para \\(|r|&lt;1\\), \\[\n  \\sum_{k=0}^{\\infty} r^k = \\frac{1}{1-r}.\n  \\]\nDerivando em relação a \\(r\\): \\[\n\\frac{d}{dr}\\left(\\sum_{k=0}^{\\infty} r^k\\right)\n= \\sum_{k=0}^{\\infty} k r^{k-1}\n= \\frac{d}{dr}\\left(\\frac{1}{1-r}\\right)\n= \\frac{1}{(1-r)^2}.\n\\]\nNote que o termo \\(k=0\\) é zero, então \\[\n\\sum_{k=1}^{\\infty} k r^{k-1} = \\frac{1}{(1-r)^2}.\n\\]\nVoltando à expressão de \\(E(X)\\): \\[\nE(X) = p \\cdot \\frac{1}{(1-r)^2}.\n\\]\nLembrando que \\(r = 1-p\\), então \\[\n1-r = 1 - (1-p) = p.\n\\]\nLogo: \\[\nE(X) = p \\cdot \\frac{1}{p^2} = \\frac{1}{p}.\n\\]\n\nCálculo de \\(E(X^2)\\)\nPela definição: \\[\nE(X^2) = \\sum_{k=1}^{\\infty} k^2 P(X=k)\n       = \\sum_{k=1}^{\\infty} k^2 p(1-p)^{k-1}\n       = p \\sum_{k=1}^{\\infty} k^2 r^{k-1},\n\\] com \\(r = 1-p\\).\nEntão precisamos do valor de \\[\n\\sum_{k=1}^{\\infty} k^2 r^{k-1}.\n\\]\nUsaremos de novo a série geométrica e suas derivadas.\nJá sabemos: \\[\n\\sum_{k=0}^{\\infty} r^k = \\frac{1}{1-r}.\n\\]\n\nPrimeira derivada: \\[\n\\sum_{k=1}^{\\infty} k r^{k-1} = \\frac{1}{(1-r)^2}.\n\\]\nMultiplicando por \\(r\\): \\[\nr \\sum_{k=1}^{\\infty} k r^{k-1}\n= \\sum_{k=1}^{\\infty} k r^{k}\n= \\frac{r}{(1-r)^2}.\n\\]\nDerivando novamente: Vamos derivar a expressão \\[\n\\sum_{k=1}^{\\infty} k r^{k}\n\\] em relação a \\(r\\): \\[\n\\frac{d}{dr}\\left(\\sum_{k=1}^{\\infty} k r^{k}\\right)\n= \\sum_{k=1}^{\\infty} k^2 r^{k-1}.\n\\]\nAgora derivamos o outro lado: \\[\n\\frac{d}{dr}\\left(\\frac{r}{(1-r)^2}\\right)\n= \\frac{(1-r)^2 - r\\cdot 2(1-r)(-1)}{(1-r)^4}.\n\\]\nSimplificando o numerador: \\[\n(1-r)^2 + 2r(1-r)\n= (1 - 2r + r^2) + (2r - 2r^2)\n= 1 - r^2\n= (1-r)(1+r).\n\\]\nAssim, \\[\n\\frac{d}{dr}\\left(\\frac{r}{(1-r)^2}\\right)\n= \\frac{(1-r)(1+r)}{(1-r)^4}\n= \\frac{1+r}{(1-r)^3}.\n\\]\n\nPortanto, \\[\n\\sum_{k=1}^{\\infty} k^2 r^{k-1}\n= \\frac{1+r}{(1-r)^3}.\n\\]\nVoltando para \\(E(X^2)\\): \\[\nE(X^2) = p \\cdot \\frac{1+r}{(1-r)^3}.\n\\]\nSubstituímos \\(r=1-p\\) e \\(1-r=p\\):\n\n\\(1+r = 1 + (1-p) = 2-p\\)\n\n\\((1-r)^3 = p^3\\)\n\nEntão: \\[\nE(X^2)= p \\cdot \\frac{2-p}{p^3}\n      = \\frac{2-p}{p^2}.\n\\]\n\nCálculo de \\(\\mathrm{Var}(X)\\)\nAgora usamos \\[\n\\mathrm{Var}(X) = E(X^2) - [E(X)]^2.\n\\]\nJá encontramos:\n\n\\(E(X) = \\dfrac{1}{p}\\)\n\\(E(X^2) = \\dfrac{2-p}{p^2}\\)\n\nLogo: \\[\n\\mathrm{Var}(X)\n= \\frac{2-p}{p^2} - \\left(\\frac{1}{p}\\right)^2\n= \\frac{2-p}{p^2} - \\frac{1}{p^2}\n= \\frac{1-p}{p^2}.\n\\]\n\nLogo,\nPara \\(X \\sim \\text{Geom}(p)\\), com \\(P(X=k)=p(1-p)^{k-1}\\), \\(k=1,2,\\dots\\), temos:\n\\[\nE(X) = \\frac{1}{p},\n\\qquad\n\\mathrm{Var}(X) = \\frac{1-p}{p^2}.\n\\]\n\n\n\nExemplo: Uma chamada telefônica é atendida com probabilidade \\(p = 0{,}15\\). Seja \\(X\\) = número de tentativas até o primeiro atendimento.\n\nCalcule \\(P(X=4)\\).\n\nCalcule \\(P(X&gt;4)\\).\n\nDetermine \\(E[X]\\).\n\nSolução:\n\n\\[\nP(X=4)=0{,}15(0{,}85)^3 \\approx 0{,}092\n\\]\n\\[\nP(X&gt;4)=0{,}85^4 \\approx 0{,}522\n\\]\n\\[\nE[X]=\\frac{1}{0{,}15}\\approx 6{,}67\n\\]",
    "crumbs": [
      "Revisão Probabilidade I"
    ]
  },
  {
    "objectID": "revisao.html#pascal-binomial-negativa-rp",
    "href": "revisao.html#pascal-binomial-negativa-rp",
    "title": "Revisão Probabilidade I",
    "section": "3.4 Pascal / Binomial Negativa \\((r,p)\\)",
    "text": "3.4 Pascal / Binomial Negativa \\((r,p)\\)\nConvenção usada: \\(Y\\) = número de falhas antes do \\(r\\)-ésimo sucesso (apoio \\(0,1,\\dots\\)).\n\n\n\nItem\nExpressão\n\n\n\n\n\\(p(k)\\)\n\\(\\binom{k+r-1}{k}(1-p)^k p^r\\)\n\n\n\\(F(k)\\)\n\\(\\sum_{j=0}^{k}\\binom{j+r-1}{j}(1-p)^j p^r\\)\n\n\n\\(E[Y]\\)\n\\(r(1-p)/p\\)\n\n\n\\(Var(Y)\\)\n\\(r(1-p)/p^2\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMostrar demonstração\n\n\n\n\n\nVamos usar a seguinte definição:\n\nEm cada tentativa, ocorre sucesso com probabilidade \\(p\\) e falha com probabilidade \\(1-p\\), independentemente.\n\\(X\\) é o número de falhas até o \\(r\\)-ésimo sucesso.\nDizemos então que \\(X \\sim \\text{Binomial Negativa}(r,p)\\), com \\(r \\in \\mathbb{N}\\).\n\nA função de probabilidade é \\[\nP(X = k)\n= \\binom{k + r - 1}{k} (1-p)^k p^r,\n\\quad k = 0,1,2,\\dots\n\\]\nNosso objetivo é demonstrar que \\[\nE(X) = \\frac{r(1-p)}{p}\n\\qquad \\text{e} \\qquad\n\\mathrm{Var}(X) = \\frac{r(1-p)}{p^2}.\n\\]\nEm vez de somar diretamente a série da f.p., vamos usar uma interpretação construída a partir da Geométrica, que já sabemos tratar.\n\nLigação com a distribuição geométrica\nConsidere o processo de Bernoulli (tentativas independentes com probabilidade de sucesso \\(p\\)).\nEntre sucessos consecutivos, o número de falhas que ocorre é sempre do mesmo tipo:\n\nAntes do 1º sucesso, temos um certo número de falhas \\(Y_1\\).\nEntre o 1º e o 2º sucesso, temos um número de falhas \\(Y_2\\).\n…\nEntre o (r-1)º e o r-ésimo sucesso, temos um número de falhas \\(Y_r\\).\n\nEssas quantidades \\(Y_1, Y_2, \\dots, Y_r\\) são independentes e têm mesma distribuição.\nAlém disso, o número total de falhas até o \\(r\\)-ésimo sucesso é \\[\nX = Y_1 + Y_2 + \\cdots + Y_r.\n\\]\nVamos então:\n\nDeterminar a distribuição de cada \\(Y_i\\).\nCalcular \\(E(Y_i)\\) e \\(\\mathrm{Var}(Y_i)\\).\nSomar para obter \\(E(X)\\) e \\(\\mathrm{Var}(X)\\).\n\n\nDistribuição de \\(Y_i\\) (número de falhas entre dois sucessos)\nEntre dois sucessos consecutivos, o experimento funciona assim:\n\nObservamos uma sequência de falhas, todas com probabilidade \\((1-p)\\),\n\nseguida de um sucesso, com probabilidade \\(p\\).\n\nSe \\(Y_i = k\\), isso significa: falha, falha, …, falha (\\(k\\) vezes), depois um sucesso.\nLogo \\[\nP(Y_i = k) = (1-p)^k p, \\quad k = 0,1,2,\\dots\n\\]\nEssa é uma Geométrica com suporte em \\(\\{0,1,2,\\dots\\}\\), às vezes chamada de geométrica “deslocada” ou “número de falhas antes do sucesso”.\nSabemos que:\n\nSe \\(G\\) tem \\(P(G=k)=p(1-p)^{k-1}\\), \\(k=1,2,\\dots\\), então \\(E(G)=\\frac{1}{p}\\) e \\(\\mathrm{Var}(G)=\\frac{1-p}{p^2}\\).\nSe definimos \\(Y = G - 1\\), então \\(Y\\) tem suporte \\(\\{0,1,2,\\dots\\}\\), e \\[\nE(Y) = E(G-1) = E(G) - 1 = \\frac{1}{p} - 1 = \\frac{1-p}{p},\n\\] \\[\n\\mathrm{Var}(Y) = \\mathrm{Var}(G-1) = \\mathrm{Var}(G) = \\frac{1-p}{p^2}.\n\\]\n\nPortanto, cada \\(Y_i\\) tem:\n\\[\nE(Y_i)=\\frac{1-p}{p}, \\qquad\n\\mathrm{Var}(Y_i)=\\frac{1-p}{p^2}.\n\\]\nE os \\(Y_i\\) são i.i.d. (independentes e identicamente distribuídos).\n\nExpressão de \\(X\\) como soma de geométricas\nRelembrando: \\[\nX = Y_1 + Y_2 + \\cdots + Y_r,\n\\] com \\(Y_i\\) independentes e com a mesma distribuição.\nVamos usar:\n\nLinearidade da esperança: \\[\nE(X) = E(Y_1) + \\cdots + E(Y_r),\n\\]\nVariância da soma de independentes: \\[\n\\mathrm{Var}(X)\n= \\mathrm{Var}(Y_1) + \\cdots + \\mathrm{Var}(Y_r),\n\\] pois não há termos de covariância (independência \\(\\Rightarrow\\) covariância zero).\n\n\nCálculo de \\(E(X)\\)\nPela linearidade da esperança: \\[\nE(X)\n= E(Y_1 + \\cdots + Y_r)\n= E(Y_1) + \\cdots + E(Y_r).\n\\]\nComo todos têm a mesma esperança: \\[\nE(X)\n= r \\cdot E(Y_1).\n\\]\nUsando o valor encontrado para a geométrica: \\[\nE(Y_1) = \\frac{1-p}{p},\n\\] então: \\[\nE(X)\n= r \\cdot \\frac{1-p}{p}\n= \\frac{r(1-p)}{p}.\n\\]\nEste é o valor esperado da Binomial Negativa (número de falhas até o \\(r\\)-ésimo sucesso).\n\nCálculo de \\(\\mathrm{Var}(X)\\)\nDa variância da soma de variáveis independentes: \\[\n\\mathrm{Var}(X)\n= \\mathrm{Var}(Y_1 + \\cdots + Y_r)\n= \\mathrm{Var}(Y_1) + \\cdots + \\mathrm{Var}(Y_r),\n\\] pois \\(\\mathrm{Cov}(Y_i, Y_j)=0\\) para \\(i\\ne j\\).\nComo todas têm a mesma variância: \\[\n\\mathrm{Var}(X)\n= r \\cdot \\mathrm{Var}(Y_1).\n\\]\nUsando o valor da geométrica: \\[\n\\mathrm{Var}(Y_1) = \\frac{1-p}{p^2},\n\\] obtemos: \\[\n\\mathrm{Var}(X)\n= r \\cdot \\frac{1-p}{p^2}\n= \\frac{r(1-p)}{p^2}.\n\\]\n\nPortanto, para \\(X \\sim \\text{Binomial Negativa}(r,p)\\) (número de falhas até o \\(r\\)-ésimo sucesso), temos:\n\\[\nE(X) = \\frac{r(1-p)}{p},\n\\qquad\n\\mathrm{Var}(X) = \\frac{r(1-p)}{p^2}.\n\\]\n\n\n\nExemplo: Um pesquisador precisa de 4 pessoas que aceitem responder um questionário. Cada tentativa tem probabilidade \\(p=0{,}25\\) de sucesso. Seja \\(Y\\) = número de recusas até o 4º sucesso.\n\nCalcule \\(P(Y=6)\\).\n\nCalcule \\(E[Y]\\).\n\nSolução:\n\n\\[\nP(Y=6)=\\binom{9}{6}(0{,}75)^6 (0{,}25)^4 \\approx 0{,}050\n\\]\n\\[\nE[Y]=\\frac{4(1-0{,}25)}{0{,}25}=12\n\\]",
    "crumbs": [
      "Revisão Probabilidade I"
    ]
  },
  {
    "objectID": "revisao.html#hipergeométrica-nkn",
    "href": "revisao.html#hipergeométrica-nkn",
    "title": "Revisão Probabilidade I",
    "section": "3.5 Hipergeométrica \\((N,K,n)\\)",
    "text": "3.5 Hipergeométrica \\((N,K,n)\\)\nAmostragem sem reposição. \\(N\\) total, \\(K\\) sucessos na população, amostra \\(n\\).\n\n\n\n\n\n\n\nItem\nExpressão\n\n\n\n\nSuporte\n\\(k=\\max(0,n-(N-K)),\\dots,\\min(n,K)\\)\n\n\n\\(p(k)\\)\n\\(\\dfrac{\\binom{K}{k}\\binom{N-K}{n-k}}{\\binom{N}{n}}\\)\n\n\n\\(E[X]\\)\n\\(n\\,\\dfrac{K}{N}\\)\n\n\n\\(Var(X)\\)\n\\(n\\,\\dfrac{K}{N}\\!\\left(1-\\dfrac{K}{N}\\right)\\!\\dfrac{N-n}{N-1}\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMostrar demonstração\n\n\n\n\n\nConsidere:\n\nUma população finita com \\(N\\) elementos.\n\\(K\\) desses \\(N\\) elementos são “sucessos” (por exemplo, peças defeituosas).\nOs demais \\(N-K\\) são “fracassos”.\nRetiramos uma amostra sem reposição de tamanho \\(n\\).\n\nSeja \\(X\\) o número de sucessos na amostra. Dizemos que \\[\nX \\sim \\text{Hipergeométrica}(N, K, n).\n\\]\nA função de probabilidade é \\[\nP(X = k)\n= \\frac{\\binom{K}{k}\\binom{N-K}{n-k}}{\\binom{N}{n}},\n\\quad k = 0,1,\\dots,\\min(K,n).\n\\]\nQueremos demonstrar que: \\[\nE(X) = n \\frac{K}{N},\n\\qquad\n\\mathrm{Var}(X) = n\\frac{K}{N}\\left(1-\\frac{K}{N}\\right)\\frac{N-n}{N-1}.\n\\]\nEm vez de somar diretamente a f.p., vamos usar uma abordagem com variáveis indicadoras.\n\nRepresentando \\(X\\) como soma de indicadores\nImagine que a amostra de tamanho \\(n\\) é extraída em ordem: 1ª retirada, 2ª retirada, …, \\(n\\)-ésima retirada.\nDefina, para cada \\(i=1,\\dots,n\\): \\[\nY_i =\n\\begin{cases}\n1, & \\text{se o $i$-ésimo elemento sorteado é sucesso;}\\\\[4pt]\n0, & \\text{caso contrário.}\n\\end{cases}\n\\]\nEntão o número total de sucessos na amostra é \\[\nX = Y_1 + Y_2 + \\cdots + Y_n.\n\\]\nIsso é intuitivo: cada \\(Y_i\\) indica se houve sucesso naquela retirada, e a soma conta quantos sucessos houve ao todo.\n\nEsperança de \\(X\\) via linearidade\nUsando linearidade da esperança: \\[\nE(X) = E(Y_1 + Y_2 + \\cdots + Y_n)\n     = E(Y_1) + E(Y_2) + \\cdots + E(Y_n).\n\\]\nComo a população é homogênea e a amostragem é simétrica, todas as retiradas têm a mesma probabilidade de ser sucesso. Ou seja: \\[\nE(Y_1)=E(Y_2)=\\cdots=E(Y_n).\n\\]\nBasta, então, calcular \\(E(Y_1)\\).\n\nCálculo de \\(E(Y_i)\\)\nPor definição, \\[\nE(Y_i) = P(Y_i=1).\n\\]\nMas \\(Y_i=1\\) significa que, na \\(i\\)-ésima retirada, escolhemos um elemento “sucesso”.\nComo as retiradas são todas igualmente prováveis (sem viés) e a população tem \\(K\\) sucessos em \\(N\\) elementos, temos: \\[\nP(Y_i=1) = \\frac{K}{N},\n\\quad \\text{para qualquer } i.\n\\]\nLogo, \\[\nE(Y_i) = \\frac{K}{N}.\n\\]\nPortanto, \\[\nE(X)\n= \\sum_{i=1}^n E(Y_i)\n= n \\cdot \\frac{K}{N}.\n\\]\nIsso demonstra: \\[\nE(X) = n \\frac{K}{N}.\n\\]\n\nVariância de \\(X\\):\nQueremos agora \\[\n\\mathrm{Var}(X) = \\mathrm{Var}(Y_1+\\cdots+Y_n).\n\\]\nLembre que, em geral, para variáveis quaisquer: \\[\n\\mathrm{Var}\\left(\\sum_{i=1}^n Y_i\\right)\n= \\sum_{i=1}^n \\mathrm{Var}(Y_i) + 2\\sum_{1\\le i&lt;j\\le n} \\mathrm{Cov}(Y_i, Y_j).\n\\]\nAs \\(Y_i\\) não são independentes (pois a amostragem é sem reposição), então precisamos levar em conta as covariâncias.\nPela simetria do problema:\n\nTodas as variâncias \\(\\mathrm{Var}(Y_i)\\) são iguais.\nTodas as covariâncias \\(\\mathrm{Cov}(Y_i,Y_j)\\) com \\(i \\ne j\\) são iguais.\n\nEntão podemos escrever: \\[\n\\mathrm{Var}(X)\n= n\\,\\mathrm{Var}(Y_1) + 2\\binom{n}{2}\\mathrm{Cov}(Y_1,Y_2)\n= n\\,\\mathrm{Var}(Y_1) + n(n-1)\\,\\mathrm{Cov}(Y_1,Y_2).\n\\]\nAssim, precisamos calcular:\n\n\\(\\mathrm{Var}(Y_1)\\)\n\n\\(\\mathrm{Cov}(Y_1,Y_2)\\)\n\n\nCálculo de \\(\\mathrm{Var}(Y_1)\\)\nComo \\(Y_1\\) é uma variável indicadora (0 ou 1) com \\[\nP(Y_1=1) = \\frac{K}{N},\n\\quad\nP(Y_1=0) = 1 - \\frac{K}{N},\n\\] temos: \\[\nE(Y_1) = \\frac{K}{N},\\qquad\nE(Y_1^2) = E(Y_1) = \\frac{K}{N}\n\\] (pois \\(Y_1^2 = Y_1\\) quando \\(Y_1 \\in \\{0,1\\}\\)).\nLogo: \\[\n\\mathrm{Var}(Y_1) = E(Y_1^2) - [E(Y_1)]^2\n= \\frac{K}{N} - \\left(\\frac{K}{N}\\right)^2\n= \\frac{K}{N}\\left(1-\\frac{K}{N}\\right)\n= \\frac{K}{N}\\cdot\\frac{N-K}{N}\n= \\frac{K(N-K)}{N^2}.\n\\]\n\nCálculo de \\(\\mathrm{Cov}(Y_1,Y_2)\\)\nPor definição: \\[\n\\mathrm{Cov}(Y_1,Y_2)\n= E(Y_1Y_2) - E(Y_1)E(Y_2).\n\\]\nJá sabemos que \\[\nE(Y_1) = E(Y_2) = \\frac{K}{N}.\n\\]\nEntão precisamos de \\(E(Y_1Y_2)\\), que é \\[\nE(Y_1Y_2) = P(Y_1=1 \\text{ e } Y_2=1),\n\\] pois \\(Y_1Y_2=1\\) somente quando ambos são 1.\n\nCálculo de \\(P(Y_1=1, Y_2=1)\\)\nInterprete o sorteio em duas etapas, sem reposição:\n\nPrimeira retirada é sucesso: probabilidade \\(K/N\\).\nDada uma primeira retirada de sucesso, restam:\n\n\\(K-1\\) sucessos\nem um total de \\(N-1\\) elementos.\n\nEntão a probabilidade de a segunda retirada também ser sucesso é: \\[\n\\frac{K-1}{N-1}.\n\\]\n\nLogo: \\[\nP(Y_1=1, Y_2=1)\n= \\frac{K}{N} \\cdot \\frac{K-1}{N-1}\n= \\frac{K(K-1)}{N(N-1)}.\n\\]\nPortanto: \\[\nE(Y_1Y_2) = \\frac{K(K-1)}{N(N-1)}.\n\\]\n\nCovariância\nAgora: \\[\n\\mathrm{Cov}(Y_1,Y_2)\n= E(Y_1Y_2) - E(Y_1)E(Y_2)\n= \\frac{K(K-1)}{N(N-1)} - \\left(\\frac{K}{N}\\right)^2.\n\\]\nVamos colocar os termos no mesmo denominador. Note que \\[\n\\left(\\frac{K}{N}\\right)^2 = \\frac{K^2}{N^2}\n= \\frac{K^2(N-1)}{N^2(N-1)}.\n\\]\nE \\[\n\\frac{K(K-1)}{N(N-1)}\n= \\frac{K(K-1)N}{N^2(N-1)}.\n\\]\nEntão: \\[\n\\mathrm{Cov}(Y_1,Y_2)\n= \\frac{K(K-1)N - K^2(N-1)}{N^2(N-1)}.\n\\]\nSimplificando o numerador: \\[\nK(K-1)N - K^2(N-1)\n= K[ N(K-1) - K(N-1) ].\n\\]\nDentro dos colchetes: \\[\nN(K-1) - K(N-1)\n= (NK - N) - (KN - K)\n= NK - N - KN + K\n= -N + K\n= K - N.\n\\]\nLogo o numerador é: \\[\nK(K-N) = -K(N-K).\n\\]\nPortanto: \\[\n\\mathrm{Cov}(Y_1,Y_2)\n= \\frac{-K(N-K)}{N^2(N-1)}.\n\\]\nIsso mostra que a covariância é negativa: faz sentido, pois sem reposição, ao observar um sucesso na primeira retirada, fica ligeiramente menos provável ver outro sucesso na segunda.\n\nVariância de \\(X\\)\nRelembrando: \\[\n\\mathrm{Var}(X)=n\\,\\mathrm{Var}(Y_1) + n(n-1)\\,\\mathrm{Cov}(Y_1,Y_2).\n\\]\nSubstituímos os valores encontrados:\n\n\\(\\mathrm{Var}(Y_1) = \\dfrac{K(N-K)}{N^2}\\)\n\\(\\mathrm{Cov}(Y_1,Y_2) = -\\dfrac{K(N-K)}{N^2(N-1)}\\)\n\nEntão: \\[\n\\mathrm{Var}(X)\n= n \\cdot \\frac{K(N-K)}{N^2}\n+ n(n-1)\\cdot\\left(-\\frac{K(N-K)}{N^2(N-1)}\\right).\n\\]\nColocamos o fator comum \\(\\dfrac{K(N-K)}{N^2}\\) em evidência: \\[\n\\mathrm{Var}(X)\n= \\frac{K(N-K)}{N^2}\n\\left[\nn - \\frac{n(n-1)}{N-1}\n\\right].\n\\]\nAgora vamos simplificar o colchete: \\[\nn - \\frac{n(n-1)}{N-1}\n= n\\left[1 - \\frac{n-1}{N-1}\\right]\n= n\\left[\\frac{N-1 - (n-1)}{N-1}\\right]\n= n\\left[\\frac{N-n}{N-1}\\right].\n\\]\nPortanto: \\[\n\\mathrm{Var}(X)\n= \\frac{K(N-K)}{N^2}\\cdot n\\frac{N-n}{N-1}.\n\\]\nPodemos reescrever \\(K(N-K)/N^2\\) como \\[\n\\frac{K}{N}\\left(1-\\frac{K}{N}\\right).\n\\]\nAssim: \\[\n\\mathrm{Var}(X)\n= n \\frac{K}{N}\\left(1-\\frac{K}{N}\\right)\\frac{N-n}{N-1}.\n\\]\n\nAssim, para \\(X \\sim \\text{Hipergeométrica}(N,K,n)\\):\n\nEsperança: \\[\nE(X) = n \\frac{K}{N}.\n\\]\nVariância: \\[\n\\mathrm{Var}(X) = n \\frac{K}{N}\\left(1-\\frac{K}{N}\\right)\\frac{N-n}{N-1}.\n\\]\n\n\n\n\nExemplo: Um lote tem \\(N=80\\) peças, sendo \\(K=10\\) defeituosas. Retira-se uma amostra de \\(n=12\\) peças. Seja \\(X\\) = número de defeituosas na amostra.\n\nCalcule \\(P(X=2)\\).\n\nCalcule \\(E[X]\\).\n\nSolução:\n\n\\[\nP(X=2)=\\frac{\\binom{10}{2}\\binom{70}{10}}{\\binom{80}{12}}\n\\]\n\nResultado aproximado: 0,283\n\n\\[\nE[X]=12 \\cdot \\frac{10}{80} = 1{,}5\n\\]",
    "crumbs": [
      "Revisão Probabilidade I"
    ]
  },
  {
    "objectID": "revisao.html#poisson-lambda",
    "href": "revisao.html#poisson-lambda",
    "title": "Revisão Probabilidade I",
    "section": "3.6 Poisson \\((\\lambda)\\)",
    "text": "3.6 Poisson \\((\\lambda)\\)\nContagem de eventos raros em intervalo fixo.\n\n\n\nItem\nExpressão\n\n\n\n\nSuporte\n\\(k=0,1,2,\\dots\\)\n\n\n\\(p(k)\\)\n\\(e^{-\\lambda}\\lambda^k/k!\\)\n\n\n\\(F(k)\\)\n\\(\\sum_{j=0}^{k}e^{-\\lambda}\\lambda^j/j!\\)\n\n\n\\(E[X]\\)\n\\(\\lambda\\)\n\n\n\\(Var(X)\\)\n\\(\\lambda\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMostrar demonstração\n\n\n\n\n\nConsidere \\(X \\sim \\text{Poisson}(\\lambda)\\), com \\(\\lambda &gt; 0\\). A função de probabilidade é: \\[\nP(X=k) = e^{-\\lambda}\\frac{\\lambda^k}{k!},\\qquad k=0,1,2,\\dots\n\\]\nNosso objetivo é demonstrar: \\[\nE(X)=\\lambda,\n\\qquad\n\\mathrm{Var}(X)=\\lambda.\n\\]\nUsaremos somente as definições: \\[\nE(X)=\\sum_{k=0}^{\\infty} k\\,P(X=k), \\qquad\n\\mathrm{Var}(X)=E(X^2)-[E(X)]^2.\n\\]\n\nCálculo de \\(E(X)\\)\nPela definição de esperança: \\[\nE(X)=\\sum_{k=0}^{\\infty} k\\,P(X=k)\n    =\\sum_{k=0}^{\\infty} k\\,e^{-\\lambda}\\frac{\\lambda^k}{k!}.\n\\]\nO termo \\(k=0\\) é nulo, então: \\[\nE(X)=e^{-\\lambda}\\sum_{k=1}^{\\infty} k\\frac{\\lambda^k}{k!}.\n\\]\nUsamos a identidade: \\[\nk\\frac{\\lambda^k}{k!}=\\frac{\\lambda^k}{(k-1)!}.\n\\]\nLogo: \\[\nE(X)=e^{-\\lambda}\\sum_{k=1}^{\\infty} \\frac{\\lambda^k}{(k-1)!}.\n\\]\nAgora fazemos a mudança de variável \\(j=k-1\\):\n\nquando \\(k=1\\), \\(j=0\\)\n\nquando \\(k\\to\\infty\\), \\(j\\to\\infty\\)\n\nEntão: \\[\nE(X)=e^{-\\lambda}\\sum_{j=0}^{\\infty} \\frac{\\lambda^{j+1}}{j!}\n= e^{-\\lambda}\\lambda \\sum_{j=0}^{\\infty} \\frac{\\lambda^{j}}{j!}.\n\\]\nMas: \\[\n\\sum_{j=0}^{\\infty} \\frac{\\lambda^{j}}{j!} = e^{\\lambda}.\n\\]\nPortanto: \\[\nE(X)=e^{-\\lambda}\\lambda e^{\\lambda} = \\lambda.\n\\]\nAssim demonstramos: \\[\nE(X)=\\lambda.\n\\]\n\nCálculo de \\(E(X^2)\\)\nAgora usamos a definição: \\[\nE(X^2)=\\sum_{k=0}^{\\infty} k^2 P(X=k)\n      =e^{-\\lambda}\\sum_{k=0}^{\\infty} k^2 \\frac{\\lambda^k}{k!}.\n\\]\nTruque clássico: escrever \\[\nk^2 = k(k-1) + k.\n\\]\nEntão: \\[\nE(X^2)\n= e^{-\\lambda}\\sum_{k=0}^{\\infty} k(k-1)\\frac{\\lambda^k}{k!}\n+ e^{-\\lambda}\\sum_{k=0}^{\\infty} k\\frac{\\lambda^k}{k!}.\n\\]\nChamemos:\n\nPrimeiro somatório:\n\\[A = e^{-\\lambda}\\sum_{k=0}^{\\infty} k(k-1)\\frac{\\lambda^k}{k!}\\]\nSegundo somatório:\n\\[B = e^{-\\lambda}\\sum_{k=0}^{\\infty} k\\frac{\\lambda^k}{k!}\\]\n\nMas já vimos antes que \\(B = E(X) = \\lambda\\).\nVamos calcular \\(A\\).\n\nCálculo do termo \\(A\\)\nNote que: \\[\nk(k-1)\\frac{\\lambda^k}{k!}\n= \\frac{\\lambda^k}{(k-2)!}.\n\\]\nPortanto: \\[\nA = e^{-\\lambda}\\sum_{k=2}^{\\infty}\\frac{\\lambda^k}{(k-2)!}.\n\\]\nFazemos a mudança de variável \\(j=k-2\\):\n\nquando \\(k=2\\), \\(j=0\\)\nquando \\(k\\to\\infty\\), \\(j\\to\\infty\\)\n\nEntão: \\[\nA = e^{-\\lambda}\\sum_{j=0}^{\\infty}\\frac{\\lambda^{j+2}}{j!}\n= e^{-\\lambda}\\lambda^2 \\sum_{j=0}^{\\infty}\\frac{\\lambda^{j}}{j!}.\n\\]\nA soma é novamente \\(e^\\lambda\\), logo: \\[\nA = e^{-\\lambda}\\lambda^2 e^\\lambda = \\lambda^2.\n\\]\nAgora juntamos:\n\n\\(A = \\lambda^2\\)\n\\(B = \\lambda\\)\n\nPortanto: \\[\nE(X^2) = A + B = \\lambda^2 + \\lambda.\n\\]\n\nVariância\nUsamos: \\[\n\\mathrm{Var}(X)=E(X^2)-[E(X)]^2.\n\\]\nSubstituindo:\n\n\\(E(X)=\\lambda\\)\n\\(E(X^2)=\\lambda^2 + \\lambda\\)\n\nTemos: \\[\n\\mathrm{Var}(X)\n= (\\lambda^2+\\lambda) - \\lambda^2\n= \\lambda.\n\\]\n\nLogo, para \\(X \\sim \\text{Poisson}(\\lambda)\\):\n\\[\nE(X)=\\lambda,\n\\qquad\n\\mathrm{Var}(X)=\\lambda.\n\\]\n\n\n\nExemplo: A taxa média de chamadas em um call center é \\(\\lambda=12\\) chamadas por hora. Seja \\(N\\) = número de chamadas.\n\nCalcule \\(P(N=10)\\).\n\nCalcule \\(P(N\\ge 15)\\).\n\nCalcule \\(E[N]\\) e \\(Var(N)\\).\n\nSolução:\n\n\\[\nP(N=10)=e^{-12}\\frac{12^{10}}{10!}\\approx 0{,}104\n\\]\n\\[\nP(N\\ge 15)=1-P(N\\le 14)\\approx 0{,}263\n\\]\n\\[\nE[N]=12,\\qquad Var(N)=12\n\\]",
    "crumbs": [
      "Revisão Probabilidade I"
    ]
  },
  {
    "objectID": "revisao.html#cálculo-de-ex-3",
    "href": "revisao.html#cálculo-de-ex-3",
    "title": "Revisão Probabilidade I",
    "section": "4.2 1. Cálculo de \\(E(X)\\)",
    "text": "4.2 1. Cálculo de \\(E(X)\\)\nPela definição: \\[\nE(X) = \\int_{-\\infty}^{\\infty} x f(x)\\,dx.\n\\]\nComo \\(f(x)=0\\) fora do intervalo \\([a,b]\\), temos: \\[\nE(X) = \\int_a^b x \\cdot \\frac{1}{b-a}\\,dx\n     = \\frac{1}{b-a} \\int_a^b x\\,dx.\n\\]\nCalculamos a integral: \\[\n\\int_a^b x\\,dx = \\left[\\frac{x^2}{2}\\right]_a^b\n= \\frac{b^2}{2} - \\frac{a^2}{2}\n= \\frac{b^2 - a^2}{2}.\n\\]\nLogo: \\[\nE(X) = \\frac{1}{b-a} \\cdot \\frac{b^2 - a^2}{2}.\n\\]\nFatoramos \\(b^2 - a^2\\): \\[\nb^2 - a^2 = (b-a)(b+a).\n\\]\nSubstituindo: \\[\nE(X) = \\frac{1}{b-a} \\cdot \\frac{(b-a)(b+a)}{2}\n     = \\frac{b+a}{2}.\n\\]\nPortanto: \\[\nE(X) = \\frac{a+b}{2}.\n\\]",
    "crumbs": [
      "Revisão Probabilidade I"
    ]
  },
  {
    "objectID": "revisao.html#cálculo-de-ex2-2",
    "href": "revisao.html#cálculo-de-ex2-2",
    "title": "Revisão Probabilidade I",
    "section": "4.8 2. Cálculo de \\(E(X^2)\\)",
    "text": "4.8 2. Cálculo de \\(E(X^2)\\)\nPela definição: \\[\nE(X^2)=\\int_0^\\infty x^2 \\lambda e^{-\\lambda x}\\,dx.\n\\]\nVamos aplicar integração por partes duas vezes.\n\n\n4.8.1 Primeira integração por partes\nEscolha: - (u = x^2) → (du = 2x,dx) - (dv = e^{-x}dx) → (v = -e^{-x})\nEntão: \\[\nE(X^2)\n= \\left[-x^2 e^{-\\lambda x}\\right]_0^\\infty\n+ \\int_0^\\infty 2x e^{-\\lambda x} \\, dx.\n\\]\nO termo de fronteira é zero, pelo mesmo argumento anterior.\nAssim: \\[\nE(X^2)= 2\\int_0^\\infty x e^{-\\lambda x}\\,dx.\n\\]\n\n\n\n4.8.2 Segunda integração por partes\nAgora resolvemos: \\[\n\\int_0^\\infty x e^{-\\lambda x}\\,dx.\n\\]\nEscolha: - (u = x) → (du = dx) - (dv = e^{-x}dx) → (v = -e^{-x})\nEntão: \\[\n\\int_0^\\infty x e^{-\\lambda x}\\,dx\n= \\left[-\\frac{x}{\\lambda} e^{-\\lambda x}\\right]_0^\\infty\n+ \\int_0^\\infty \\frac{1}{\\lambda} e^{-\\lambda x}\\,dx.\n\\]\nO termo de fronteira novamente é zero.\nResta: \\[\n\\int_0^\\infty x e^{-\\lambda x}\\,dx\n= \\frac{1}{\\lambda} \\int_0^\\infty e^{-\\lambda x}\\,dx\n= \\frac{1}{\\lambda}\\cdot\\frac{1}{\\lambda}\n= \\frac{1}{\\lambda^2}.\n\\]\n\n\n\n4.8.3 Conclusão do cálculo de \\(E(X^2)\\)\nVoltando ao ponto onde paramos: \\[\nE(X^2)= 2\\left(\\frac{1}{\\lambda^2}\\right)\n= \\frac{2}{\\lambda^2}.\n\\]",
    "crumbs": [
      "Revisão Probabilidade I"
    ]
  },
  {
    "objectID": "revisao.html#variância",
    "href": "revisao.html#variância",
    "title": "Revisão Probabilidade I",
    "section": "3.9 3. Variância",
    "text": "3.9 3. Variância\nUsamos: \\[\n\\mathrm{Var}(X)=E(X^2)-[E(X)]^2.\n\\]\nSubstituindo:\n\n\\(E(X)=\\lambda\\)\n\\(E(X^2)=\\lambda^2 + \\lambda\\)\n\nTemos: \\[\n\\mathrm{Var}(X)\n= (\\lambda^2+\\lambda) - \\lambda^2\n= \\lambda.\n\\]",
    "crumbs": [
      "Revisão Probabilidade I"
    ]
  },
  {
    "objectID": "revisao.html#resultado-final-4",
    "href": "revisao.html#resultado-final-4",
    "title": "Revisão Probabilidade I",
    "section": "4.14 3. Resultado final",
    "text": "4.14 3. Resultado final\nPara uma variável aleatória normal geral \\(X \\sim N(\\mu,\\sigma^2)\\), demonstramos que:\n\\[\nE(X) = \\mu,\n\\qquad\n\\mathrm{Var}(X) = \\sigma^2.\n\\]",
    "crumbs": [
      "Revisão Probabilidade I"
    ]
  },
  {
    "objectID": "revisao.html#uniforme-ab",
    "href": "revisao.html#uniforme-ab",
    "title": "Revisão Probabilidade I",
    "section": "4.1 Uniforme \\((a,b)\\)",
    "text": "4.1 Uniforme \\((a,b)\\)\n\n\n\nItem\nExpressão\n\n\n\n\nSuporte\n\\(a\\le x\\le b\\)\n\n\n\\(f(x)\\)\n\\(1/(b-a)\\)\n\n\n\\(F(x)\\)\n\\((x-a)/(b-a)\\) para \\(a\\le x\\le b\\)\n\n\n\\(E[X]\\)\n\\((a+b)/2\\)\n\n\n\\(Var(X)\\)\n\\((b-a)^2/12\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMostrar demonstração\n\n\n\n\n\nConsidere \\(X \\sim \\text{Uniforme}(a,b)\\), com \\(a &lt; b\\).\nA função densidade é: \\[\nf(x) =\n\\begin{cases}\n\\dfrac{1}{b-a}, & a \\le x \\le b,\\\\[6pt]\n0, & \\text{caso contrário.}\n\\end{cases}\n\\]\nNosso objetivo é demonstrar que \\[\nE(X) = \\frac{a+b}{2},\n\\qquad\n\\mathrm{Var}(X) = \\frac{(b-a)^2}{12},\n\\] usando apenas as definições: \\[\nE(X)=\\int_{-\\infty}^{\\infty} x f(x)\\,dx,\n\\qquad\nE(X^2)=\\int_{-\\infty}^{\\infty} x^2 f(x)\\,dx,\n\\qquad\n\\mathrm{Var}(X)=E(X^2)-[E(X)]^2.\n\\]\n\nCálculo de \\(E(X)\\)\nPela definição: \\[\nE(X) = \\int_{-\\infty}^{\\infty} x f(x)\\,dx.\n\\]\nComo \\(f(x)=0\\) fora do intervalo \\([a,b]\\), temos: \\[\nE(X) = \\int_a^b x \\cdot \\frac{1}{b-a}\\,dx\n     = \\frac{1}{b-a} \\int_a^b x\\,dx.\n\\]\nCalculamos a integral: \\[\n\\int_a^b x\\,dx = \\left[\\frac{x^2}{2}\\right]_a^b\n= \\frac{b^2}{2} - \\frac{a^2}{2}\n= \\frac{b^2 - a^2}{2}.\n\\]\nLogo: \\[\nE(X) = \\frac{1}{b-a} \\cdot \\frac{b^2 - a^2}{2}.\n\\]\nFatoramos \\(b^2 - a^2\\): \\[\nb^2 - a^2 = (b-a)(b+a).\n\\]\nSubstituindo: \\[\nE(X) = \\frac{1}{b-a} \\cdot \\frac{(b-a)(b+a)}{2}\n     = \\frac{b+a}{2}.\n\\]\nPortanto: \\[\nE(X) = \\frac{a+b}{2}.\n\\]\n\nCálculo de \\(E(X^2)\\)\nPela definição: \\[\nE(X^2) = \\int_{-\\infty}^{\\infty} x^2 f(x)\\,dx\n       = \\int_a^b x^2 \\cdot \\frac{1}{b-a}\\,dx\n       = \\frac{1}{b-a}\\int_a^b x^2\\,dx.\n\\]\nCalculamos a integral: \\[\n\\int_a^b x^2\\,dx = \\left[\\frac{x^3}{3}\\right]_a^b\n= \\frac{b^3}{3} - \\frac{a^3}{3}\n= \\frac{b^3 - a^3}{3}.\n\\]\nEntão: \\[\nE(X^2) = \\frac{1}{b-a} \\cdot \\frac{b^3 - a^3}{3}\n       = \\frac{b^3 - a^3}{3(b-a)}.\n\\]\nAgora usamos a fatoração: \\[\nb^3 - a^3 = (b-a)(b^2 + ab + a^2).\n\\]\nSubstituindo: \\[\nE(X^2) = \\frac{(b-a)(b^2 + ab + a^2)}{3(b-a)}\n       = \\frac{b^2 + ab + a^2}{3}.\n\\]\nPortanto: \\[\nE(X^2) = \\frac{a^2 + ab + b^2}{3}.\n\\]\n\nCálculo de \\(\\mathrm{Var}(X)\\)\nUsamos: \\[\n\\mathrm{Var}(X) = E(X^2) - [E(X)]^2.\n\\]\nJá obtivemos:\n\n\\(E(X) = \\dfrac{a+b}{2}\\)\n\n\\(E(X^2) = \\dfrac{a^2 + ab + b^2}{3}\\)\n\nEntão: \\[\n\\mathrm{Var}(X)\n= \\frac{a^2 + ab + b^2}{3} - \\left(\\frac{a+b}{2}\\right)^2.\n\\]\nPrimeiro, calculemos o quadrado: \\[\n\\left(\\frac{a+b}{2}\\right)^2\n= \\frac{(a+b)^2}{4}\n= \\frac{a^2 + 2ab + b^2}{4}.\n\\]\nAgora escrevemos a variância com denominador comum. O denominador comum entre 3 e 4 é 12:\n\nPrimeiro termo: \\[\n\\frac{a^2 + ab + b^2}{3}\n= \\frac{4(a^2 + ab + b^2)}{12}.\n\\]\nSegundo termo: \\[\n\\frac{a^2 + 2ab + b^2}{4}\n= \\frac{3(a^2 + 2ab + b^2)}{12}.\n\\]\n\nEntão: \\[\n\\mathrm{Var}(X)\n= \\frac{4(a^2 + ab + b^2)}{12} - \\frac{3(a^2 + 2ab + b^2)}{12}.\n\\]\nSubtraindo os numeradores: \\[\n4(a^2 + ab + b^2) - 3(a^2 + 2ab + b^2)\n= 4a^2 + 4ab + 4b^2 - 3a^2 - 6ab - 3b^2\n= (4a^2 - 3a^2) + (4ab - 6ab) + (4b^2 - 3b^2)\n= a^2 - 2ab + b^2.\n\\]\nLogo: \\[\n\\mathrm{Var}(X)\n= \\frac{a^2 - 2ab + b^2}{12}\n= \\frac{(b-a)^2}{12},\n\\] pois \\[\na^2 - 2ab + b^2 = (b-a)^2.\n\\]\n\nAssim, para \\(X \\sim \\text{Uniforme}(a,b)\\), demonstramos que: \\[\nE(X) = \\frac{a+b}{2},\n\\qquad\n\\mathrm{Var}(X) = \\frac{(b-a)^2}{12}.\n\\]\n\n\n\nExemplo: O tempo de resposta de um servidor web varia uniformemente entre 50 ms e 90 ms, ou seja \\(T \\sim U(50, 90)\\).\n\nCalcule \\(P(60 &lt; T &lt; 80)\\).\n\nCalcule \\(E[T]\\) e \\(Var(T)\\).\n\nInterprete o valor esperado no contexto.\n\nSolução:\n\n\\[\nP(60&lt;T&lt;80)=\\frac{80-60}{90-50}=\\frac{20}{40}=0{,}5\n\\]\n\\[\nE[T]=\\frac{50+90}{2}=70\n\\]\n\n\\[\nVar(T)=\\frac{(90-50)^2}{12}=\\frac{1600}{12}\\approx 133{,}33\n\\]\n\nO tempo médio de resposta é 70 ms.",
    "crumbs": [
      "Revisão Probabilidade I"
    ]
  },
  {
    "objectID": "revisao.html#cálculo-de-ex-4",
    "href": "revisao.html#cálculo-de-ex-4",
    "title": "Revisão Probabilidade I",
    "section": "4.7 1. Cálculo de \\(E(X)\\)",
    "text": "4.7 1. Cálculo de \\(E(X)\\)\nPela definição: \\[\nE(X) = \\int_0^\\infty x\\lambda e^{-\\lambda x}\\,dx.\n\\]\nPara resolver a integral, usamos integração por partes.\nEscolhemos: - (u = x) → (du = dx) - (dv = e^{-x}dx) → (v = -e^{-x})\nAplicando integração por partes: \\[\n\\int_0^\\infty x\\lambda e^{-\\lambda x}\\,dx\n= \\left[-x e^{-\\lambda x}\\right]_{0}^{\\infty}\n+ \\int_0^\\infty e^{-\\lambda x}\\,dx.\n\\]\nAgora avaliamos cada termo:\n\nTermo de fronteira:\n\nQuando (x): (x e^{-x} ) (exponencial domina linear).\nQuando (x=0): (0 e^{0} = 0).\n\nLogo: \\[\n\\left[-x e^{-\\lambda x}\\right]_{0}^{\\infty} = 0.\n\\]\nIntegral restante: \\[\n\\int_0^\\infty e^{-\\lambda x}\\,dx\n= \\left[-\\frac{1}{\\lambda}e^{-\\lambda x}\\right]_{0}^{\\infty}\n= \\frac{1}{\\lambda}.\n\\]\n\nPortanto: \\[\nE(X)=\\frac{1}{\\lambda}.\n\\]",
    "crumbs": [
      "Revisão Probabilidade I"
    ]
  },
  {
    "objectID": "revisao.html#cálculo-de-ex2-3",
    "href": "revisao.html#cálculo-de-ex2-3",
    "title": "Revisão Probabilidade I",
    "section": "4.8 2. Cálculo de \\(E(X^2)\\)",
    "text": "4.8 2. Cálculo de \\(E(X^2)\\)\nPela definição: \\[\nE(X^2)=\\int_0^\\infty x^2 \\lambda e^{-\\lambda x}\\,dx.\n\\]\nVamos aplicar integração por partes duas vezes.\n\n\n4.8.1 Primeira integração por partes\nEscolha: - (u = x^2) → (du = 2x,dx) - (dv = e^{-x}dx) → (v = -e^{-x})\nEntão: \\[\nE(X^2)\n= \\left[-x^2 e^{-\\lambda x}\\right]_0^\\infty\n+ \\int_0^\\infty 2x e^{-\\lambda x} \\, dx.\n\\]\nO termo de fronteira é zero, pelo mesmo argumento anterior.\nAssim: \\[\nE(X^2)= 2\\int_0^\\infty x e^{-\\lambda x}\\,dx.\n\\]\n\n\n\n4.8.2 Segunda integração por partes\nAgora resolvemos: \\[\n\\int_0^\\infty x e^{-\\lambda x}\\,dx.\n\\]\nEscolha: - (u = x) → (du = dx) - (dv = e^{-x}dx) → (v = -e^{-x})\nEntão: \\[\n\\int_0^\\infty x e^{-\\lambda x}\\,dx\n= \\left[-\\frac{x}{\\lambda} e^{-\\lambda x}\\right]_0^\\infty\n+ \\int_0^\\infty \\frac{1}{\\lambda} e^{-\\lambda x}\\,dx.\n\\]\nO termo de fronteira novamente é zero.\nResta: \\[\n\\int_0^\\infty x e^{-\\lambda x}\\,dx\n= \\frac{1}{\\lambda} \\int_0^\\infty e^{-\\lambda x}\\,dx\n= \\frac{1}{\\lambda}\\cdot\\frac{1}{\\lambda}\n= \\frac{1}{\\lambda^2}.\n\\]\n\n\n\n4.8.3 Conclusão do cálculo de \\(E(X^2)\\)\nVoltando ao ponto onde paramos: \\[\nE(X^2)= 2\\left(\\frac{1}{\\lambda^2}\\right)\n= \\frac{2}{\\lambda^2}.\n\\]",
    "crumbs": [
      "Revisão Probabilidade I"
    ]
  },
  {
    "objectID": "revisao.html#cálculo-de-mathrmvarx-3",
    "href": "revisao.html#cálculo-de-mathrmvarx-3",
    "title": "Revisão Probabilidade I",
    "section": "4.4 3. Cálculo de \\(\\mathrm{Var}(X)\\)",
    "text": "4.4 3. Cálculo de \\(\\mathrm{Var}(X)\\)\nUsamos: \\[\n\\mathrm{Var}(X) = E(X^2) - [E(X)]^2.\n\\]\nJá obtivemos:\n\n\\(E(X) = \\dfrac{a+b}{2}\\)\n\n\\(E(X^2) = \\dfrac{a^2 + ab + b^2}{3}\\)\n\nEntão: \\[\n\\mathrm{Var}(X)\n= \\frac{a^2 + ab + b^2}{3} - \\left(\\frac{a+b}{2}\\right)^2.\n\\]\nPrimeiro, calculemos o quadrado: \\[\n\\left(\\frac{a+b}{2}\\right)^2\n= \\frac{(a+b)^2}{4}\n= \\frac{a^2 + 2ab + b^2}{4}.\n\\]\nAgora escrevemos a variância com denominador comum.\nO denominador comum entre 3 e 4 é 12:\n\nPrimeiro termo: \\[\n\\frac{a^2 + ab + b^2}{3}\n= \\frac{4(a^2 + ab + b^2)}{12}.\n\\]\nSegundo termo: \\[\n\\frac{a^2 + 2ab + b^2}{4}\n= \\frac{3(a^2 + 2ab + b^2)}{12}.\n\\]\n\nEntão: \\[\n\\mathrm{Var}(X)\n= \\frac{4(a^2 + ab + b^2)}{12} - \\frac{3(a^2 + 2ab + b^2)}{12}.\n\\]\nSubtraindo os numeradores: \\[\n4(a^2 + ab + b^2) - 3(a^2 + 2ab + b^2)\n= 4a^2 + 4ab + 4b^2 - 3a^2 - 6ab - 3b^2\n= (4a^2 - 3a^2) + (4ab - 6ab) + (4b^2 - 3b^2)\n= a^2 - 2ab + b^2.\n\\]\nLogo: \\[\n\\mathrm{Var}(X)\n= \\frac{a^2 - 2ab + b^2}{12}\n= \\frac{(b-a)^2}{12},\n\\] pois \\[\na^2 - 2ab + b^2 = (b-a)^2.\n\\]",
    "crumbs": [
      "Revisão Probabilidade I"
    ]
  },
  {
    "objectID": "revisao.html#resultado-final-5",
    "href": "revisao.html#resultado-final-5",
    "title": "Revisão Probabilidade I",
    "section": "4.10 4. Resultado final",
    "text": "4.10 4. Resultado final\nPara \\(X \\sim \\text{Exponencial}(\\lambda)\\):\n\\[\nE(X)=\\frac{1}{\\lambda},\n\\qquad\n\\mathrm{Var}(X)=\\frac{1}{\\lambda^2}.\n\\]",
    "crumbs": [
      "Revisão Probabilidade I"
    ]
  },
  {
    "objectID": "revisao.html#exponencial-lambda-parametrização-por-taxa",
    "href": "revisao.html#exponencial-lambda-parametrização-por-taxa",
    "title": "Revisão Probabilidade I",
    "section": "4.2 Exponencial \\((\\lambda)\\) (parametrização por taxa)",
    "text": "4.2 Exponencial \\((\\lambda)\\) (parametrização por taxa)\n\n\n\nItem\nExpressão\n\n\n\n\nSuporte\n\\(x\\ge 0\\)\n\n\n\\(f(x)\\)\n\\(\\lambda e^{-\\lambda x}\\)\n\n\n\\(F(x)\\)\n\\(1-e^{-\\lambda x}\\)\n\n\n\\(E[X]\\)\n\\(1/\\lambda\\)\n\n\n\\(Var(X)\\)\n\\(1/\\lambda^2\\)\n\n\nPropriedade\nSem memória: \\(P(X&gt;s+t\\mid X&gt;t)=P(X&gt;s)\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMostrar demonstração\n\n\n\n\n\nConsidere \\(X \\sim \\text{Exponencial}(\\lambda)\\), com \\(\\lambda &gt; 0\\).\nA função densidade é \\[\nf(x) =\n\\begin{cases}\n\\lambda e^{-\\lambda x}, & x \\ge 0, \\\\[4pt]\n0, & x &lt; 0.\n\\end{cases}\n\\]\nNosso objetivo é demonstrar: \\[\nE(X) = \\frac{1}{\\lambda},\n\\qquad\n\\mathrm{Var}(X) = \\frac{1}{\\lambda^2},\n\\] usando apenas as definições: \\[\nE(X) = \\int_0^\\infty x\\,\\lambda e^{-\\lambda x}\\,dx,\n\\qquad\nE(X^2) = \\int_0^\\infty x^2\\,\\lambda e^{-\\lambda x}\\,dx.\n\\]\n\n\n4.3 1. Cálculo de \\(E(X)\\)\nPela definição: \\[\nE(X) = \\int_0^\\infty x\\lambda e^{-\\lambda x}\\,dx.\n\\]\nPara resolver a integral, usamos integração por partes.\nEscolhemos: - (u = x) → (du = dx) - (dv = e^{-x}dx) → (v = -e^{-x})\nAplicando integração por partes: \\[\n\\int_0^\\infty x\\lambda e^{-\\lambda x}\\,dx\n= \\left[-x e^{-\\lambda x}\\right]_{0}^{\\infty}\n+ \\int_0^\\infty e^{-\\lambda x}\\,dx.\n\\]\nAgora avaliamos cada termo:\n\nTermo de fronteira:\n\nQuando (x): (x e^{-x} ) (exponencial domina linear).\nQuando (x=0): (0 e^{0} = 0).\n\nLogo: \\[\n\\left[-x e^{-\\lambda x}\\right]_{0}^{\\infty} = 0.\n\\]\nIntegral restante: \\[\n\\int_0^\\infty e^{-\\lambda x}\\,dx\n= \\left[-\\frac{1}{\\lambda}e^{-\\lambda x}\\right]_{0}^{\\infty}\n= \\frac{1}{\\lambda}.\n\\]\n\nPortanto: \\[\nE(X)=\\frac{1}{\\lambda}.\n\\]\n\n\n\n4.4 2. Cálculo de \\(E(X^2)\\)\nPela definição: \\[\nE(X^2)=\\int_0^\\infty x^2 \\lambda e^{-\\lambda x}\\,dx.\n\\]\nVamos aplicar integração por partes duas vezes.\n\n\n4.4.1 Primeira integração por partes\nEscolha: - (u = x^2) → (du = 2x,dx) - (dv = e^{-x}dx) → (v = -e^{-x})\nEntão: \\[\nE(X^2)\n= \\left[-x^2 e^{-\\lambda x}\\right]_0^\\infty\n+ \\int_0^\\infty 2x e^{-\\lambda x} \\, dx.\n\\]\nO termo de fronteira é zero, pelo mesmo argumento anterior.\nAssim: \\[\nE(X^2)= 2\\int_0^\\infty x e^{-\\lambda x}\\,dx.\n\\]\n\n\n\n4.4.2 Segunda integração por partes\nAgora resolvemos: \\[\n\\int_0^\\infty x e^{-\\lambda x}\\,dx.\n\\]\nEscolha: - (u = x) → (du = dx) - (dv = e^{-x}dx) → (v = -e^{-x})\nEntão: \\[\n\\int_0^\\infty x e^{-\\lambda x}\\,dx\n= \\left[-\\frac{x}{\\lambda} e^{-\\lambda x}\\right]_0^\\infty\n+ \\int_0^\\infty \\frac{1}{\\lambda} e^{-\\lambda x}\\,dx.\n\\]\nO termo de fronteira novamente é zero.\nResta: \\[\n\\int_0^\\infty x e^{-\\lambda x}\\,dx\n= \\frac{1}{\\lambda} \\int_0^\\infty e^{-\\lambda x}\\,dx\n= \\frac{1}{\\lambda}\\cdot\\frac{1}{\\lambda}\n= \\frac{1}{\\lambda^2}.\n\\]\n\n\n\n4.4.3 Conclusão do cálculo de \\(E(X^2)\\)\nVoltando ao ponto onde paramos: \\[\nE(X^2)= 2\\left(\\frac{1}{\\lambda^2}\\right)\n= \\frac{2}{\\lambda^2}.\n\\]\n\n\n\n\n4.5 3. Cálculo da variância\nUsamos: \\[\n\\mathrm{Var}(X)=E(X^2)-[E(X)]^2.\n\\]\nSubstituindo:\n\n\\(E(X^2)=\\frac{2}{\\lambda^2}\\)\n\\(E(X)=\\frac{1}{\\lambda}\\)\n\nEntão: \\[\n\\mathrm{Var}(X)\n= \\frac{2}{\\lambda^2} - \\left(\\frac{1}{\\lambda}\\right)^2\n= \\frac{2}{\\lambda^2} - \\frac{1}{\\lambda^2}\n= \\frac{1}{\\lambda^2}.\n\\]\n\n\n\n4.6 4. Resultado final\nPara \\(X \\sim \\text{Exponencial}(\\lambda)\\):\n\\[\nE(X)=\\frac{1}{\\lambda},\n\\qquad\n\\mathrm{Var}(X)=\\frac{1}{\\lambda^2}.\n\\]\n\n\n\n\nExemplo: O tempo entre chegadas ao caixa segue \\(X \\sim Exp(0{,}2)\\).\n\nCalcule \\(P(X&gt;8)\\).\n\nDetermine a mediana.\n\nInterprete a propriedade “sem memória”.\n\nSolução:\n\n\\[\nP(X&gt;8)=e^{-0{,}2\\cdot 8}=e^{-1{,}6}\\approx 0{,}2019\n\\]\n\\[\nm=\\frac{\\ln 2}{0{,}2}=5\\ln 2 \\approx 3{,}47\n\\]\nO tempo adicional não depende do tempo já passado.",
    "crumbs": [
      "Revisão Probabilidade I"
    ]
  },
  {
    "objectID": "revisao.html#cálculo-de-ex-5",
    "href": "revisao.html#cálculo-de-ex-5",
    "title": "Revisão Probabilidade I",
    "section": "4.7 1. Cálculo de \\(E(X)\\)",
    "text": "4.7 1. Cálculo de \\(E(X)\\)\nPela definição: \\[\nE(X) = \\int_0^\\infty x\\lambda e^{-\\lambda x}\\,dx.\n\\]\nPara resolver a integral, usamos integração por partes.\nEscolhemos: - (u = x) → (du = dx) - (dv = e^{-x}dx) → (v = -e^{-x})\nAplicando integração por partes: \\[\n\\int_0^\\infty x\\lambda e^{-\\lambda x}\\,dx\n= \\left[-x e^{-\\lambda x}\\right]_{0}^{\\infty}\n+ \\int_0^\\infty e^{-\\lambda x}\\,dx.\n\\]\nAgora avaliamos cada termo:\n\nTermo de fronteira:\n\nQuando (x): (x e^{-x} ) (exponencial domina linear).\nQuando (x=0): (0 e^{0} = 0).\n\nLogo: \\[\n\\left[-x e^{-\\lambda x}\\right]_{0}^{\\infty} = 0.\n\\]\nIntegral restante: \\[\n\\int_0^\\infty e^{-\\lambda x}\\,dx\n= \\left[-\\frac{1}{\\lambda}e^{-\\lambda x}\\right]_{0}^{\\infty}\n= \\frac{1}{\\lambda}.\n\\]\n\nPortanto: \\[\nE(X)=\\frac{1}{\\lambda}.\n\\]",
    "crumbs": [
      "Revisão Probabilidade I"
    ]
  },
  {
    "objectID": "revisao.html#cálculo-de-ex2-4",
    "href": "revisao.html#cálculo-de-ex2-4",
    "title": "Revisão Probabilidade I",
    "section": "4.8 2. Cálculo de \\(E(X^2)\\)",
    "text": "4.8 2. Cálculo de \\(E(X^2)\\)\nPela definição: \\[\nE(X^2)=\\int_0^\\infty x^2 \\lambda e^{-\\lambda x}\\,dx.\n\\]\nVamos aplicar integração por partes duas vezes.\n\n\n4.8.1 Primeira integração por partes\nEscolha: - (u = x^2) → (du = 2x,dx) - (dv = e^{-x}dx) → (v = -e^{-x})\nEntão: \\[\nE(X^2)\n= \\left[-x^2 e^{-\\lambda x}\\right]_0^\\infty\n+ \\int_0^\\infty 2x e^{-\\lambda x} \\, dx.\n\\]\nO termo de fronteira é zero, pelo mesmo argumento anterior.\nAssim: \\[\nE(X^2)= 2\\int_0^\\infty x e^{-\\lambda x}\\,dx.\n\\]\n\n\n\n4.8.2 Segunda integração por partes\nAgora resolvemos: \\[\n\\int_0^\\infty x e^{-\\lambda x}\\,dx.\n\\]\nEscolha: - (u = x) → (du = dx) - (dv = e^{-x}dx) → (v = -e^{-x})\nEntão: \\[\n\\int_0^\\infty x e^{-\\lambda x}\\,dx\n= \\left[-\\frac{x}{\\lambda} e^{-\\lambda x}\\right]_0^\\infty\n+ \\int_0^\\infty \\frac{1}{\\lambda} e^{-\\lambda x}\\,dx.\n\\]\nO termo de fronteira novamente é zero.\nResta: \\[\n\\int_0^\\infty x e^{-\\lambda x}\\,dx\n= \\frac{1}{\\lambda} \\int_0^\\infty e^{-\\lambda x}\\,dx\n= \\frac{1}{\\lambda}\\cdot\\frac{1}{\\lambda}\n= \\frac{1}{\\lambda^2}.\n\\]\n\n\n\n4.8.3 Conclusão do cálculo de \\(E(X^2)\\)\nVoltando ao ponto onde paramos: \\[\nE(X^2)= 2\\left(\\frac{1}{\\lambda^2}\\right)\n= \\frac{2}{\\lambda^2}.\n\\]",
    "crumbs": [
      "Revisão Probabilidade I"
    ]
  },
  {
    "objectID": "revisao.html#cálculo-da-variância",
    "href": "revisao.html#cálculo-da-variância",
    "title": "Revisão Probabilidade I",
    "section": "4.3 3. Cálculo da variância",
    "text": "4.3 3. Cálculo da variância\nUsamos: \\[\n\\mathrm{Var}(X)=E(X^2)-[E(X)]^2.\n\\]\nSubstituindo:\n\n\\(E(X^2)=\\frac{2}{\\lambda^2}\\)\n\\(E(X)=\\frac{1}{\\lambda}\\)\n\nEntão: \\[\n\\mathrm{Var}(X)\n= \\frac{2}{\\lambda^2} - \\left(\\frac{1}{\\lambda}\\right)^2\n= \\frac{2}{\\lambda^2} - \\frac{1}{\\lambda^2}\n= \\frac{1}{\\lambda^2}.\n\\]",
    "crumbs": [
      "Revisão Probabilidade I"
    ]
  },
  {
    "objectID": "revisao.html#resultado-final-6",
    "href": "revisao.html#resultado-final-6",
    "title": "Revisão Probabilidade I",
    "section": "4.14 3. Resultado final",
    "text": "4.14 3. Resultado final\nPara uma variável aleatória normal geral \\(X \\sim N(\\mu,\\sigma^2)\\), demonstramos que:\n\\[\nE(X) = \\mu,\n\\qquad\n\\mathrm{Var}(X) = \\sigma^2.\n\\]",
    "crumbs": [
      "Revisão Probabilidade I"
    ]
  },
  {
    "objectID": "revisao.html#normal-musigma2",
    "href": "revisao.html#normal-musigma2",
    "title": "Revisão Probabilidade I",
    "section": "4.3 Normal \\((\\mu,\\sigma^2)\\)",
    "text": "4.3 Normal \\((\\mu,\\sigma^2)\\)\n\n\n\n\n\n\n\nItem\nExpressão\n\n\n\n\nSuporte\n\\(x\\in\\mathbb{R}\\)\n\n\n\\(f(x)\\)\n\\(\\dfrac{1}{\\sigma\\sqrt{2\\pi}}\\,\\exp\\left(-\\dfrac{(x-\\mu)^2}{2\\sigma^2}\\right)\\)\n\n\n\\(F(x)\\)\n\\(\\Phi\\!\\left(\\dfrac{x-\\mu}{\\sigma}\\right)\\) (não possui forma fechada)\n\n\n\\(E[X]\\)\n\\(\\mu\\)\n\n\n\\(Var(X)\\)\n\\(\\sigma^2\\)\n\n\n\nUso de Tabelas: padronize \\(Z=(X-\\mu)/\\sigma\\) e leia \\(P(Z\\le z)\\) na tabela da Normal padrão \\(\\Phi(z)\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMostrar demonstração\n\n\n\n\n\nVamos considerar uma variável aleatória normal geral \\[\nX \\sim N(\\mu,\\sigma^2), \\quad \\sigma &gt; 0.\n\\]\nA densidade é \\[\nf_X(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}}\n\\exp\\!\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right),\\quad x\\in\\mathbb{R}.\n\\]\nNosso objetivo é demonstrar, a partir das definições, que \\[\nE(X)=\\mu,\n\\qquad\n\\mathrm{Var}(X)=\\sigma^2.\n\\]\nA estratégia será:\n\nCalcular \\(E(Z)\\) e \\(\\mathrm{Var}(Z)\\) para a normal padrão \\(Z\\sim N(0,1)\\).\nUsar a relação \\(X = \\mu + \\sigma Z\\).\n\n\nNormal padrão \\(Z \\sim N(0,1)\\)\nPara a normal padrão, a densidade é \\[\nf_Z(z)=\\frac{1}{\\sqrt{2\\pi}}e^{-z^2/2},\\quad z\\in\\mathbb{R}.\n\\]\nQueremos mostrar que: \\[\nE(Z)=0, \\qquad E(Z^2)=1 \\quad\\Rightarrow\\quad \\mathrm{Var}(Z)=1.\n\\]\n\nCálculo de \\(E(Z)\\)\nPela definição: \\[\nE(Z)=\\int_{-\\infty}^{\\infty} z f_Z(z)\\,dz\n= \\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{\\infty} z e^{-z^2/2}\\,dz.\n\\]\nObserve que:\n\nA função \\(e^{-z^2/2}\\) é par (simétrica): \\(e^{-(-z)^2/2}=e^{-z^2/2}\\).\nA função \\(z\\) é ímpar: \\((-z) = -z\\).\nLogo, o produto \\(z e^{-z^2/2}\\) é ímpar.\n\nA integral de uma função ímpar em intervalo simétrico é zero: \\[\n\\int_{-\\infty}^{\\infty} z e^{-z^2/2}\\,dz = 0.\n\\]\nPortanto: \\[\nE(Z)=0.\n\\]\n\nCálculo de \\(E(Z^2)\\)\nPela definição: \\[\nE(Z^2)=\\int_{-\\infty}^{\\infty} z^2 f_Z(z)\\,dz\n= \\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{\\infty} z^2 e^{-z^2/2}\\,dz.\n\\]\nPara calcular \\[\n\\int_{-\\infty}^{\\infty} z^2 e^{-z^2/2}\\,dz,\n\\] vamos usar um truque padrão com um parâmetro auxiliar.\nConsidere, para \\(a&gt;0\\): \\[\nI(a) = \\int_{-\\infty}^{\\infty} e^{-a z^2/2}\\,dz.\n\\]\nEste é um integral gaussiano. Sabe-se (ou demonstra-se via coordenadas polares) que \\[\nI(a) = \\sqrt{\\frac{2\\pi}{a}}.\n\\]\nAgora vamos derivar \\(I(a)\\) em relação a \\(a\\) para obter uma integral com \\(z^2\\).\n\nDerivando \\(I(a)\\)\nPor um lado, derivando “dentro” da integral (legítimo sob condições usuais):\n\\[\nI'(a) = \\frac{d}{da}\\int_{-\\infty}^{\\infty} e^{-a z^2/2}\\,dz\n      = \\int_{-\\infty}^{\\infty} \\frac{\\partial}{\\partial a}\n        \\big(e^{-a z^2/2}\\big)\\,dz.\n\\]\nMas \\[\n\\frac{\\partial}{\\partial a} e^{-a z^2/2}\n= -\\frac{z^2}{2} e^{-a z^2/2}.\n\\]\nLogo: \\[\nI'(a) = \\int_{-\\infty}^{\\infty} -\\frac{z^2}{2} e^{-a z^2/2}\\,dz\n      = -\\frac{1}{2}\\int_{-\\infty}^{\\infty} z^2 e^{-a z^2/2}\\,dz.\n\\]\nPor outro lado, derivando a expressão fechada \\[\nI(a) = \\sqrt{\\frac{2\\pi}{a}}\n= (2\\pi)^{1/2} a^{-1/2},\n\\]\ntemos \\[\nI'(a) = (2\\pi)^{1/2}\\left(-\\frac{1}{2}\\right)a^{-3/2}\n      = -\\frac{\\sqrt{2\\pi}}{2}\\,a^{-3/2}.\n\\]\nIgualando as duas expressões para \\(I'(a)\\): \\[\n-\\frac{1}{2}\\int_{-\\infty}^{\\infty} z^2 e^{-a z^2/2}\\,dz\n= -\\frac{\\sqrt{2\\pi}}{2}\\,a^{-3/2}.\n\\]\nMultiplicando por \\(-2\\) ambos os lados: \\[\n\\int_{-\\infty}^{\\infty} z^2 e^{-a z^2/2}\\,dz\n= \\sqrt{2\\pi}\\,a^{-3/2}.\n\\]\n\nAplicando o resultado em \\(a=1\\)\nQueremos o caso com \\(a=1\\), isto é, \\[\n\\int_{-\\infty}^{\\infty} z^2 e^{-z^2/2}\\,dz.\n\\]\nPela fórmula: \\[\n\\int_{-\\infty}^{\\infty} z^2 e^{-z^2/2}\\,dz\n= \\sqrt{2\\pi}\\cdot 1^{-3/2}\n= \\sqrt{2\\pi}.\n\\]\nAgora voltamos para \\(E(Z^2)\\): \\[\nE(Z^2)\n= \\frac{1}{\\sqrt{2\\pi}}\n\\int_{-\\infty}^{\\infty} z^2 e^{-z^2/2}\\,dz\n= \\frac{1}{\\sqrt{2\\pi}}\\cdot \\sqrt{2\\pi} = 1.\n\\]\nLogo: \\[\nE(Z^2)=1.\n\\]\n\nVariância da normal padrão\nA variância é \\[\n\\mathrm{Var}(Z)=E(Z^2)-[E(Z)]^2=1-0^2=1.\n\\]\nConcluímos: \\[\nE(Z)=0,\\qquad \\mathrm{Var}(Z)=1\n\\quad\\text{para } Z\\sim N(0,1).\n\\]\n\nNormal geral \\(X \\sim N(\\mu,\\sigma^2)\\)\nUma variável normal geral pode ser escrita como \\[\nX = \\mu + \\sigma Z,\n\\] onde \\(Z \\sim N(0,1)\\).\nVamos usar essa relação para encontrar \\(E(X)\\) e \\(\\mathrm{Var}(X)\\).\n\nCálculo de \\(E(X)\\)\nUsando a linearidade da esperança: \\[\nE(X) = E(\\mu + \\sigma Z)\n     = E(\\mu) + E(\\sigma Z)\n     = \\mu + \\sigma E(Z).\n\\]\nComo já mostramos que \\(E(Z)=0\\), obtemos: \\[\nE(X) = \\mu + \\sigma \\cdot 0 = \\mu.\n\\]\n\nCálculo de \\(\\mathrm{Var}(X)\\)\nUsamos a propriedade de variância para transformações lineares: \\[\n\\mathrm{Var}(a + bZ) = b^2\\,\\mathrm{Var}(Z).\n\\]\nAqui, \\(a=\\mu\\) e \\(b=\\sigma\\), então: \\[\n\\mathrm{Var}(X) = \\mathrm{Var}(\\mu + \\sigma Z)\n                = \\sigma^2\\,\\mathrm{Var}(Z).\n\\]\nSabemos que \\(\\mathrm{Var}(Z)=1\\), logo: \\[\n\\mathrm{Var}(X) = \\sigma^2 \\cdot 1 = \\sigma^2.\n\\]\n\nAssim, para uma variável aleatória normal geral \\(X \\sim N(\\mu,\\sigma^2)\\), demonstramos que:\n\\[\nE(X) = \\mu,\n\\qquad\n\\mathrm{Var}(X) = \\sigma^2.\n\\]\n\n\n\nExemplo: Pesos de pacotes seguem \\(W \\sim N(25, 1{,}5^2)\\).\n\nCalcule \\(P(24 &lt; W &lt; 27)\\).\n\nDetermine o percentil 95%.\n\nInterprete o percentil no controle de qualidade.\n\nSolução:\n\n\\[\nP(24&lt;W&lt;27)=P(-0{,}67&lt;Z&lt;1{,}33)=\\Phi(1{,}33)-\\Phi(-0{,}67)\n\\]\n\n\\[\n\\approx 0{,}9082 - 0{,}2514 = 0{,}6568\n\\]\n\n\\[\nx_{0{,}95}=25 + 1{,}645\\cdot 1{,}5 = 27{,}4675\n\\]\n95% dos pacotes pesam até 27,47 kg.",
    "crumbs": [
      "Revisão Probabilidade I"
    ]
  },
  {
    "objectID": "revisao.html#normal-padrão-z-sim-n01",
    "href": "revisao.html#normal-padrão-z-sim-n01",
    "title": "Revisão Probabilidade I",
    "section": "4.4 1. Normal padrão \\(Z \\sim N(0,1)\\)",
    "text": "4.4 1. Normal padrão \\(Z \\sim N(0,1)\\)\nPara a normal padrão, a densidade é \\[\nf_Z(z)=\\frac{1}{\\sqrt{2\\pi}}e^{-z^2/2},\\quad z\\in\\mathbb{R}.\n\\]\nQueremos mostrar que: \\[\nE(Z)=0, \\qquad E(Z^2)=1 \\quad\\Rightarrow\\quad \\mathrm{Var}(Z)=1.\n\\]\n\n\n4.4.1 1.1 Cálculo de \\(E(Z)\\)\nPela definição: \\[\nE(Z)=\\int_{-\\infty}^{\\infty} z f_Z(z)\\,dz\n= \\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{\\infty} z e^{-z^2/2}\\,dz.\n\\]\nObserve que:\n\nA função \\(e^{-z^2/2}\\) é par (simétrica): \\(e^{-(-z)^2/2}=e^{-z^2/2}\\).\nA função \\(z\\) é ímpar: \\((-z) = -z\\).\nLogo, o produto \\(z e^{-z^2/2}\\) é ímpar.\n\nA integral de uma função ímpar em intervalo simétrico é zero: \\[\n\\int_{-\\infty}^{\\infty} z e^{-z^2/2}\\,dz = 0.\n\\]\nPortanto: \\[\nE(Z)=0.\n\\]\n\n\n\n4.4.2 1.2 Cálculo de \\(E(Z^2)\\)\nPela definição: \\[\nE(Z^2)=\\int_{-\\infty}^{\\infty} z^2 f_Z(z)\\,dz\n= \\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{\\infty} z^2 e^{-z^2/2}\\,dz.\n\\]\nPara calcular \\[\n\\int_{-\\infty}^{\\infty} z^2 e^{-z^2/2}\\,dz,\n\\] vamos usar um truque padrão com um parâmetro auxiliar.\nConsidere, para \\(a&gt;0\\): \\[\nI(a) = \\int_{-\\infty}^{\\infty} e^{-a z^2/2}\\,dz.\n\\]\nEste é um integral gaussiano. Sabe-se (ou demonstra-se via coordenadas polares) que \\[\nI(a) = \\sqrt{\\frac{2\\pi}{a}}.\n\\]\nAgora vamos derivar \\(I(a)\\) em relação a \\(a\\) para obter uma integral com \\(z^2\\).\n\n\n4.4.2.1 Derivando \\(I(a)\\)\nPor um lado, derivando “dentro” da integral (legítimo sob condições usuais):\n\\[\nI'(a) = \\frac{d}{da}\\int_{-\\infty}^{\\infty} e^{-a z^2/2}\\,dz\n      = \\int_{-\\infty}^{\\infty} \\frac{\\partial}{\\partial a}\n        \\big(e^{-a z^2/2}\\big)\\,dz.\n\\]\nMas \\[\n\\frac{\\partial}{\\partial a} e^{-a z^2/2}\n= -\\frac{z^2}{2} e^{-a z^2/2}.\n\\]\nLogo: \\[\nI'(a) = \\int_{-\\infty}^{\\infty} -\\frac{z^2}{2} e^{-a z^2/2}\\,dz\n      = -\\frac{1}{2}\\int_{-\\infty}^{\\infty} z^2 e^{-a z^2/2}\\,dz.\n\\]\nPor outro lado, derivando a expressão fechada \\[\nI(a) = \\sqrt{\\frac{2\\pi}{a}}\n= (2\\pi)^{1/2} a^{-1/2},\n\\]\ntemos \\[\nI'(a) = (2\\pi)^{1/2}\\left(-\\frac{1}{2}\\right)a^{-3/2}\n      = -\\frac{\\sqrt{2\\pi}}{2}\\,a^{-3/2}.\n\\]\nIgualando as duas expressões para \\(I'(a)\\): \\[\n-\\frac{1}{2}\\int_{-\\infty}^{\\infty} z^2 e^{-a z^2/2}\\,dz\n= -\\frac{\\sqrt{2\\pi}}{2}\\,a^{-3/2}.\n\\]\nMultiplicando por \\(-2\\) ambos os lados: \\[\n\\int_{-\\infty}^{\\infty} z^2 e^{-a z^2/2}\\,dz\n= \\sqrt{2\\pi}\\,a^{-3/2}.\n\\]\n\n\n\n4.4.2.2 Aplicando o resultado em \\(a=1\\)\nQueremos o caso com \\(a=1\\), isto é, \\[\n\\int_{-\\infty}^{\\infty} z^2 e^{-z^2/2}\\,dz.\n\\]\nPela fórmula: \\[\n\\int_{-\\infty}^{\\infty} z^2 e^{-z^2/2}\\,dz\n= \\sqrt{2\\pi}\\cdot 1^{-3/2}\n= \\sqrt{2\\pi}.\n\\]\nAgora voltamos para \\(E(Z^2)\\): \\[\nE(Z^2)\n= \\frac{1}{\\sqrt{2\\pi}}\n\\int_{-\\infty}^{\\infty} z^2 e^{-z^2/2}\\,dz\n= \\frac{1}{\\sqrt{2\\pi}}\\cdot \\sqrt{2\\pi} = 1.\n\\]\nLogo: \\[\nE(Z^2)=1.\n\\]\n\n\n\n\n4.4.3 1.3 Variância da normal padrão\nA variância é \\[\n\\mathrm{Var}(Z)=E(Z^2)-[E(Z)]^2=1-0^2=1.\n\\]\nConcluímos: \\[\nE(Z)=0,\\qquad \\mathrm{Var}(Z)=1\n\\quad\\text{para } Z\\sim N(0,1).\n\\]",
    "crumbs": [
      "Revisão Probabilidade I"
    ]
  },
  {
    "objectID": "revisao.html#normal-geral-x-sim-nmusigma2",
    "href": "revisao.html#normal-geral-x-sim-nmusigma2",
    "title": "Revisão Probabilidade I",
    "section": "4.5 2. Normal geral \\(X \\sim N(\\mu,\\sigma^2)\\)",
    "text": "4.5 2. Normal geral \\(X \\sim N(\\mu,\\sigma^2)\\)\nUma variável normal geral pode ser escrita como \\[\nX = \\mu + \\sigma Z,\n\\] onde \\(Z \\sim N(0,1)\\).\nVamos usar essa relação para encontrar \\(E(X)\\) e \\(\\mathrm{Var}(X)\\).\n\n\n4.5.1 2.1 Cálculo de \\(E(X)\\)\nUsando a linearidade da esperança: \\[\nE(X) = E(\\mu + \\sigma Z)\n     = E(\\mu) + E(\\sigma Z)\n     = \\mu + \\sigma E(Z).\n\\]\nComo já mostramos que \\(E(Z)=0\\), obtemos: \\[\nE(X) = \\mu + \\sigma \\cdot 0 = \\mu.\n\\]\n\n\n\n4.5.2 2.2 Cálculo de \\(\\mathrm{Var}(X)\\)\nUsamos a propriedade de variância para transformações lineares: \\[\n\\mathrm{Var}(a + bZ) = b^2\\,\\mathrm{Var}(Z).\n\\]\nAqui, \\(a=\\mu\\) e \\(b=\\sigma\\), então: \\[\n\\mathrm{Var}(X) = \\mathrm{Var}(\\mu + \\sigma Z)\n                = \\sigma^2\\,\\mathrm{Var}(Z).\n\\]\nSabemos que \\(\\mathrm{Var}(Z)=1\\), logo: \\[\n\\mathrm{Var}(X) = \\sigma^2 \\cdot 1 = \\sigma^2.\n\\]",
    "crumbs": [
      "Revisão Probabilidade I"
    ]
  },
  {
    "objectID": "revisao.html#resultado-final-7",
    "href": "revisao.html#resultado-final-7",
    "title": "Revisão Probabilidade I",
    "section": "4.14 3. Resultado final",
    "text": "4.14 3. Resultado final\nPara uma variável aleatória normal geral \\(X \\sim N(\\mu,\\sigma^2)\\), demonstramos que:\n\\[\nE(X) = \\mu,\n\\qquad\n\\mathrm{Var}(X) = \\sigma^2.\n\\]",
    "crumbs": [
      "Revisão Probabilidade I"
    ]
  },
  {
    "objectID": "revisao.html#hipergeométrica-approx-binomial",
    "href": "revisao.html#hipergeométrica-approx-binomial",
    "title": "Revisão Probabilidade I",
    "section": "5.1 Hipergeométrica \\(\\approx\\) Binomial",
    "text": "5.1 Hipergeométrica \\(\\approx\\) Binomial\nCondição: população grande vs. amostra pequena (fração amostral \\(n/N\\) pequena).\nUse \\(X\\sim Bin(n, p=K/N)\\) como aproximação.",
    "crumbs": [
      "Revisão Probabilidade I"
    ]
  },
  {
    "objectID": "revisao.html#binomial-approx-poisson",
    "href": "revisao.html#binomial-approx-poisson",
    "title": "Revisão Probabilidade I",
    "section": "5.2 Binomial \\(\\approx\\) Poisson",
    "text": "5.2 Binomial \\(\\approx\\) Poisson\nCondição: \\(n\\) grande, \\(p\\) pequeno, \\(\\lambda=np\\) moderado.\nAproximação: \\(P_{Bin}(X=k)\\approx e^{-\\lambda}\\lambda^k/k!\\).",
    "crumbs": [
      "Revisão Probabilidade I"
    ]
  },
  {
    "objectID": "revisao.html#binomial-approx-normal",
    "href": "revisao.html#binomial-approx-normal",
    "title": "Revisão Probabilidade I",
    "section": "5.3 Binomial \\(\\approx\\) Normal",
    "text": "5.3 Binomial \\(\\approx\\) Normal\nCondição: \\(np(1-p)\\gtrsim 10\\).\nAproximação: \\(X\\approx N(\\mu=np,\\sigma^2=np(1-p))\\) com correção de continuidade.",
    "crumbs": [
      "Revisão Probabilidade I"
    ]
  },
  {
    "objectID": "revisao.html#poisson-approx-normal",
    "href": "revisao.html#poisson-approx-normal",
    "title": "Revisão Probabilidade I",
    "section": "5.4 Poisson \\(\\approx\\) Normal",
    "text": "5.4 Poisson \\(\\approx\\) Normal\nCondição: \\(\\lambda\\gtrsim 10\\).\nAproximação: \\(X\\approx N(\\mu=\\lambda,\\sigma^2=\\lambda)\\) com correção de continuidade.",
    "crumbs": [
      "Revisão Probabilidade I"
    ]
  },
  {
    "objectID": "revisao.html#cálculo-de-ex-2",
    "href": "revisao.html#cálculo-de-ex-2",
    "title": "Revisão Probabilidade I",
    "section": "4.7 1. Cálculo de \\(E(X)\\)",
    "text": "4.7 1. Cálculo de \\(E(X)\\)\nPela definição: \\[\nE(X) = \\int_0^\\infty x\\lambda e^{-\\lambda x}\\,dx.\n\\]\nPara resolver a integral, usamos integração por partes.\nEscolhemos: - (u = x) → (du = dx) - (dv = e^{-x}dx) → (v = -e^{-x})\nAplicando integração por partes: \\[\n\\int_0^\\infty x\\lambda e^{-\\lambda x}\\,dx\n= \\left[-x e^{-\\lambda x}\\right]_{0}^{\\infty}\n+ \\int_0^\\infty e^{-\\lambda x}\\,dx.\n\\]\nAgora avaliamos cada termo:\n\nTermo de fronteira:\n\nQuando (x): (x e^{-x} ) (exponencial domina linear).\nQuando (x=0): (0 e^{0} = 0).\n\nLogo: \\[\n\\left[-x e^{-\\lambda x}\\right]_{0}^{\\infty} = 0.\n\\]\nIntegral restante: \\[\n\\int_0^\\infty e^{-\\lambda x}\\,dx\n= \\left[-\\frac{1}{\\lambda}e^{-\\lambda x}\\right]_{0}^{\\infty}\n= \\frac{1}{\\lambda}.\n\\]\n\nPortanto: \\[\nE(X)=\\frac{1}{\\lambda}.\n\\]",
    "crumbs": [
      "Revisão Probabilidade I"
    ]
  },
  {
    "objectID": "revisao.html#cálculo-de-ex2-1",
    "href": "revisao.html#cálculo-de-ex2-1",
    "title": "Revisão Probabilidade I",
    "section": "4.8 2. Cálculo de \\(E(X^2)\\)",
    "text": "4.8 2. Cálculo de \\(E(X^2)\\)\nPela definição: \\[\nE(X^2)=\\int_0^\\infty x^2 \\lambda e^{-\\lambda x}\\,dx.\n\\]\nVamos aplicar integração por partes duas vezes.\n\n\n4.8.1 Primeira integração por partes\nEscolha: - (u = x^2) → (du = 2x,dx) - (dv = e^{-x}dx) → (v = -e^{-x})\nEntão: \\[\nE(X^2)\n= \\left[-x^2 e^{-\\lambda x}\\right]_0^\\infty\n+ \\int_0^\\infty 2x e^{-\\lambda x} \\, dx.\n\\]\nO termo de fronteira é zero, pelo mesmo argumento anterior.\nAssim: \\[\nE(X^2)= 2\\int_0^\\infty x e^{-\\lambda x}\\,dx.\n\\]\n\n\n\n4.8.2 Segunda integração por partes\nAgora resolvemos: \\[\n\\int_0^\\infty x e^{-\\lambda x}\\,dx.\n\\]\nEscolha: - (u = x) → (du = dx) - (dv = e^{-x}dx) → (v = -e^{-x})\nEntão: \\[\n\\int_0^\\infty x e^{-\\lambda x}\\,dx\n= \\left[-\\frac{x}{\\lambda} e^{-\\lambda x}\\right]_0^\\infty\n+ \\int_0^\\infty \\frac{1}{\\lambda} e^{-\\lambda x}\\,dx.\n\\]\nO termo de fronteira novamente é zero.\nResta: \\[\n\\int_0^\\infty x e^{-\\lambda x}\\,dx\n= \\frac{1}{\\lambda} \\int_0^\\infty e^{-\\lambda x}\\,dx\n= \\frac{1}{\\lambda}\\cdot\\frac{1}{\\lambda}\n= \\frac{1}{\\lambda^2}.\n\\]\n\n\n\n4.8.3 Conclusão do cálculo de \\(E(X^2)\\)\nVoltando ao ponto onde paramos: \\[\nE(X^2)= 2\\left(\\frac{1}{\\lambda^2}\\right)\n= \\frac{2}{\\lambda^2}.\n\\]",
    "crumbs": [
      "Revisão Probabilidade I"
    ]
  },
  {
    "objectID": "revisao.html#resultado-final-3",
    "href": "revisao.html#resultado-final-3",
    "title": "Revisão Probabilidade I",
    "section": "4.10 4. Resultado final",
    "text": "4.10 4. Resultado final\nPara \\(X \\sim \\text{Exponencial}(\\lambda)\\):\n\\[\nE(X)=\\frac{1}{\\lambda},\n\\qquad\n\\mathrm{Var}(X)=\\frac{1}{\\lambda^2}.\n\\]",
    "crumbs": [
      "Revisão Probabilidade I"
    ]
  },
  {
    "objectID": "revisao.html#cálculo-de-mathrmvarx-2",
    "href": "revisao.html#cálculo-de-mathrmvarx-2",
    "title": "Revisão Probabilidade I",
    "section": "4.4 3. Cálculo de \\(\\mathrm{Var}(X)\\)",
    "text": "4.4 3. Cálculo de \\(\\mathrm{Var}(X)\\)\nUsamos: \\[\n\\mathrm{Var}(X) = E(X^2) - [E(X)]^2.\n\\]\nJá obtivemos:\n\n\\(E(X) = \\dfrac{a+b}{2}\\)\n\n\\(E(X^2) = \\dfrac{a^2 + ab + b^2}{3}\\)\n\nEntão: \\[\n\\mathrm{Var}(X)\n= \\frac{a^2 + ab + b^2}{3} - \\left(\\frac{a+b}{2}\\right)^2.\n\\]\nPrimeiro, calculemos o quadrado: \\[\n\\left(\\frac{a+b}{2}\\right)^2\n= \\frac{(a+b)^2}{4}\n= \\frac{a^2 + 2ab + b^2}{4}.\n\\]\nAgora escrevemos a variância com denominador comum.\nO denominador comum entre 3 e 4 é 12:\n\nPrimeiro termo: \\[\n\\frac{a^2 + ab + b^2}{3}\n= \\frac{4(a^2 + ab + b^2)}{12}.\n\\]\nSegundo termo: \\[\n\\frac{a^2 + 2ab + b^2}{4}\n= \\frac{3(a^2 + 2ab + b^2)}{12}.\n\\]\n\nEntão: \\[\n\\mathrm{Var}(X)\n= \\frac{4(a^2 + ab + b^2)}{12} - \\frac{3(a^2 + 2ab + b^2)}{12}.\n\\]\nSubtraindo os numeradores: \\[\n4(a^2 + ab + b^2) - 3(a^2 + 2ab + b^2)\n= 4a^2 + 4ab + 4b^2 - 3a^2 - 6ab - 3b^2\n= (4a^2 - 3a^2) + (4ab - 6ab) + (4b^2 - 3b^2)\n= a^2 - 2ab + b^2.\n\\]\nLogo: \\[\n\\mathrm{Var}(X)\n= \\frac{a^2 - 2ab + b^2}{12}\n= \\frac{(b-a)^2}{12},\n\\] pois \\[\na^2 - 2ab + b^2 = (b-a)^2.\n\\]",
    "crumbs": [
      "Revisão Probabilidade I"
    ]
  },
  {
    "objectID": "revisao.html#cálculo-de-ex",
    "href": "revisao.html#cálculo-de-ex",
    "title": "Revisão Probabilidade I",
    "section": "4.3 1. Cálculo de \\(E(X)\\)",
    "text": "4.3 1. Cálculo de \\(E(X)\\)\nPela definição: \\[\nE(X) = \\int_0^\\infty x\\lambda e^{-\\lambda x}\\,dx.\n\\]\nPara resolver a integral, usamos integração por partes.\nEscolhemos: - (u = x) → (du = dx) - (dv = e^{-x}dx) → (v = -e^{-x})\nAplicando integração por partes: \\[\n\\int_0^\\infty x\\lambda e^{-\\lambda x}\\,dx\n= \\left[-x e^{-\\lambda x}\\right]_{0}^{\\infty}\n+ \\int_0^\\infty e^{-\\lambda x}\\,dx.\n\\]\nAgora avaliamos cada termo:\n\nTermo de fronteira:\n\nQuando (x): (x e^{-x} ) (exponencial domina linear).\nQuando (x=0): (0 e^{0} = 0).\n\nLogo: \\[\n\\left[-x e^{-\\lambda x}\\right]_{0}^{\\infty} = 0.\n\\]\nIntegral restante: \\[\n\\int_0^\\infty e^{-\\lambda x}\\,dx\n= \\left[-\\frac{1}{\\lambda}e^{-\\lambda x}\\right]_{0}^{\\infty}\n= \\frac{1}{\\lambda}.\n\\]\n\nPortanto: \\[\nE(X)=\\frac{1}{\\lambda}.\n\\]",
    "crumbs": [
      "Revisão Probabilidade I"
    ]
  },
  {
    "objectID": "revisao.html#cálculo-de-ex2",
    "href": "revisao.html#cálculo-de-ex2",
    "title": "Revisão Probabilidade I",
    "section": "4.3 2. Cálculo de \\(E(X^2)\\)",
    "text": "4.3 2. Cálculo de \\(E(X^2)\\)\nPela definição: \\[\nE(X^2)=\\int_0^\\infty x^2 \\lambda e^{-\\lambda x}\\,dx.\n\\]\nVamos aplicar integração por partes duas vezes.\n\n\n4.3.1 Primeira integração por partes\nEscolha: - (u = x^2) → (du = 2x,dx) - (dv = e^{-x}dx) → (v = -e^{-x})\nEntão: \\[\nE(X^2)\n= \\left[-x^2 e^{-\\lambda x}\\right]_0^\\infty\n+ \\int_0^\\infty 2x e^{-\\lambda x} \\, dx.\n\\]\nO termo de fronteira é zero, pelo mesmo argumento anterior.\nAssim: \\[\nE(X^2)= 2\\int_0^\\infty x e^{-\\lambda x}\\,dx.\n\\]\n\n\n\n4.3.2 Segunda integração por partes\nAgora resolvemos: \\[\n\\int_0^\\infty x e^{-\\lambda x}\\,dx.\n\\]\nEscolha: - (u = x) → (du = dx) - (dv = e^{-x}dx) → (v = -e^{-x})\nEntão: \\[\n\\int_0^\\infty x e^{-\\lambda x}\\,dx\n= \\left[-\\frac{x}{\\lambda} e^{-\\lambda x}\\right]_0^\\infty\n+ \\int_0^\\infty \\frac{1}{\\lambda} e^{-\\lambda x}\\,dx.\n\\]\nO termo de fronteira novamente é zero.\nResta: \\[\n\\int_0^\\infty x e^{-\\lambda x}\\,dx\n= \\frac{1}{\\lambda} \\int_0^\\infty e^{-\\lambda x}\\,dx\n= \\frac{1}{\\lambda}\\cdot\\frac{1}{\\lambda}\n= \\frac{1}{\\lambda^2}.\n\\]\n\n\n\n4.3.3 Conclusão do cálculo de \\(E(X^2)\\)\nVoltando ao ponto onde paramos: \\[\nE(X^2)= 2\\left(\\frac{1}{\\lambda^2}\\right)\n= \\frac{2}{\\lambda^2}.\n\\]",
    "crumbs": [
      "Revisão Probabilidade I"
    ]
  },
  {
    "objectID": "revisao.html#resultado-final-1",
    "href": "revisao.html#resultado-final-1",
    "title": "Revisão Probabilidade I",
    "section": "4.8 3. Resultado final",
    "text": "4.8 3. Resultado final\nPara uma variável aleatória normal geral \\(X \\sim N(\\mu,\\sigma^2)\\), demonstramos que:\n\\[\nE(X) = \\mu,\n\\qquad\n\\mathrm{Var}(X) = \\sigma^2.\n\\]",
    "crumbs": [
      "Revisão Probabilidade I"
    ]
  },
  {
    "objectID": "revisao.html#cálculo-de-ex-1",
    "href": "revisao.html#cálculo-de-ex-1",
    "title": "Revisão Probabilidade I",
    "section": "4.7 1. Cálculo de \\(E(X)\\)",
    "text": "4.7 1. Cálculo de \\(E(X)\\)\nPela definição: \\[\nE(X) = \\int_0^\\infty x\\lambda e^{-\\lambda x}\\,dx.\n\\]\nPara resolver a integral, usamos integração por partes.\nEscolhemos: - (u = x) → (du = dx) - (dv = e^{-x}dx) → (v = -e^{-x})\nAplicando integração por partes: \\[\n\\int_0^\\infty x\\lambda e^{-\\lambda x}\\,dx\n= \\left[-x e^{-\\lambda x}\\right]_{0}^{\\infty}\n+ \\int_0^\\infty e^{-\\lambda x}\\,dx.\n\\]\nAgora avaliamos cada termo:\n\nTermo de fronteira:\n\nQuando (x): (x e^{-x} ) (exponencial domina linear).\nQuando (x=0): (0 e^{0} = 0).\n\nLogo: \\[\n\\left[-x e^{-\\lambda x}\\right]_{0}^{\\infty} = 0.\n\\]\nIntegral restante: \\[\n\\int_0^\\infty e^{-\\lambda x}\\,dx\n= \\left[-\\frac{1}{\\lambda}e^{-\\lambda x}\\right]_{0}^{\\infty}\n= \\frac{1}{\\lambda}.\n\\]\n\nPortanto: \\[\nE(X)=\\frac{1}{\\lambda}.\n\\]",
    "crumbs": [
      "Revisão Probabilidade I"
    ]
  },
  {
    "objectID": "revisao.html#cálculo-de-mathrmvarx",
    "href": "revisao.html#cálculo-de-mathrmvarx",
    "title": "Revisão Probabilidade I",
    "section": "4.4 3. Cálculo de \\(\\mathrm{Var}(X)\\)",
    "text": "4.4 3. Cálculo de \\(\\mathrm{Var}(X)\\)\nUsamos: \\[\n\\mathrm{Var}(X) = E(X^2) - [E(X)]^2.\n\\]\nJá obtivemos:\n\n\\(E(X) = \\dfrac{a+b}{2}\\)\n\n\\(E(X^2) = \\dfrac{a^2 + ab + b^2}{3}\\)\n\nEntão: \\[\n\\mathrm{Var}(X)\n= \\frac{a^2 + ab + b^2}{3} - \\left(\\frac{a+b}{2}\\right)^2.\n\\]\nPrimeiro, calculemos o quadrado: \\[\n\\left(\\frac{a+b}{2}\\right)^2\n= \\frac{(a+b)^2}{4}\n= \\frac{a^2 + 2ab + b^2}{4}.\n\\]\nAgora escrevemos a variância com denominador comum.\nO denominador comum entre 3 e 4 é 12:\n\nPrimeiro termo: \\[\n\\frac{a^2 + ab + b^2}{3}\n= \\frac{4(a^2 + ab + b^2)}{12}.\n\\]\nSegundo termo: \\[\n\\frac{a^2 + 2ab + b^2}{4}\n= \\frac{3(a^2 + 2ab + b^2)}{12}.\n\\]\n\nEntão: \\[\n\\mathrm{Var}(X)\n= \\frac{4(a^2 + ab + b^2)}{12} - \\frac{3(a^2 + 2ab + b^2)}{12}.\n\\]\nSubtraindo os numeradores: \\[\n4(a^2 + ab + b^2) - 3(a^2 + 2ab + b^2)\n= 4a^2 + 4ab + 4b^2 - 3a^2 - 6ab - 3b^2\n= (4a^2 - 3a^2) + (4ab - 6ab) + (4b^2 - 3b^2)\n= a^2 - 2ab + b^2.\n\\]\nLogo: \\[\n\\mathrm{Var}(X)\n= \\frac{a^2 - 2ab + b^2}{12}\n= \\frac{(b-a)^2}{12},\n\\] pois \\[\na^2 - 2ab + b^2 = (b-a)^2.\n\\]",
    "crumbs": [
      "Revisão Probabilidade I"
    ]
  },
  {
    "objectID": "revisao.html#resultado-final-2",
    "href": "revisao.html#resultado-final-2",
    "title": "Revisão Probabilidade I",
    "section": "4.14 3. Resultado final",
    "text": "4.14 3. Resultado final\nPara uma variável aleatória normal geral \\(X \\sim N(\\mu,\\sigma^2)\\), demonstramos que:\n\\[\nE(X) = \\mu,\n\\qquad\n\\mathrm{Var}(X) = \\sigma^2.\n\\]",
    "crumbs": [
      "Revisão Probabilidade I"
    ]
  },
  {
    "objectID": "revisao.html#resultado-final",
    "href": "revisao.html#resultado-final",
    "title": "Revisão Probabilidade I",
    "section": "4.6 3. Resultado final",
    "text": "4.6 3. Resultado final\nPara uma variável aleatória normal geral \\(X \\sim N(\\mu,\\sigma^2)\\), demonstramos que:\n\\[\nE(X) = \\mu,\n\\qquad\n\\mathrm{Var}(X) = \\sigma^2.\n\\]",
    "crumbs": [
      "Revisão Probabilidade I"
    ]
  },
  {
    "objectID": "revisao.html#exponencial-lambda",
    "href": "revisao.html#exponencial-lambda",
    "title": "Revisão Probabilidade I",
    "section": "4.2 Exponencial \\((\\lambda)\\)",
    "text": "4.2 Exponencial \\((\\lambda)\\)\n\n\n\nItem\nExpressão\n\n\n\n\nSuporte\n\\(x\\ge 0\\)\n\n\n\\(f(x)\\)\n\\(\\lambda e^{-\\lambda x}\\)\n\n\n\\(F(x)\\)\n\\(1-e^{-\\lambda x}\\)\n\n\n\\(E[X]\\)\n\\(1/\\lambda\\)\n\n\n\\(Var(X)\\)\n\\(1/\\lambda^2\\)\n\n\nPropriedade\nSem memória: \\(P(X&gt;s+t\\mid X&gt;t)=P(X&gt;s)\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMostrar demonstração\n\n\n\n\n\nConsidere \\(X \\sim \\text{Exponencial}(\\lambda)\\), com \\(\\lambda &gt; 0\\).\nA função densidade é \\[\nf(x) =\n\\begin{cases}\n\\lambda e^{-\\lambda x}, & x \\ge 0, \\\\[4pt]\n0, & x &lt; 0.\n\\end{cases}\n\\]\nNosso objetivo é demonstrar: \\[\nE(X) = \\frac{1}{\\lambda},\n\\qquad\n\\mathrm{Var}(X) = \\frac{1}{\\lambda^2},\n\\] usando apenas as definições: \\[\nE(X) = \\int_0^\\infty x\\,\\lambda e^{-\\lambda x}\\,dx,\n\\qquad\nE(X^2) = \\int_0^\\infty x^2\\,\\lambda e^{-\\lambda x}\\,dx.\n\\]\n\nCálculo de \\(E(X)\\)\nPela definição: \\[\nE(X) = \\int_0^\\infty x\\lambda e^{-\\lambda x}\\,dx.\n\\]\nPara resolver a integral, usamos integração por partes.\nEscolhemos: - \\(u = x\\) → \\(du = dx\\) - \\(dv = \\lambda e^{-\\lambda x}dx\\) → \\(v = -e^{-\\lambda x}\\)\nAplicando integração por partes: \\[\n\\int_0^\\infty x\\lambda e^{-\\lambda x}\\,dx\n= \\left[-x e^{-\\lambda x}\\right]_{0}^{\\infty}\n+ \\int_0^\\infty e^{-\\lambda x}\\,dx.\n\\]\nAgora avaliamos cada termo:\n\nPrimeira parte:\n\nQuando \\(x\\to\\infty\\): \\(x e^{-\\lambda x} \\to 0\\).\nQuando \\(x=0\\): \\(0 \\cdot e^{0} = 0\\).\n\nLogo: \\[\n\\left[-x e^{-\\lambda x}\\right]_{0}^{\\infty} = 0.\n\\]\nIntegral restante: \\[\n\\int_0^\\infty e^{-\\lambda x}\\,dx\n= \\left[-\\frac{1}{\\lambda}e^{-\\lambda x}\\right]_{0}^{\\infty}\n= \\frac{1}{\\lambda}.\n\\]\n\nPortanto: \\[\nE(X)=\\frac{1}{\\lambda}.\n\\]\n\nCálculo de \\(E(X^2)\\)\nPela definição: \\[\nE(X^2)=\\int_0^\\infty x^2 \\lambda e^{-\\lambda x}\\,dx.\n\\]\nVamos aplicar integração por partes duas vezes.\n\nPrimeira integração por partes\nEscolha: - \\(u = x^2\\) → \\(du = 2x\\,dx\\) - \\(dv = \\lambda e^{-\\lambda x}dx\\) → \\(v = -e^{-\\lambda x}\\)\nEntão: \\[\nE(X^2)\n= \\left[-x^2 e^{-\\lambda x}\\right]_0^\\infty\n+ \\int_0^\\infty 2x e^{-\\lambda x} \\, dx.\n\\]\nO primeiro termo é zero, pelo mesmo argumento anterior.\nAssim: \\[\nE(X^2)= 2\\int_0^\\infty x e^{-\\lambda x}\\,dx.\n\\]\n\nSegunda integração por partes\nAgora resolvemos: \\[\n\\int_0^\\infty x e^{-\\lambda x}\\,dx.\n\\]\nEscolha: - \\(u = x\\) → \\(du = dx\\) - \\(dv = e^{-\\lambda x}dx\\) → \\(v = -\\frac{1}{\\lambda}e^{-\\lambda x}\\)\nEntão: \\[\n\\int_0^\\infty x e^{-\\lambda x}\\,dx\n= \\left[-\\frac{x}{\\lambda} e^{-\\lambda x}\\right]_0^\\infty\n+ \\int_0^\\infty \\frac{1}{\\lambda} e^{-\\lambda x}\\,dx.\n\\]\nO primeiro termo novamente é zero.\nResta: \\[\n\\int_0^\\infty x e^{-\\lambda x}\\,dx\n= \\frac{1}{\\lambda} \\int_0^\\infty e^{-\\lambda x}\\,dx\n= \\frac{1}{\\lambda}\\cdot\\frac{1}{\\lambda}\n= \\frac{1}{\\lambda^2}.\n\\]\nVoltando ao ponto onde paramos: \\[\nE(X^2)= 2\\left(\\frac{1}{\\lambda^2}\\right)\n= \\frac{2}{\\lambda^2}.\n\\]\n\nCálculo da variância\nUsamos: \\[\n\\mathrm{Var}(X)=E(X^2)-[E(X)]^2.\n\\]\nSubstituindo:\n\n\\(E(X^2)=\\frac{2}{\\lambda^2}\\)\n\\(E(X)=\\frac{1}{\\lambda}\\)\n\nEntão: \\[\n\\mathrm{Var}(X)\n= \\frac{2}{\\lambda^2} - \\left(\\frac{1}{\\lambda}\\right)^2\n= \\frac{2}{\\lambda^2} - \\frac{1}{\\lambda^2}\n= \\frac{1}{\\lambda^2}.\n\\]\nLoggo, para \\(X \\sim \\text{Exponencial}(\\lambda)\\):\n\\[\nE(X)=\\frac{1}{\\lambda},\n\\qquad\n\\mathrm{Var}(X)=\\frac{1}{\\lambda^2}.\n\\]\n\n\n\nExemplo: O tempo entre chegadas ao caixa segue \\(X \\sim Exp(0{,}2)\\).\n\nCalcule \\(P(X&gt;8)\\).\n\nDetermine a mediana.\n\nInterprete a propriedade “sem memória”.\n\nSolução:\n\n\\[\nP(X&gt;8)=e^{-0{,}2\\cdot 8}=e^{-1{,}6}\\approx 0{,}2019\n\\]\n\\[\nm=\\frac{\\ln 2}{0{,}2}=5\\ln 2 \\approx 3{,}47\n\\]\nO tempo adicional não depende do tempo já passado.",
    "crumbs": [
      "Revisão Probabilidade I"
    ]
  },
  {
    "objectID": "exercicios.html",
    "href": "exercicios.html",
    "title": "Listas de exercícios",
    "section": "",
    "text": "Lista de exercícios 01 - Data de entrega: 25/11/2025",
    "crumbs": [
      "Listas de exercícios"
    ]
  }
]